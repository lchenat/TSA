{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from deep_rl.gridworld import ReachGridWorld, PickGridWorld, PORGBEnv, GoalManager, ScaleObsEnv\n",
    "from deep_rl.network import *\n",
    "from deep_rl.utils import *\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import dill\n",
    "import json\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "from collections import Counter, namedtuple\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from IPython.core.debugger import Tracer\n",
    "\n",
    "def seed(s):\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)\n",
    "\n",
    "seed(0) # set seed \n",
    "\n",
    "def fload(fn, ftype):\n",
    "    if ftype == 'json':\n",
    "        with open(fn) as f:\n",
    "            return json.load(f)\n",
    "    elif ftype == 'pkl':\n",
    "        with open(fn, 'rb') as f:\n",
    "            return dill.load(f)\n",
    "    elif ftype == 'png':\n",
    "        raise NotImplementedError\n",
    "    else:\n",
    "        raise Exception('cannot read this data type: {}'.format(ftype))\n",
    "    \n",
    "def fsave(data, fn, ftype):\n",
    "    dirname = os.path.dirname(fn)\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "    if ftype == 'json':\n",
    "        with open(fn, 'w') as f:\n",
    "            json.dump(data, f)\n",
    "    elif ftype == 'pkl':\n",
    "        with open(fn, 'wb') as f:\n",
    "            dill.dump(data, f)    \n",
    "    elif ftype == 'png':\n",
    "        Image.fromarray(data).save(fn)\n",
    "    else:\n",
    "        raise Exception('unsupported file type: {}'.format(ftype))\n",
    "        \n",
    "GoalConfig = namedtuple('GoalConfig', ['map_name', 'n_goal', 'min_dis'])  \n",
    "\n",
    "# multitask NMF from: https://ieeexplore.ieee.org/document/6939673\n",
    "class MTNMF:\n",
    "    def __init__(self, n_components, l1_ratio=0.0, max_iter=200, tol=0.0001):\n",
    "        self.n_components = n_components\n",
    "        self.l1_ratio = l1_ratio\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "\n",
    "    def loss(self, X, A, S):\n",
    "        return 0.5 * ((X - np.matmul(A, S)) ** 2).sum() + self.l1_ratio * S.sum()\n",
    "        \n",
    "    # input: a stack of observed data X_1, ..., X_K\n",
    "    # output: S, A_1, ..., A_K\n",
    "    def fit(self, X):\n",
    "        K, N, M = X.shape\n",
    "        A = np.random.rand(K, N, self.n_components)\n",
    "        S = np.random.rand(self.n_components, M)\n",
    "        prev_loss = np.inf\n",
    "        cur_loss = None\n",
    "        for i in range(self.max_iter):\n",
    "            A_T = A.transpose(0, 2, 1)\n",
    "            new_S = S * (np.matmul(A_T, X).sum(0)) / (np.matmul(np.matmul(A_T, A), S).sum(0) + K * self.l1_ratio * np.ones((self.n_components, M)))\n",
    "            S = new_S\n",
    "            new_A = A * np.matmul(X, S.T) / np.matmul(np.matmul(A, S), S.T)\n",
    "            A = new_A\n",
    "            cur_loss = self.loss(X, A, S)\n",
    "            if i % 100 == 0: print('NMF loss:', cur_loss)\n",
    "            if abs(cur_loss - prev_loss) < self.tol: break\n",
    "            prev_loss = cur_loss # update loss\n",
    "        return A, S, {'loss': cur_loss, 'iter': i}\n",
    "    \n",
    "def rollout(env, idx, policy=None, horizon=100):\n",
    "    states = []\n",
    "    returns = 0.0\n",
    "    done = False\n",
    "    #normalizer = ImageNormalizer()\n",
    "    state = env.reset(sample_obj_pos=False) # very important!\n",
    "    info = dict(task_id=[idx])\n",
    "    for _ in range(horizon):\n",
    "        states.append(state)\n",
    "        if policy is None:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = policy([state], info)['a'][0].cpu().detach().numpy()\n",
    "        state, r, done, _ = env.step(action) # note that info is not used\n",
    "        returns += r\n",
    "        if done: break\n",
    "    return states, returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTNMF (Fourroom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n",
      "{'map_names': ['fourroom'], 'train_combos': [(0, 0)], 'test_combos': [(0, 0)], 'num_obj_types': 4, 'obj_pos': [[(1, 1), (9, 1), (1, 9), (9, 9)]]}\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "maps: [(0, 'fourroom')]\n",
      "tasks: [(0, ('A',)), (1, ('B',)), (2, ('C',)), (3, ('D',))]\n",
      "train: [(0, 0)]\n",
      "test: [(0, 0)]\n",
      "{'map_names': ['fourroom'], 'train_combos': [(0, 1)], 'test_combos': [(0, 1)], 'num_obj_types': 4, 'obj_pos': [[(1, 1), (9, 1), (1, 9), (9, 9)]]}\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "maps: [(0, 'fourroom')]\n",
      "tasks: [(0, ('A',)), (1, ('B',)), (2, ('C',)), (3, ('D',))]\n",
      "train: [(0, 1)]\n",
      "test: [(0, 1)]\n",
      "{'map_names': ['fourroom'], 'train_combos': [(0, 2)], 'test_combos': [(0, 2)], 'num_obj_types': 4, 'obj_pos': [[(1, 1), (9, 1), (1, 9), (9, 9)]]}\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "maps: [(0, 'fourroom')]\n",
      "tasks: [(0, ('A',)), (1, ('B',)), (2, ('C',)), (3, ('D',))]\n",
      "train: [(0, 2)]\n",
      "test: [(0, 2)]\n",
      "goal: 0, mean return: 10.824000000000002\n",
      "goal: 1, mean return: 10.819999999999999\n",
      "goal: 2, mean return: 10.72\n",
      "states shape: (983, 6, 22, 22)\n",
      "(983, 5)\n",
      "(983, 5)\n",
      "(983, 5)\n",
      "NMF loss: 234.22740151888232\n",
      "NMF loss: 1.715115761157115\n",
      "NMF loss: 0.9351606117174749\n",
      "NMF loss: 0.7143472353790111\n",
      "NMF loss: 0.5857272733751162\n",
      "NMF loss: 0.4870151420803509\n",
      "NMF loss: 0.37983777692654325\n",
      "NMF loss: 0.2725072675774854\n",
      "NMF loss: 0.218962539031047\n",
      "NMF loss: 0.18800336450773686\n",
      "NMF loss: 0.16248980544973096\n",
      "NMF loss: 0.13932396919723855\n",
      "NMF loss: 0.11828266629781659\n",
      "NMF loss: 0.09993615869439122\n",
      "NMF loss: 0.08438988881282965\n",
      "NMF loss: 0.07196183010601112\n",
      "(3, 983, 5)\n"
     ]
    }
   ],
   "source": [
    "%pdb on\n",
    "n_abs = 20\n",
    "l1_ratio = 0.0 # this is currently not working... since alpha is not set\n",
    "feat_dim = 512\n",
    "action_dim = 5\n",
    "horizon = 100\n",
    "n_trajs = 10\n",
    "scale=2\n",
    "n_objs = 4\n",
    "\n",
    "def get_expert(weight_path, action_dim):\n",
    "    visual_body = TSAMiniMiniConvBody(\n",
    "        2 + n_objs, \n",
    "        feature_dim=feat_dim,\n",
    "        scale=scale,\n",
    "        gate=F.softplus,\n",
    "    )\n",
    "    expert = CategoricalActorCriticNet(\n",
    "        n_objs,\n",
    "        0, # state_dim\n",
    "        action_dim,\n",
    "        visual_body,\n",
    "    )\n",
    "    # load weight\n",
    "    weight_dict = expert.state_dict()\n",
    "    loaded_weight_dict = {k: v for k, v in torch.load(\n",
    "        weight_path,\n",
    "        map_location=lambda storage, loc: storage)['network'].items()\n",
    "        if k in weight_dict}\n",
    "    weight_dict.update(loaded_weight_dict)\n",
    "    expert.load_state_dict(weight_dict)\n",
    "    return expert\n",
    "\n",
    "seed(0)\n",
    "\n",
    "expert_dict = {\n",
    "    0: '../log/pick.mask.fourroom.0.min_dis-1/tsa.baseline.n_abs-512/_/0.190325-172629/models/step-92160-mean-10.86',\n",
    "    1: '../log/pick.mask.fourroom.1.min_dis-1/tsa.baseline.n_abs-512/_/0.190325-172739/models/step-92160-mean-10.74',\n",
    "    2: '../log/pick.mask.fourroom.2.min_dis-1/tsa.baseline.n_abs-512/_/0.190325-172847/models/step-92160-mean-10.81',\n",
    "}\n",
    "\n",
    "envs = dict()\n",
    "for i in range(3):\n",
    "    with open('../data/env_configs/pick/fourroom/fourroom.{}'.format(i), 'rb') as f:\n",
    "        env_config = dill.load(f)\n",
    "    print(env_config)\n",
    "    envs[i] = ScaleObsEnv(\n",
    "        PickGridWorld(\n",
    "            **env_config,\n",
    "            min_dis=1,\n",
    "            window=1,\n",
    "            task_length=1,\n",
    "            seed=0,\n",
    "        ),\n",
    "        2,\n",
    "    )\n",
    "\n",
    "decomposer = MTNMF(n_abs, max_iter=5000, tol=0.0001)\n",
    "\n",
    "states = []\n",
    "experts = dict()\n",
    "\n",
    "for goal_idx, weight_path in expert_dict.items():\n",
    "    returns = []\n",
    "    experts[goal_idx] = get_expert(weight_path, action_dim)\n",
    "#     for _ in range(n_trajs):\n",
    "#         rollout_states, rollout_returns = rollout(envs[goal_idx], goal_idx, experts[goal_idx], horizon=horizon)\n",
    "#         states.append(rollout_states)\n",
    "#         returns.append(rollout_returns)\n",
    "    for _ in range(n_trajs // 2):\n",
    "        rollout_states, rollout_returns = rollout(envs[goal_idx], goal_idx, experts[goal_idx], horizon=horizon)\n",
    "        states.append(rollout_states)\n",
    "        returns.append(rollout_returns)\n",
    "    for _ in range(n_trajs - (n_trajs // 2)):\n",
    "        rollout_states, rollout_returns = rollout(envs[goal_idx], goal_idx, None, horizon=horizon)\n",
    "        states.append(rollout_states)\n",
    "    if returns: print('goal: {}, mean return: {}'.format(goal_idx, np.mean(returns)))\n",
    "states = np.concatenate(states)\n",
    "print('states shape:', states.shape)\n",
    "\n",
    "pvs = []\n",
    "\n",
    "for goal_idx in expert_dict:\n",
    "    cur_infos = {'task_id': [goal_idx] * len(states)}\n",
    "    pv = F.softmax(experts[goal_idx].get_logits(states, cur_infos), dim=-1).cpu().detach().numpy()\n",
    "    print(pv.shape)\n",
    "    pv = pv.reshape(pv.shape[0], -1)\n",
    "    pvs.append(pv)\n",
    "\n",
    "pvs = np.stack(pvs, 0)\n",
    "A, S, info = MTNMF(n_abs, max_iter=5000, l1_ratio=l1_ratio).fit(pvs.transpose(0, 2, 1))\n",
    "print(pvs.shape)\n",
    "\n",
    "fsave(\n",
    "    dict(\n",
    "        abs=S.T,\n",
    "        policies=list(pvs),\n",
    "        states=[states for _ in range(len(pvs))],\n",
    "        infos=list([[{'task_id': i} for _ in range(len(states))] for i in experts.keys()]),\n",
    "    ),\n",
    "    '../data/nmf_sample/pick/fourroom/split.mix.{}-{}'.format(n_trajs, n_abs),\n",
    "    'pkl',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTNMF (16x16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n",
      "{'map_names': ['map49'], 'train_combos': [(0, 1)], 'test_combos': [(0, 0)], 'num_obj_types': 5, 'obj_pos': [[(13, 6), (4, 13), (12, 2), (7, 4), (10, 14)]]}\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "maps: [(0, 'map49')]\n",
      "tasks: [(0, ('A',)), (1, ('B',)), (2, ('C',)), (3, ('D',)), (4, ('E',))]\n",
      "train: [(0, 1)]\n",
      "test: [(0, 0)]\n",
      "{'map_names': ['map49'], 'train_combos': [(0, 2)], 'test_combos': [(0, 0)], 'num_obj_types': 5, 'obj_pos': [[(13, 6), (4, 13), (12, 2), (7, 4), (10, 14)]]}\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "maps: [(0, 'map49')]\n",
      "tasks: [(0, ('A',)), (1, ('B',)), (2, ('C',)), (3, ('D',)), (4, ('E',))]\n",
      "train: [(0, 2)]\n",
      "test: [(0, 0)]\n",
      "{'map_names': ['map49'], 'train_combos': [(0, 3)], 'test_combos': [(0, 0)], 'num_obj_types': 5, 'obj_pos': [[(13, 6), (4, 13), (12, 2), (7, 4), (10, 14)]]}\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "maps: [(0, 'map49')]\n",
      "tasks: [(0, ('A',)), (1, ('B',)), (2, ('C',)), (3, ('D',)), (4, ('E',))]\n",
      "train: [(0, 3)]\n",
      "test: [(0, 0)]\n",
      "goal: 1, mean return: 10.825\n",
      "goal: 2, mean return: 10.729\n",
      "goal: 3, mean return: 10.750499999999999\n",
      "states shape: (1386, 7, 32, 32)\n",
      "(1386, 5)\n",
      "(1386, 5)\n",
      "(1386, 5)\n",
      "NMF loss: 1104.8571246345532\n",
      "NMF loss: 23.12595219799843\n",
      "NMF loss: 22.184904303321844\n",
      "NMF loss: 21.32048805388361\n",
      "NMF loss: 19.76810116572497\n",
      "NMF loss: 16.648377347860286\n",
      "NMF loss: 7.78037957804346\n",
      "NMF loss: 7.5439797820258425\n",
      "NMF loss: 7.460257348741382\n",
      "NMF loss: 7.420814346402271\n",
      "NMF loss: 7.395461485713542\n",
      "NMF loss: 7.346009176010723\n",
      "NMF loss: 7.270553708720518\n",
      "NMF loss: 7.258510736870807\n",
      "(3, 1386, 5)\n"
     ]
    }
   ],
   "source": [
    "%pdb on\n",
    "n_abs = 20\n",
    "l1_ratio = 0.0 # this is currently not working... since alpha is not set\n",
    "feat_dim = 512\n",
    "action_dim = 5\n",
    "horizon = 100\n",
    "n_trajs = 20\n",
    "scale=2\n",
    "\n",
    "def get_expert(weight_path, action_dim):\n",
    "    visual_body = TSAMiniConvBody(\n",
    "        7, \n",
    "        feature_dim=feat_dim,\n",
    "        scale=scale)\n",
    "    expert = CategoricalActorCriticNet(\n",
    "        5,\n",
    "        0, # state_dim\n",
    "        action_dim,\n",
    "        visual_body,\n",
    "    )\n",
    "    # load weight\n",
    "    weight_dict = expert.state_dict()\n",
    "    loaded_weight_dict = {k: v for k, v in torch.load(\n",
    "        weight_path,\n",
    "        map_location=lambda storage, loc: storage)['network'].items()\n",
    "        if k in weight_dict}\n",
    "    weight_dict.update(loaded_weight_dict)\n",
    "    expert.load_state_dict(weight_dict)\n",
    "    return expert\n",
    "\n",
    "seed(0)\n",
    "\n",
    "expert_dict = {\n",
    "    1: '../log/pick.mask.map49.5-1.min_dis-1/tsa.baseline.n_abs-512/_/0.190323-000115/models/step-1945600-mean-10.81',\n",
    "    2: '../log/pick.mask.map49.5-2.min_dis-1/tsa.baseline.n_abs-512/_/0.190323-002744/models/step-1945600-mean-10.72',\n",
    "    3: '../log/pick.mask.map49.5-3.min_dis-1/tsa.baseline.n_abs-512/_/0.190323-005347/models/step-1945600-mean-10.78',\n",
    "}\n",
    "\n",
    "envs = dict()\n",
    "for i in range(1, 4):\n",
    "    with open('../data/env_configs/pick/nmf/map49.5-{}'.format(i), 'rb') as f:\n",
    "        env_config = dill.load(f)\n",
    "    print(env_config)\n",
    "    envs[i] = ScaleObsEnv(\n",
    "        PickGridWorld(\n",
    "            **env_config,\n",
    "            min_dis=1,\n",
    "            window=1,\n",
    "            task_length=1,\n",
    "            seed=0,\n",
    "        ),\n",
    "        2,\n",
    "    )\n",
    "\n",
    "decomposer = MTNMF(n_abs, max_iter=5000, tol=0.0001)\n",
    "\n",
    "states = []\n",
    "experts = dict()\n",
    "\n",
    "for goal_idx, weight_path in expert_dict.items():\n",
    "    returns = []\n",
    "    experts[goal_idx] = get_expert(weight_path, action_dim)\n",
    "    for _ in range(n_trajs):\n",
    "        rollout_states, rollout_returns = rollout(envs[goal_idx], goal_idx, experts[goal_idx], horizon=horizon)\n",
    "        states.append(rollout_states)\n",
    "        returns.append(rollout_returns)\n",
    "    #for _ in range(n_trajs // 2):\n",
    "    #    states.append(rollout(envs[goal_idx], experts[goal_idx], horizon=horizon))\n",
    "    #for _ in range(n_trajs - (n_trajs // 2)):\n",
    "    #    states.append(rollout(envs[goal_idx-1], None, horizon=horizon))\n",
    "    print('goal: {}, mean return: {}'.format(goal_idx, np.mean(returns)))\n",
    "states = np.concatenate(states)\n",
    "print('states shape:', states.shape)\n",
    "\n",
    "pvs = []\n",
    "\n",
    "for goal_idx in expert_dict:\n",
    "    cur_infos = {'task_id': [goal_idx] * len(states)}\n",
    "    pv = F.softmax(experts[goal_idx].get_logits(states, cur_infos), dim=-1).cpu().detach().numpy()\n",
    "    print(pv.shape)\n",
    "    pv = pv.reshape(pv.shape[0], -1)\n",
    "    pvs.append(pv)\n",
    "\n",
    "pvs = np.stack(pvs, 0)\n",
    "A, S, info = MTNMF(n_abs, max_iter=5000, l1_ratio=l1_ratio).fit(pvs.transpose(0, 2, 1))\n",
    "print(pvs.shape)\n",
    "\n",
    "fsave(\n",
    "    dict(\n",
    "        abs=S.T,\n",
    "        policies=list(pvs),\n",
    "        states=[states for _ in range(len(pvs))],\n",
    "        infos=list([[{'task_id': i} for _ in range(len(states))] for i in experts.keys()]),\n",
    "    ),\n",
    "    '../data/nmf_sample/pick/split.{}'.format(n_abs),\n",
    "    'pkl',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate EnvConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'map_names': ['map49'], 'train_combos': [(0, 1), (0, 2), (0, 3)], 'test_combos': [(0, 0)], 'num_obj_types': 5, 'obj_pos': [[(13, 6), (4, 13), (12, 2), (7, 4), (10, 14)]]}\n"
     ]
    }
   ],
   "source": [
    "seed(0) # set seed \n",
    "\n",
    "def get_pick_config(goal_config, train_combos=None):\n",
    "    MAX_OBJ_NUM = 15\n",
    "    goal_manager = GoalManager(goal_config.map_name)\n",
    "    obj_pos = goal_manager.gen_goals(MAX_OBJ_NUM + 1, min_dis=goal_config.min_dis)\n",
    "    obj_pos = [obj_pos[-1:] + obj_pos[:goal_config.n_goal-1]] # always the same test\n",
    "    if train_combos is None:\n",
    "        train_combos = [(0, i) for i in range(1, goal_config.n_goal)]\n",
    "    env_config = dict(\n",
    "        map_names = [goal_config.map_name],\n",
    "        train_combos = train_combos,\n",
    "        test_combos = [(0, 0)],\n",
    "        num_obj_types=goal_config.n_goal,\n",
    "        obj_pos=obj_pos,\n",
    "    )\n",
    "    return env_config \n",
    "\n",
    "map_name = 'map49'\n",
    "n_goal = 5\n",
    "train_idx = 4\n",
    "\n",
    "\n",
    "env_config = get_pick_config(\n",
    "    GoalConfig(\n",
    "        map_name=map_name,\n",
    "        n_goal=n_goal,\n",
    "        min_dis=4,\n",
    "    ),\n",
    "    #train_combos=[(0, train_idx)],\n",
    "    train_combos=[(0, 1), (0, 2), (0, 3)],\n",
    ")\n",
    "\n",
    "print(env_config)\n",
    "fsave(env_config, \n",
    "      #'../data/env_configs/pick/nmf/{}.{}-{}'.format(map_name, n_goal, train_idx), \n",
    "      '../data/env_configs/pick/nmf/{}.{}-f3'.format(map_name, n_goal),\n",
    "      ftype='pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EnvConfig for fourroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(0)\n",
    "\n",
    "corner = 2\n",
    "\n",
    "env_config = dict(\n",
    "    map_names=['fourroom'],\n",
    "    #train_combos=[(0, corner)],\n",
    "    #test_combos=[(0, corner)],\n",
    "    train_combos=[(0, 0), (0, 1), (0, 2)],\n",
    "    test_combos=[(0, 0)],\n",
    "    num_obj_types=4,\n",
    "    obj_pos=[[(1, 1), (9, 1), (1, 9), (9, 9)]],\n",
    ")\n",
    "\n",
    "fsave(\n",
    "    env_config,\n",
    "    #'../data/env_configs/pick/fourroom/{}'.format(corner),\n",
    "    '../data/env_configs/pick/fourroom/f3',\n",
    "    ftype='pkl',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EnvConfig for fourroom-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(0)\n",
    "\n",
    "corner = 3\n",
    "\n",
    "env_config = dict(\n",
    "    map_names=['fourroom-16'],\n",
    "    train_combos=[(0, corner)],\n",
    "    test_combos=[(0, corner)],\n",
    "    num_obj_types=4,\n",
    "    obj_pos=[[(1, 1), (14, 1), (1, 14), (14, 14)]],\n",
    ")\n",
    "\n",
    "fsave(\n",
    "    env_config,\n",
    "    '../data/env_configs/pick/fourroom-16.{}'.format(corner),\n",
    "    ftype='pkl',\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
