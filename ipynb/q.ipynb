{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "warning: mujoco is not installed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "You appear to be missing a License Key for mujoco.  We expected to find the\n",
      "file here: /home/liyuc/.mujoco/mjkey.txt\n",
      "\n",
      "You can get licenses at this page:\n",
      "\n",
      "    https://www.roboti.us/license.html\n",
      "\n",
      "If python tries to activate an invalid license, the process will exit.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from deep_rl.gridworld import ReachGridWorld, PickGridWorld, PORGBEnv, GoalManager, ScaleObsEnv\n",
    "from deep_rl.network import *\n",
    "from deep_rl.utils import *\n",
    "from train import _exp_parser, get_visual_body, get_network, get_env_config, PickGridWorldTask\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import dill\n",
    "import json\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "from collections import Counter, namedtuple\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from IPython.core.debugger import Tracer\n",
    "\n",
    "def set_seed(s):\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)\n",
    "    torch.manual_seed(s)\n",
    "\n",
    "set_seed(0) # set seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Fitted Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_objs = 4\n",
    "action_dim = 5\n",
    "feat_dim = 512\n",
    "scale = 2\n",
    "discount = 0.99\n",
    "\n",
    "def get_expert(weight_path):\n",
    "    visual_body = TSAMiniConvBody(\n",
    "        2 + n_objs, \n",
    "        feature_dim=feat_dim,\n",
    "        scale=scale,\n",
    "    )\n",
    "    expert = VanillaNet(action_dim, visual_body)\n",
    "    # load weight\n",
    "    weight_dict = expert.state_dict()\n",
    "    loaded_weight_dict = {k: v for k, v in torch.load(\n",
    "        weight_path,\n",
    "        map_location=lambda storage, loc: storage)['network'].items()\n",
    "        if k in weight_dict}\n",
    "    weight_dict.update(loaded_weight_dict)\n",
    "    expert.load_state_dict(weight_dict)\n",
    "    return expert\n",
    "\n",
    "def get_env(env_config):\n",
    "    states = []\n",
    "    positions = []\n",
    "    qs = []\n",
    "    reward_config = {'wall_penalty': -0.01, 'time_penalty': -0.01, 'complete_sub_task': 0.1, 'complete_all': 1, 'fail': -1}\n",
    "    with open(env_config, 'rb') as f:\n",
    "        env_config = dill.load(f)\n",
    "    env = ScaleObsEnv(\n",
    "        PickGridWorld(\n",
    "                **env_config,\n",
    "                min_dis=1,\n",
    "                window=1,\n",
    "                task_length=1,\n",
    "                reward_config=reward_config,\n",
    "                seed=0,\n",
    "        ),\n",
    "        2,\n",
    "    )\n",
    "    env.reset(sample_obj_pos=False)\n",
    "    positions = env.unwrapped.pos_candidates\n",
    "    for pos in positions:\n",
    "        o, _, _, _ = env.teleport(*pos)\n",
    "        states.append(o)\n",
    "        qs.append(env.get_q(discount))\n",
    "    return env, states, positions, qs\n",
    "\n",
    "def rollout(env, q, horizon=100, epsilon=0.0, feat_state=False):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    next_states = []\n",
    "    terminals = []\n",
    "    qs = []\n",
    "    returns = 0.0\n",
    "    done = False\n",
    "    state = env.reset(sample_obj_pos=False) # very important!\n",
    "    for _ in range(horizon):\n",
    "        if feat_state:\n",
    "            states.append(q.body(tensor([state])).cpu().detach().numpy()[0])\n",
    "        else:\n",
    "            states.append(state)\n",
    "        qval = q([state]).cpu().detach().numpy().flatten()\n",
    "        qs.append(qval)\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = qval.argmax()\n",
    "        state, r, done, _ = env.step(action) # note that info is not used\n",
    "        actions.append(action)\n",
    "        if feat_state:\n",
    "            next_states.append(q.body(tensor([state])).cpu().detach().numpy()[0])\n",
    "        else:\n",
    "            next_states.append(state)\n",
    "        rewards.append(r)\n",
    "        terminals.append(done)\n",
    "        returns += r\n",
    "        if done: break\n",
    "    return states, actions, next_states, rewards, terminals, qs, returns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "maps: [(0, 'fourroom-16')]\n",
      "tasks: [(0, ('A',)), (1, ('B',)), (2, ('C',)), (3, ('D',))]\n",
      "train: [(0, 0)]\n",
      "test: [(0, 0)]\n",
      "expert returns: 0.91\n",
      "expert returns: 1.0\n",
      "expert returns: 0.9800000000000001\n",
      "expert returns: 0.88\n",
      "expert returns: 0.9\n",
      "expert returns: 0.9400000000000001\n",
      "expert returns: 0.9600000000000001\n",
      "expert returns: 1.0\n",
      "expert returns: 0.9700000000000001\n",
      "expert returns: 0.9400000000000001\n",
      "random returns: -1.1200000000000008\n",
      "random returns: -1.0700000000000007\n",
      "random returns: -1.0500000000000007\n",
      "random returns: -1.1700000000000008\n",
      "random returns: -1.2300000000000009\n",
      "random returns: -1.1600000000000008\n",
      "random returns: -1.0800000000000007\n",
      "random returns: -1.0700000000000007\n",
      "random returns: -1.0900000000000007\n",
      "random returns: -2.1200000000000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liyuc/anaconda3/envs/tsa/lib/python3.6/site-packages/ipykernel_launcher.py:64: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7530391236797941\n",
      "6.604199421511358\n",
      "4.154672512829174\n",
      "2.4240169852112743\n",
      "2731844996070853.0\n",
      "2.898442468664994\n",
      "3.534794307722824\n",
      "5.090476179537211\n",
      "4.90384586440879\n",
      "3.437958506738988\n",
      "3.158840799500725\n",
      "3.171617646016419\n",
      "3.520866007552742\n",
      "5.362635730216085\n",
      "2731844977714423.5\n",
      "2.526371566359914\n",
      "3.668366791723194\n",
      "3.55749571545002e+16\n",
      "2.5725146662220584\n",
      "4.2764989804098406\n",
      "682961249382715.8\n",
      "2.7808678450840327\n",
      "3.910702749640933\n",
      "4.0794259431713575\n",
      "2.8042517285729103\n",
      "3.7363403122466616\n",
      "4.060435356313744\n",
      "1.6759303515066033\n",
      "5.112886566453829\n",
      "4.416063274029171\n",
      "3.927258479465625\n",
      "4.465514169661016\n",
      "1.998799884100855\n",
      "4.06469224927175\n",
      "3.706488226212251\n",
      "2.4920296374496087\n",
      "3.7462679213716115\n",
      "1.2465396885408199e+19\n",
      "2.7850605864564946\n",
      "2.684912384570506\n",
      "3.710083741226179\n",
      "2.804782595772397e+19\n",
      "2.3784140635245805\n",
      "2.8746454971102966\n",
      "2.562785060324813\n",
      "3.430447977489571\n",
      "3.557495703561783e+16\n",
      "2731844964232539.0\n",
      "3.9575808113045\n",
      "3.688430402707513\n"
     ]
    }
   ],
   "source": [
    "%pdb on\n",
    "n_expert_trajs = 10\n",
    "n_random_trajs = 10\n",
    "epsilon = 0.0\n",
    "feat_state = True\n",
    "\n",
    "weight_path = '../log/pick.mask.fourroom-16.0.min_dis-1/dqn/double_q/0.190425-220424/models/step-3000000-mean-0.96'\n",
    "env_config_path = '../data/env_configs/pick/fourroom-16.0'\n",
    "\n",
    "\n",
    "expert = get_expert(weight_path)\n",
    "env, all_states, positions, optimal_q = get_env(env_config_path)\n",
    "\n",
    "states = []\n",
    "actions = []\n",
    "next_states = []\n",
    "rewards = []\n",
    "terminals = []\n",
    "qs = []\n",
    "\n",
    "for _ in range(n_expert_trajs):\n",
    "    states_, actions_, next_states_, rewards_, terminals_, qs_, returns = rollout(env, expert, epsilon=epsilon, feat_state=feat_state)\n",
    "    print('expert returns:', returns)\n",
    "    states.append(states_)\n",
    "    actions.append(actions_)\n",
    "    next_states.append(next_states_)\n",
    "    rewards.append(rewards_)\n",
    "    terminals.append(terminals_)\n",
    "    qs.append(qs_)\n",
    "    \n",
    "for _ in range(n_random_trajs):\n",
    "    states_, actions_, next_states_, rewards_, terminals_, qs_, returns = rollout(env, expert, epsilon=1.0, feat_state=feat_state)\n",
    "    print('random returns:', returns)\n",
    "    states.append(states_)\n",
    "    actions.append(actions_)\n",
    "    next_states.append(next_states_)\n",
    "    rewards.append(rewards_)\n",
    "    terminals.append(terminals_)\n",
    "    qs.append(qs_)\n",
    "\n",
    "\n",
    "data = dict(\n",
    "    states=np.concatenate(states),\n",
    "    actions=np.concatenate(actions),\n",
    "    next_states=np.concatenate(next_states),\n",
    "    rewards=np.concatenate(rewards),\n",
    "    terminals=np.concatenate(terminals),\n",
    "    expert_q=np.concatenate(qs),\n",
    ")\n",
    "\n",
    "# Fitted Q iteration\n",
    "weight = np.zeros((feat_dim, action_dim))\n",
    "for _ in range(50):\n",
    "    new_weight = np.zeros((feat_dim, action_dim))\n",
    "    for a in range(action_dim):\n",
    "        indices = data['actions'] == a\n",
    "        states = data['states'][indices]\n",
    "        actions = data['actions'][indices]\n",
    "        next_states = data['next_states'][indices]\n",
    "        rewards = data['rewards'][indices]\n",
    "        terminals = data['terminals'][indices]\n",
    "        qs = (states @ weight).argmax(1)\n",
    "        targets = rewards + discount * (1 - terminals) * qs\n",
    "        new_weight[:, a] = np.linalg.lstsq(states, targets)[0]\n",
    "        #new_weight[:, a] = np.linalg.lstsq(states.T @ states / states.shape[0], states.T.dot(targets) / states.shape[0])[0]\n",
    "    weight = new_weight\n",
    "    #print(data['states'][:5] @ weight)\n",
    "    print(((data['expert_q'] - data['states'] @ weight) ** 2).mean())\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
