{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "warning: mujoco is not installed\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from deep_rl.gridworld import ReachGridWorld, PickGridWorld, PORGBEnv, GoalManager, ScaleObsEnv\n",
    "from deep_rl.network import *\n",
    "from deep_rl.utils import *\n",
    "from train import _exp_parser, get_visual_body, get_network, get_env_config, PickGridWorldTask\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import dill\n",
    "import json\n",
    "import copy\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "from collections import Counter, namedtuple\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from IPython.core.debugger import Tracer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def set_seed(s):\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)\n",
    "    torch.manual_seed(s)\n",
    "\n",
    "set_seed(0) # set seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Fitted Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_objs = 4\n",
    "action_dim = 5\n",
    "feat_dim = 512\n",
    "scale = 2\n",
    "discount = 0.99\n",
    "\n",
    "def get_expert(weight_path):\n",
    "    visual_body = TSAMiniConvBody(\n",
    "        2 + n_objs, \n",
    "        feature_dim=feat_dim,\n",
    "        scale=scale,\n",
    "    )\n",
    "    expert = VanillaNet(action_dim, visual_body)\n",
    "    # load weight\n",
    "    weight_dict = expert.state_dict()\n",
    "    loaded_weight_dict = {k: v for k, v in torch.load(\n",
    "        weight_path,\n",
    "        map_location=lambda storage, loc: storage)['network'].items()\n",
    "        if k in weight_dict}\n",
    "    weight_dict.update(loaded_weight_dict)\n",
    "    expert.load_state_dict(weight_dict)\n",
    "    return expert\n",
    "\n",
    "def get_env(env_config):\n",
    "    states = []\n",
    "    positions = []\n",
    "    qs = []\n",
    "    reward_config = {'wall_penalty': -0.01, 'time_penalty': -0.01, 'complete_sub_task': 0.1, 'complete_all': 1, 'fail': -1}\n",
    "    with open(env_config, 'rb') as f:\n",
    "        env_config = dill.load(f)\n",
    "    env = ScaleObsEnv(\n",
    "        PickGridWorld(\n",
    "                **env_config,\n",
    "                min_dis=1,\n",
    "                window=1,\n",
    "                task_length=1,\n",
    "                reward_config=reward_config,\n",
    "                seed=0,\n",
    "        ),\n",
    "        2,\n",
    "    )\n",
    "    env.reset(sample_obj_pos=False)\n",
    "    positions = env.unwrapped.pos_candidates\n",
    "    for pos in positions:\n",
    "        o, _, _, _ = env.teleport(*pos)\n",
    "        states.append(o)\n",
    "        qs.append(env.get_q(discount))\n",
    "    return env, states, positions, qs\n",
    "\n",
    "def rollout(env, q, horizon=100, epsilon=0.0, feat_state=False):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    next_states = []\n",
    "    terminals = []\n",
    "    qs = []\n",
    "    returns = 0.0\n",
    "    done = False\n",
    "    state = env.reset(sample_obj_pos=False) # very important!\n",
    "    for _ in range(horizon):\n",
    "        if feat_state:\n",
    "            states.append(q.body(tensor([state])).cpu().detach().numpy()[0])\n",
    "        else:\n",
    "            states.append(state)\n",
    "        qval = q([state]).cpu().detach().numpy().flatten()\n",
    "        qs.append(qval)\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = qval.argmax()\n",
    "        state, r, done, _ = env.step(action) # note that info is not used\n",
    "        actions.append(action)\n",
    "        if feat_state:\n",
    "            next_states.append(q.body(tensor([state])).cpu().detach().numpy()[0])\n",
    "        else:\n",
    "            next_states.append(state)\n",
    "        rewards.append(r)\n",
    "        terminals.append(done)\n",
    "        returns += r\n",
    "        if done: break\n",
    "    return states, actions, next_states, rewards, terminals, qs, returns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "maps: [(0, 'fourroom-16')]\n",
      "tasks: [(0, ('A',)), (1, ('B',)), (2, ('C',)), (3, ('D',))]\n",
      "train: [(0, 0)]\n",
      "test: [(0, 0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 28.64it/s]\n",
      "  1%|          | 3/291 [00:00<00:10, 26.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of transitions: 291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 291/291 [00:09<00:00, 31.35it/s]\n",
      "/home/liyuc/anaconda3/envs/tsa/lib/python3.6/site-packages/ipykernel_launcher.py:65: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03546138380832832\n",
      "difference between argmax: 114\n"
     ]
    }
   ],
   "source": [
    "%pdb on\n",
    "n_expert_trajs = 20\n",
    "epsilon = 0.0\n",
    "feat_state = False\n",
    "\n",
    "weight_path = '../log/pick.mask.fourroom-16.0.min_dis-1/dqn/double_q/0.190425-220424/models/step-3000000-mean-0.96'\n",
    "env_config_path = '../data/env_configs/pick/fourroom-16.0'\n",
    "\n",
    "expert = get_expert(weight_path)\n",
    "env, all_states, positions, optimal_q = get_env(env_config_path)\n",
    "\n",
    "states = []\n",
    "actions = []\n",
    "next_states = []\n",
    "rewards = []\n",
    "terminals = []\n",
    "qs = []\n",
    "\n",
    "for _ in tqdm(range(n_expert_trajs)):\n",
    "    states_, actions_, next_states_, rewards_, terminals_, qs_, returns = rollout(env, expert, epsilon=epsilon, feat_state=feat_state)\n",
    "    #print('expert returns:', returns)\n",
    "    states.append(states_)\n",
    "    actions.append(actions_)\n",
    "    next_states.append(next_states_)\n",
    "    rewards.append(rewards_)\n",
    "    terminals.append(terminals_)\n",
    "    qs.append(qs_)\n",
    "\n",
    "data = dict(\n",
    "    states=np.concatenate(states),\n",
    "    actions=np.concatenate(actions),\n",
    "    next_states=np.concatenate(next_states),\n",
    "    rewards=np.concatenate(rewards),\n",
    "    terminals=np.concatenate(terminals),\n",
    "    expert_q=np.concatenate(qs),\n",
    ")\n",
    "print('num of transitions:', len(data['states']))\n",
    "\n",
    "# input: experiences, feature extractor\n",
    "# output: linear layer\n",
    "def fitted_q(data, body, feat_dim, action_dim):\n",
    "    A = np.zeros((feat_dim * action_dim + action_dim, feat_dim * action_dim + action_dim))\n",
    "    b = np.zeros(feat_dim * action_dim + action_dim)\n",
    "    N = len(data['states'])\n",
    "\n",
    "    pbar = tqdm(total=N)\n",
    "    for i, transition in enumerate(zip(data['states'], data['actions'], data['next_states'], data['rewards'], data['terminals'])):\n",
    "        state, action, next_state, reward, terminal = transition\n",
    "        phi = np.zeros(feat_dim * action_dim + action_dim)\n",
    "        phi[feat_dim * action: feat_dim * (action + 1)] = body(tensor([state])).detach().cpu().numpy()[0]\n",
    "        phi[feat_dim * action_dim + action] = 1\n",
    "        b += reward * phi / N\n",
    "        if terminal:\n",
    "            A += np.outer(phi, phi) / N\n",
    "        else:\n",
    "            next_phi = np.zeros(feat_dim * action_dim + action_dim)\n",
    "            s_idx = feat_dim * data['actions'][i+1] # assume trajectories is contiguous\n",
    "            next_phi[s_idx: s_idx + feat_dim] = body(tensor([next_state])).detach().cpu().numpy()[0]\n",
    "            next_phi[feat_dim * action_dim + data['actions'][i+1]] = 1\n",
    "            A += np.outer(phi, phi - discount * next_phi)\n",
    "        pbar.update(1)\n",
    "    pbar.close() \n",
    "\n",
    "    # update weight\n",
    "    total_weight = np.linalg.lstsq(A, b)[0]\n",
    "    weight = total_weight[:-action_dim].reshape(-1, feat_dim).T\n",
    "    bias = total_weight[-action_dim:]\n",
    "    return weight, bias\n",
    "    \n",
    "\n",
    "# # Fitted Q iteration\n",
    "# A = np.zeros((feat_dim * action_dim + action_dim, feat_dim * action_dim + action_dim))\n",
    "# b = np.zeros(feat_dim * action_dim + action_dim)\n",
    "# N = len(data['states'])\n",
    "\n",
    "# #weight = expert.fc_head.weight.cpu().detach().numpy()\n",
    "# #bias = expert.fc_head.bias.cpu().detach().numpy()\n",
    "# #print(weight.shape, bias.shape)\n",
    "# #w = np.concatenate([weight.flatten(), bias])\n",
    "\n",
    "# pbar = tqdm(total=N)\n",
    "# for i, transition in enumerate(zip(data['states'], data['actions'], data['next_states'], data['rewards'], data['terminals'], data['expert_q'])):\n",
    "#     state, action, next_state, reward, terminal, expert_q = transition\n",
    "#     phi = np.zeros(feat_dim * action_dim + action_dim)\n",
    "#     phi[feat_dim * action: feat_dim * (action + 1)] = state\n",
    "#     phi[feat_dim * action_dim + action] = 1\n",
    "#     #print(phi.dot(w) - expert_q[action])\n",
    "#     #print(phi.dot(w) - (weight.dot(state) + bias)[action])\n",
    "#     b += reward * phi / N\n",
    "#     if terminal:\n",
    "#         A += np.outer(phi, phi) / N\n",
    "#         #print(phi.dot(w) - expert_q[action])\n",
    "#     else:\n",
    "#         next_phi = np.zeros(feat_dim * action_dim + action_dim)\n",
    "#         s_idx = feat_dim * data['actions'][i+1]\n",
    "#         next_phi[s_idx: s_idx + feat_dim] = next_state\n",
    "#         next_phi[feat_dim * action_dim + data['actions'][i+1]] = 1\n",
    "#         A += np.outer(phi, phi - discount * next_phi)\n",
    "#         #print(phi.dot(w) - reward - discount * next_phi.dot(w), expert_q[action] - reward - discount * data['expert_q'][i+1][data['actions'][i+1]])\n",
    "#         #print(next_phi.dot(w) - data['expert_q'][i+1][data['actions'][i+1]])\n",
    "#     #if i % 10 == 0: print('at {}'.format(i))\n",
    "#     pbar.update(1)\n",
    "# pbar.close() \n",
    "\n",
    "# # update weight\n",
    "# total_weight = np.linalg.lstsq(A, b)[0]\n",
    "# weight = total_weight[:-action_dim].reshape(-1, feat_dim).T\n",
    "# bias = total_weight[-action_dim:]\n",
    "\n",
    "weight, bias = fitted_q(data, expert.body, feat_dim, action_dim)\n",
    "#weight, bias = np.random.randn(feat_dim, action_dim), np.random.randn(action_dim)\n",
    "\n",
    "estimate_q = expert.body(tensor(data['states'])).detach().cpu().numpy() @ weight + bias\n",
    "print(((estimate_q - data['expert_q']) ** 2).mean())\n",
    "print('difference between argmax:', (estimate_q.argmax(1) != data['expert_q'].argmax(1)).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Linear Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "maps: [(0, 'fourroom-16')]\n",
      "tasks: [(0, ('A',)), (1, ('B',)), (2, ('C',)), (3, ('D',))]\n",
      "train: [(0, 0)]\n",
      "test: [(0, 0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|█         | 3/30 [00:00<00:00, 27.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 17%|█▋        | 5/30 [00:00<00:01, 24.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 27%|██▋       | 8/30 [00:00<00:00, 25.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 37%|███▋      | 11/30 [00:00<00:00, 25.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 15/30 [00:00<00:00, 27.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 60%|██████    | 18/30 [00:00<00:00, 25.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████   | 21/30 [00:00<00:00, 25.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 83%|████████▎ | 25/30 [00:00<00:00, 28.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 97%|█████████▋| 29/30 [00:01<00:00, 28.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 30/30 [00:01<00:00, 26.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/420 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 1/420 [00:00<00:04, 90.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of transitions: 420\n",
      "tot_A: tensor([[-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0004,  0.0000,  0.0000,  ...,  0.0024,  0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        ...,\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<TBackward>)\n",
      "tensor([[-0.1490,  0.0000,  0.0000,  ...,  1.0000,  0.0000,  0.0000]],\n",
      "       grad_fn=<SubBackward0>)\n",
      "tensor([0., 0., 0.,  ..., 1., 0., 0.], grad_fn=<CopySlices>)\n",
      "tensor([-0.1490,  0.0000,  0.0000,  ...,  1.0000,  0.0000,  0.0000],\n",
      "       grad_fn=<SubBackward0>)\n",
      "tensor(3.4925e-10, grad_fn=<MaxBackward1>)\n",
      "tensor(3.6380e-12, grad_fn=<MaxBackward1>)\n",
      "0-th loss: 0.85890085\n"
     ]
    }
   ],
   "source": [
    "# D_1, ..., D_n\n",
    "# D, body / \\phi -> A, b -> w(\\phi)\n",
    "# Q(\\phi) - Q_E as loss\n",
    "\n",
    "n_expert_trajs = 30\n",
    "epsilon = 0.0\n",
    "\n",
    "weight_path = '../log/pick.mask.fourroom-16.0.min_dis-1/dqn/double_q/0.190425-220424/models/step-3000000-mean-0.96'\n",
    "env_config_path = '../data/env_configs/pick/fourroom-16.0'\n",
    "\n",
    "expert = get_expert(weight_path)\n",
    "env, all_states, positions, optimal_q = get_env(env_config_path)\n",
    "\n",
    "states = []\n",
    "actions = []\n",
    "next_states = []\n",
    "rewards = []\n",
    "terminals = []\n",
    "qs = []\n",
    "\n",
    "for _ in tqdm(range(n_expert_trajs)):\n",
    "    states_, actions_, next_states_, rewards_, terminals_, qs_, returns = rollout(env, expert, epsilon=epsilon)\n",
    "    #print('expert returns:', returns)\n",
    "    states.append(states_)\n",
    "    actions.append(actions_)\n",
    "    next_states.append(next_states_)\n",
    "    rewards.append(rewards_)\n",
    "    terminals.append(terminals_)\n",
    "    qs.append(qs_)\n",
    "\n",
    "data = dict(\n",
    "    states=np.concatenate(states),\n",
    "    actions=np.concatenate(actions),\n",
    "    next_states=np.concatenate(next_states),\n",
    "    rewards=np.concatenate(rewards),\n",
    "    terminals=np.concatenate(terminals),\n",
    "    expert_q=np.concatenate(qs),\n",
    ")\n",
    "#data = {k: v[:2] for k, v in data.items()}\n",
    "print('num of transitions:', len(data['states']))\n",
    "\n",
    "sub_idx = 1\n",
    "\n",
    "def fitted_q(data, body, feat_dim, action_dim):\n",
    "    A = torch.zeros(feat_dim * action_dim + action_dim, feat_dim * action_dim + action_dim)\n",
    "    b = torch.zeros(feat_dim * action_dim + action_dim)\n",
    "    N = len(data['states'])\n",
    "\n",
    "    feats = body(tensor(data['states'])).repeat(1, action_dim)\n",
    "    a_vec = one_hot.encode(tensor(data['actions'], torch.long), action_dim)\n",
    "    phis = torch.cat([feats * a_vec.repeat_interleave(feat_dim, 1), a_vec], 1)\n",
    "    tot_A = torch.matmul(phis[:sub_idx].t(), phis[:sub_idx] - discount * tensor(1 - data['terminals'])[:sub_idx].unsqueeze(1) * phis.roll(-1, 0)[:sub_idx]) / N\n",
    "    tot_b = torch.matmul(phis[:sub_idx].t(), tensor(data['rewards'])[:sub_idx]) / N\n",
    "    \n",
    "    pbar = tqdm(total=N)\n",
    "    for i, transition in enumerate(zip(data['states'], data['actions'], data['next_states'], data['rewards'], data['terminals'])):\n",
    "        if i == sub_idx: break\n",
    "        state, action, next_state, reward, terminal = transition\n",
    "        phi = torch.zeros(feat_dim * action_dim + action_dim)\n",
    "        phi[feat_dim * action: feat_dim * (action + 1)] = body(tensor([state]))[0]\n",
    "        phi[feat_dim * action_dim + action] = 1\n",
    "        b += float(reward) * phi / N\n",
    "        if terminal:\n",
    "            A += torch.ger(phi, phi) / N\n",
    "        else:\n",
    "            next_phi = torch.zeros(feat_dim * action_dim + action_dim)\n",
    "            s_idx = feat_dim * data['actions'][i+1] # assume trajectories is contiguous\n",
    "            next_phi[s_idx: s_idx + feat_dim] = body(tensor([next_state]))[0]\n",
    "            next_phi[feat_dim * action_dim + data['actions'][i+1]] = 1\n",
    "            #print(torch.abs(phis[i+1]-next_phi).max())\n",
    "            #print(torch.abs(next_phi - next_phis[i]).max())\n",
    "            A += torch.ger(phi, phi - discount * next_phi) / N\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    print(torch.abs(tot_A - A).max())\n",
    "    print(torch.abs(tot_b - b).max())\n",
    "    \n",
    "    # update weight\n",
    "    #print(torch.isnan(A).any())\n",
    "    total_weight = torch.matmul(torch.inverse(A + 0.1 * torch.eye(A.shape[0])), b)\n",
    "    weight = total_weight[:-action_dim].view(-1, feat_dim).t()\n",
    "    bias = total_weight[-action_dim:]\n",
    "    return weight, bias\n",
    "\n",
    "\n",
    "model = copy.deepcopy(expert)\n",
    "optim = torch.optim.RMSprop(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), lr=0.00025, alpha=0.95, eps=0.01, centered=True)\n",
    "#weight, bias = torch.randn(feat_dim, action_dim), torch.randn(action_dim)\n",
    "for i in range(1):\n",
    "    weight, bias = fitted_q(data, model.body, feat_dim, action_dim)\n",
    "    estimate_q = torch.matmul(model.body(tensor(data['states'])), weight) + bias\n",
    "    loss = F.mse_loss(estimate_q, tensor(data['expert_q']))\n",
    "    print('{}-th loss:'.format(i), loss.detach().cpu().numpy())\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  0.,  0.],\n",
      "        [ 3.,  4.,  5.],\n",
      "        [ 6.,  8., 10.]])\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# print(torch.__version__)\n",
    "\n",
    "# class DummyModule(torch.nn.Module):\n",
    "#     def forward(self, x):\n",
    "#         V = torch.Tensor(2, 2)\n",
    "#         V[0, 0] = x\n",
    "#         return torch.sum(V * 3)\n",
    "\n",
    "\n",
    "# x = torch.tensor([1], requires_grad=True)\n",
    "# r = DummyModule()(x)\n",
    "# r.backward()\n",
    "# print(x.grad)\n",
    "\n",
    "\n",
    "a = tensor([[0, 1, 2]])\n",
    "b = tensor([[3, 4, 5]])\n",
    "print(torch.matmul(a.t(), b))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
