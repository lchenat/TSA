{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from deep_rl.gridworld import ReachGridWorld, PickGridWorld, PORGBEnv, GoalManager, ScaleObsEnv\n",
    "from deep_rl.network import *\n",
    "from deep_rl.utils import *\n",
    "from train import _exp_parser, get_visual_body, get_network, get_env_config, PickGridWorldTask\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import dill\n",
    "import json\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "from collections import Counter, namedtuple\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from IPython.core.debugger import Tracer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def set_seed(s):\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)\n",
    "    torch.manual_seed(s)\n",
    "\n",
    "set_seed(0) # set seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Fitted Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_objs = 4\n",
    "action_dim = 5\n",
    "feat_dim = 512\n",
    "scale = 2\n",
    "discount = 0.99\n",
    "\n",
    "def get_expert(weight_path):\n",
    "    visual_body = TSAMiniConvBody(\n",
    "        2 + n_objs, \n",
    "        feature_dim=feat_dim,\n",
    "        scale=scale,\n",
    "    )\n",
    "    expert = VanillaNet(action_dim, visual_body)\n",
    "    # load weight\n",
    "    weight_dict = expert.state_dict()\n",
    "    loaded_weight_dict = {k: v for k, v in torch.load(\n",
    "        weight_path,\n",
    "        map_location=lambda storage, loc: storage)['network'].items()\n",
    "        if k in weight_dict}\n",
    "    weight_dict.update(loaded_weight_dict)\n",
    "    expert.load_state_dict(weight_dict)\n",
    "    return expert\n",
    "\n",
    "def get_env(env_config):\n",
    "    states = []\n",
    "    positions = []\n",
    "    qs = []\n",
    "    reward_config = {'wall_penalty': -0.01, 'time_penalty': -0.01, 'complete_sub_task': 0.1, 'complete_all': 1, 'fail': -1}\n",
    "    with open(env_config, 'rb') as f:\n",
    "        env_config = dill.load(f)\n",
    "    env = ScaleObsEnv(\n",
    "        PickGridWorld(\n",
    "                **env_config,\n",
    "                min_dis=1,\n",
    "                window=1,\n",
    "                task_length=1,\n",
    "                reward_config=reward_config,\n",
    "                seed=0,\n",
    "        ),\n",
    "        2,\n",
    "    )\n",
    "    env.reset(sample_obj_pos=False)\n",
    "    positions = env.unwrapped.pos_candidates\n",
    "    for pos in positions:\n",
    "        o, _, _, _ = env.teleport(*pos)\n",
    "        states.append(o)\n",
    "        qs.append(env.get_q(discount))\n",
    "    return env, states, positions, qs\n",
    "\n",
    "def rollout(env, q, horizon=100, epsilon=0.0, feat_state=False):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    next_states = []\n",
    "    terminals = []\n",
    "    qs = []\n",
    "    returns = 0.0\n",
    "    done = False\n",
    "    state = env.reset(sample_obj_pos=False) # very important!\n",
    "    for _ in range(horizon):\n",
    "        if feat_state:\n",
    "            states.append(q.body(tensor([state])).cpu().detach().numpy()[0])\n",
    "        else:\n",
    "            states.append(state)\n",
    "        qval = q([state]).cpu().detach().numpy().flatten()\n",
    "        qs.append(qval)\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = qval.argmax()\n",
    "        state, r, done, _ = env.step(action) # note that info is not used\n",
    "        actions.append(action)\n",
    "        if feat_state:\n",
    "            next_states.append(q.body(tensor([state])).cpu().detach().numpy()[0])\n",
    "        else:\n",
    "            next_states.append(state)\n",
    "        rewards.append(r)\n",
    "        terminals.append(done)\n",
    "        returns += r\n",
    "        if done: break\n",
    "    return states, actions, next_states, rewards, terminals, qs, returns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "maps: [(0, 'fourroom-16')]\n",
      "tasks: [(0, ('A',)), (1, ('B',)), (2, ('C',)), (3, ('D',))]\n",
      "train: [(0, 0)]\n",
      "test: [(0, 0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 3/19 [00:00<00:00, 27.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expert returns: 0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 31.27it/s]\n",
      "/home/liyuc/anaconda3/envs/tsa/lib/python3.6/site-packages/ipykernel_launcher.py:73: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3271358519712087\n"
     ]
    }
   ],
   "source": [
    "%pdb on\n",
    "n_expert_trajs = 1\n",
    "epsilon = 0.0\n",
    "feat_state = True\n",
    "\n",
    "weight_path = '../log/pick.mask.fourroom-16.0.min_dis-1/dqn/double_q/0.190425-220424/models/step-3000000-mean-0.96'\n",
    "env_config_path = '../data/env_configs/pick/fourroom-16.0'\n",
    "\n",
    "expert = get_expert(weight_path)\n",
    "env, all_states, positions, optimal_q = get_env(env_config_path)\n",
    "\n",
    "states = []\n",
    "actions = []\n",
    "next_states = []\n",
    "rewards = []\n",
    "terminals = []\n",
    "qs = []\n",
    "\n",
    "for _ in range(n_expert_trajs):\n",
    "    states_, actions_, next_states_, rewards_, terminals_, qs_, returns = rollout(env, expert, epsilon=epsilon, feat_state=feat_state)\n",
    "    print('expert returns:', returns)\n",
    "    states.append(states_)\n",
    "    actions.append(actions_)\n",
    "    next_states.append(next_states_)\n",
    "    rewards.append(rewards_)\n",
    "    terminals.append(terminals_)\n",
    "    qs.append(qs_)\n",
    "\n",
    "data = dict(\n",
    "    states=np.concatenate(states),\n",
    "    actions=np.concatenate(actions),\n",
    "    next_states=np.concatenate(next_states),\n",
    "    rewards=np.concatenate(rewards),\n",
    "    terminals=np.concatenate(terminals),\n",
    "    expert_q=np.concatenate(qs),\n",
    ")\n",
    "\n",
    "# input: experiences, feature extractor\n",
    "# output: linear layer\n",
    "def fitted_q(data, body, feat_dim, action_dim):\n",
    "    A = np.zeros((feat_dim * action_dim + action_dim, feat_dim * action_dim + action_dim))\n",
    "    b = np.zeros(feat_dim * action_dim + action_dim)\n",
    "    N = len(data['states'])\n",
    "\n",
    "# Fitted Q iteration\n",
    "A = np.zeros((feat_dim * action_dim + action_dim, feat_dim * action_dim + action_dim))\n",
    "b = np.zeros(feat_dim * action_dim + action_dim)\n",
    "N = len(data['states'])\n",
    "\n",
    "#weight = expert.fc_head.weight.cpu().detach().numpy()\n",
    "#bias = expert.fc_head.bias.cpu().detach().numpy()\n",
    "#print(weight.shape, bias.shape)\n",
    "#w = np.concatenate([weight.flatten(), bias])\n",
    "\n",
    "pbar = tqdm(total=N)\n",
    "for i, transition in enumerate(zip(data['states'], data['actions'], data['next_states'], data['rewards'], data['terminals'], data['expert_q'])):\n",
    "    state, action, next_state, reward, terminal, expert_q = transition\n",
    "    phi = np.zeros(feat_dim * action_dim + action_dim)\n",
    "    phi[feat_dim * action: feat_dim * (action + 1)] = state\n",
    "    phi[feat_dim * action_dim + action] = 1\n",
    "    #print(phi.dot(w) - expert_q[action])\n",
    "    #print(phi.dot(w) - (weight.dot(state) + bias)[action])\n",
    "    b += reward * phi / N\n",
    "    if terminal:\n",
    "        A += np.outer(phi, phi) / N\n",
    "        #print(phi.dot(w) - expert_q[action])\n",
    "    else:\n",
    "        next_phi = np.zeros(feat_dim * action_dim + action_dim)\n",
    "        s_idx = feat_dim * data['actions'][i+1]\n",
    "        next_phi[s_idx: s_idx + feat_dim] = next_state\n",
    "        next_phi[feat_dim * action_dim + data['actions'][i+1]] = 1\n",
    "        A += np.outer(phi, phi - discount * next_phi)\n",
    "        #print(phi.dot(w) - reward - discount * next_phi.dot(w), expert_q[action] - reward - discount * data['expert_q'][i+1][data['actions'][i+1]])\n",
    "        #print(next_phi.dot(w) - data['expert_q'][i+1][data['actions'][i+1]])\n",
    "    #if i % 10 == 0: print('at {}'.format(i))\n",
    "    pbar.update(1)\n",
    "pbar.close() \n",
    "\n",
    "# update weight\n",
    "total_weight = np.linalg.lstsq(A, b)[0]\n",
    "weight = total_weight[:-action_dim].reshape(-1, feat_dim).T\n",
    "bias = total_weight[-action_dim:]\n",
    "\n",
    "estimate_q = data['states'] @ weight + bias\n",
    "print(((estimate_q - data['expert_q']) ** 2).mean())\n",
    "print('difference between argmax:', (estimate_q.argmax() != expert_q.argmax()).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Linear Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D_1, ..., D_n\n",
    "# D, \\phi -> A, b -> w(\\phi)\n",
    "# Q(\\phi) - Q_E as loss\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
