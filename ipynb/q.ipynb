{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from deep_rl.gridworld import ReachGridWorld, PickGridWorld, PORGBEnv, GoalManager, ScaleObsEnv\n",
    "from deep_rl.network import *\n",
    "from deep_rl.utils import *\n",
    "from train import _exp_parser, get_visual_body, get_network, get_env_config, PickGridWorldTask\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import dill\n",
    "import json\n",
    "import copy\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "from collections import Counter, namedtuple\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from IPython.core.debugger import Tracer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def set_seed(s):\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)\n",
    "    torch.manual_seed(s)\n",
    "\n",
    "set_seed(0) # set seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Fitted Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_objs = 4\n",
    "action_dim = 5\n",
    "feat_dim = 512\n",
    "scale = 2\n",
    "discount = 0.99\n",
    "\n",
    "def get_expert(weight_path):\n",
    "    visual_body = TSAMiniConvBody(\n",
    "        2 + n_objs, \n",
    "        feature_dim=feat_dim,\n",
    "        scale=scale,\n",
    "    )\n",
    "    expert = VanillaNet(action_dim, visual_body)\n",
    "    # load weight\n",
    "    weight_dict = expert.state_dict()\n",
    "    loaded_weight_dict = {k: v for k, v in torch.load(\n",
    "        weight_path,\n",
    "        map_location=lambda storage, loc: storage)['network'].items()\n",
    "        if k in weight_dict}\n",
    "    weight_dict.update(loaded_weight_dict)\n",
    "    expert.load_state_dict(weight_dict)\n",
    "    return expert\n",
    "\n",
    "def get_env(env_config):\n",
    "    states = []\n",
    "    positions = []\n",
    "    qs = []\n",
    "    reward_config = {'wall_penalty': -0.01, 'time_penalty': -0.01, 'complete_sub_task': 0.1, 'complete_all': 1, 'fail': -1}\n",
    "    with open(env_config, 'rb') as f:\n",
    "        env_config = dill.load(f)\n",
    "    env = ScaleObsEnv(\n",
    "        PickGridWorld(\n",
    "                **env_config,\n",
    "                min_dis=1,\n",
    "                window=1,\n",
    "                task_length=1,\n",
    "                reward_config=reward_config,\n",
    "                seed=0,\n",
    "        ),\n",
    "        2,\n",
    "    )\n",
    "    env.reset(sample_obj_pos=False)\n",
    "    positions = env.unwrapped.pos_candidates\n",
    "    for pos in positions:\n",
    "        o, _, _, _ = env.teleport(*pos)\n",
    "        states.append(o)\n",
    "        qs.append(env.get_q(discount))\n",
    "    return env, states, positions, qs\n",
    "\n",
    "def rollout(env, q, horizon=100, epsilon=0.0, feat_state=False):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    next_states = []\n",
    "    terminals = []\n",
    "    qs = []\n",
    "    returns = 0.0\n",
    "    done = False\n",
    "    state = env.reset(sample_obj_pos=False) # very important!\n",
    "    for _ in range(horizon):\n",
    "        if feat_state:\n",
    "            states.append(q.body(tensor([state])).cpu().detach().numpy()[0])\n",
    "        else:\n",
    "            states.append(state)\n",
    "        qval = q([state]).cpu().detach().numpy().flatten()\n",
    "        qs.append(qval)\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = qval.argmax()\n",
    "        state, r, done, _ = env.step(action) # note that info is not used\n",
    "        actions.append(action)\n",
    "        if feat_state:\n",
    "            next_states.append(q.body(tensor([state])).cpu().detach().numpy()[0])\n",
    "        else:\n",
    "            next_states.append(state)\n",
    "        rewards.append(r)\n",
    "        terminals.append(done)\n",
    "        returns += r\n",
    "        if done: break\n",
    "    return states, actions, next_states, rewards, terminals, qs, returns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "maps: [(0, 'fourroom-16')]\n",
      "tasks: [(0, ('A',)), (1, ('B',)), (2, ('C',)), (3, ('D',))]\n",
      "train: [(0, 0)]\n",
      "test: [(0, 0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:12<00:00, 27.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of transitions: 29454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liyuc/anaconda3/envs/tsa/lib/python3.6/site-packages/ipykernel_launcher.py:52: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01659769967412279\n",
      "difference between argmax: 17129\n",
      "[[0.74836036 0.60369045 0.74413376 0.6654658  0.76228272]\n",
      " [0.76175127 0.63501297 0.76053703 0.68347097 0.79641732]\n",
      " [0.77954674 0.6798445  0.78007591 0.66827122 0.79370532]\n",
      " ...\n",
      " [1.0210867  0.82932089 1.048409   0.757595   0.92456787]\n",
      " [1.04761135 0.7969205  1.0691     0.7887416  1.02937634]\n",
      " [1.05806591 0.76661029 1.07518061 0.8062914  1.09      ]]\n",
      "\n",
      "[[0.73559886 0.7194368  0.7483418  0.7340713  0.72777945]\n",
      " [0.7664412  0.7394179  0.76616466 0.72755235 0.7490153 ]\n",
      " [0.7852332  0.7568129  0.7838565  0.7514845  0.7688418 ]\n",
      " ...\n",
      " [1.0151633  1.0040917  1.0483844  1.001756   1.0287615 ]\n",
      " [1.0374562  1.027259   1.0691859  1.0235871  1.0467873 ]\n",
      " [1.0582035  1.0501345  1.0585701  1.0527699  1.0902131 ]]\n"
     ]
    }
   ],
   "source": [
    "%pdb on\n",
    "n_expert_trajs = 2000\n",
    "epsilon = 0.0\n",
    "feat_state = False\n",
    "\n",
    "weight_path = '../log/pick.mask.fourroom-16.0.min_dis-1/dqn/double_q/0.190425-220424/models/step-3000000-mean-0.96'\n",
    "env_config_path = '../data/env_configs/pick/fourroom-16.0'\n",
    "\n",
    "expert = get_expert(weight_path)\n",
    "env, all_states, positions, optimal_q = get_env(env_config_path)\n",
    "\n",
    "states = []\n",
    "actions = []\n",
    "next_states = []\n",
    "rewards = []\n",
    "terminals = []\n",
    "qs = []\n",
    "\n",
    "for _ in tqdm(range(n_expert_trajs)):\n",
    "    states_, actions_, next_states_, rewards_, terminals_, qs_, returns = rollout(env, expert, epsilon=epsilon, feat_state=feat_state)\n",
    "    #print('expert returns:', returns)\n",
    "    states.append(states_)\n",
    "    actions.append(actions_)\n",
    "    next_states.append(next_states_)\n",
    "    rewards.append(rewards_)\n",
    "    terminals.append(terminals_)\n",
    "    qs.append(qs_)\n",
    "\n",
    "data = dict(\n",
    "    states=np.concatenate(states),\n",
    "    actions=np.concatenate(actions),\n",
    "    next_states=np.concatenate(next_states),\n",
    "    rewards=np.concatenate(rewards),\n",
    "    terminals=np.concatenate(terminals),\n",
    "    expert_q=np.concatenate(qs),\n",
    ")\n",
    "print('num of transitions:', len(data['states']))\n",
    "\n",
    "def fitted_q(data, body, feat_dim, action_dim):\n",
    "    N = len(data['states'])\n",
    "\n",
    "    feats = body(tensor(data['states'])).repeat(1, action_dim).detach().cpu().numpy()\n",
    "    a_vec = np.eye(action_dim)[data['actions']]\n",
    "    phis = np.concatenate([feats * a_vec.repeat(feat_dim, 1), a_vec], 1)\n",
    "    \n",
    "    A = phis.T @ (phis - discount * np.expand_dims(1 - data['terminals'], 1) * np.roll(phis, -1, 0)) / N\n",
    "    b = phis.T @ data['rewards'] / N\n",
    "    \n",
    "    # update weight\n",
    "    #print(torch.isnan(A).any())\n",
    "    #total_weight = np.linalg.inv(A + 1e-4 * np.eye(A.shape[0])) @ b\n",
    "    total_weight = np.linalg.lstsq(A, b)[0]\n",
    "    weight = total_weight[:-action_dim].reshape(-1, feat_dim).T\n",
    "    bias = total_weight[-action_dim:]\n",
    "    return weight, bias\n",
    "\n",
    "# input: experiences, feature extractor\n",
    "# output: linear layer\n",
    "# def fitted_q(data, body, feat_dim, action_dim):\n",
    "#     A = np.zeros((feat_dim * action_dim + action_dim, feat_dim * action_dim + action_dim))\n",
    "#     b = np.zeros(feat_dim * action_dim + action_dim)\n",
    "#     N = len(data['states'])\n",
    "\n",
    "#     pbar = tqdm(total=N)\n",
    "#     for i, transition in enumerate(zip(data['states'], data['actions'], data['next_states'], data['rewards'], data['terminals'])):\n",
    "#         state, action, next_state, reward, terminal = transition\n",
    "#         phi = np.zeros(feat_dim * action_dim + action_dim)\n",
    "#         phi[feat_dim * action: feat_dim * (action + 1)] = body(tensor([state])).detach().cpu().numpy()[0]\n",
    "#         phi[feat_dim * action_dim + action] = 1\n",
    "#         b += reward * phi / N\n",
    "#         if terminal:\n",
    "#             A += np.outer(phi, phi) / N\n",
    "#         else:\n",
    "#             next_phi = np.zeros(feat_dim * action_dim + action_dim)\n",
    "#             s_idx = feat_dim * data['actions'][i+1] # assume trajectories is contiguous\n",
    "#             next_phi[s_idx: s_idx + feat_dim] = body(tensor([next_state])).detach().cpu().numpy()[0]\n",
    "#             next_phi[feat_dim * action_dim + data['actions'][i+1]] = 1\n",
    "#             A += np.outer(phi, phi - discount * next_phi) / N\n",
    "#         pbar.update(1)\n",
    "#     pbar.close() \n",
    "\n",
    "#     # update weight\n",
    "#     total_weight = np.linalg.lstsq(A, b)[0]\n",
    "#     weight = total_weight[:-action_dim].reshape(-1, feat_dim).T\n",
    "#     bias = total_weight[-action_dim:]\n",
    "#     return weight, bias\n",
    "    \n",
    "\n",
    "weight, bias = fitted_q(data, expert.body, feat_dim, action_dim)\n",
    "\n",
    "# model = copy.deepcopy(expert)\n",
    "# model.fc_head.weight.data.copy_(tensor(weight).t())\n",
    "# model.fc_head.bias.data.copy_(tensor(bias))\n",
    "# for _ in range(10):\n",
    "#     res = rollout(env, model)\n",
    "#     print(res[-1])\n",
    "\n",
    "estimate_q = expert.body(tensor(data['states'])).detach().cpu().numpy() @ weight + bias\n",
    "print(((estimate_q - data['expert_q']) ** 2).mean())\n",
    "print('difference between argmax:', (estimate_q.argmax(1) != data['expert_q'].argmax(1)).sum())\n",
    "\n",
    "print(estimate_q)\n",
    "print()\n",
    "print(data['expert_q'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Linear Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "maps: [(0, 'fourroom-16')]\n",
      "tasks: [(0, ('A',)), (1, ('B',)), (2, ('C',)), (3, ('D',))]\n",
      "train: [(0, 0)]\n",
      "test: [(0, 0)]\n",
      "num of transitions: 29454\n",
      "0-th loss: 0.026722785\n"
     ]
    }
   ],
   "source": [
    "# D_1, ..., D_n\n",
    "# D, body / \\phi -> A, b -> w(\\phi)\n",
    "# Q(\\phi) - Q_E as loss\n",
    "\n",
    "n_expert_trajs = 2000\n",
    "epsilon = 0.0\n",
    "mkdir('log/meta_linear_q')\n",
    "\n",
    "weight_path = '../log/pick.mask.fourroom-16.0.min_dis-1/dqn/double_q/0.190425-220424/models/step-3000000-mean-0.96'\n",
    "env_config_path = '../data/env_configs/pick/fourroom-16.0'\n",
    "\n",
    "expert = get_expert(weight_path)\n",
    "env, all_states, positions, optimal_q = get_env(env_config_path)\n",
    "\n",
    "states = []\n",
    "actions = []\n",
    "next_states = []\n",
    "rewards = []\n",
    "terminals = []\n",
    "qs = []\n",
    "\n",
    "# for _ in tqdm(range(n_expert_trajs)):\n",
    "#     states_, actions_, next_states_, rewards_, terminals_, qs_, returns = rollout(env, expert, epsilon=epsilon)\n",
    "#     #print('expert returns:', returns)\n",
    "#     states.append(states_)\n",
    "#     actions.append(actions_)\n",
    "#     next_states.append(next_states_)\n",
    "#     rewards.append(rewards_)\n",
    "#     terminals.append(terminals_)\n",
    "#     qs.append(qs_)\n",
    "\n",
    "# data = dict(\n",
    "#     states=np.concatenate(states),\n",
    "#     actions=np.concatenate(actions),\n",
    "#     next_states=np.concatenate(next_states),\n",
    "#     rewards=np.concatenate(rewards),\n",
    "#     terminals=np.concatenate(terminals),\n",
    "#     expert_q=np.concatenate(qs),\n",
    "# )\n",
    "print('num of transitions:', len(data['states']))\n",
    "\n",
    "def fitted_q(data, body, feat_dim, action_dim):\n",
    "    A = torch.zeros(feat_dim * action_dim + action_dim, feat_dim * action_dim + action_dim)\n",
    "    b = torch.zeros(feat_dim * action_dim + action_dim)\n",
    "    N = len(data['states'])\n",
    "\n",
    "    feats = body(tensor(data['states'])).repeat(1, action_dim)\n",
    "    a_vec = one_hot.encode(tensor(data['actions'], torch.long), action_dim)\n",
    "    phis = torch.cat([feats * a_vec.repeat_interleave(feat_dim, 1), a_vec], 1)\n",
    "    \n",
    "    A = torch.matmul(phis.t(), phis - discount * tensor(1 - data['terminals']).unsqueeze(1) * phis.roll(-1, 0)) / N\n",
    "    b = torch.matmul(phis.t(), tensor(data['rewards'])) / N\n",
    "    \n",
    "    # update weight\n",
    "    #print(torch.isnan(A).any())\n",
    "    total_weight = torch.matmul(torch.inverse(A + 1e-4 * torch.eye(A.shape[0])), b)\n",
    "    #total_weight = torch.matmul(torch.pinverse(A), b)\n",
    "    weight = total_weight[:-action_dim].view(-1, feat_dim).t()\n",
    "    bias = total_weight[-action_dim:]\n",
    "    return weight, bias\n",
    "\n",
    "\n",
    "model = copy.deepcopy(expert)\n",
    "optim = torch.optim.RMSprop(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), lr=0.00025, alpha=0.95, eps=0.01, centered=True)\n",
    "#weight, bias = torch.randn(feat_dim, action_dim), torch.randn(action_dim)\n",
    "for i in range(2000):\n",
    "    weight, bias = fitted_q(data, model.body, feat_dim, action_dim)\n",
    "    model.fc_head.weight.data.copy_(weight.t())\n",
    "    model.fc_head.bias.data.copy_(bias)\n",
    "    estimate_q = model(data['states'])\n",
    "    #estimate_q = torch.matmul(model.body(tensor(data['states'])), weight) + bias\n",
    "    loss = F.mse_loss(estimate_q, tensor(data['expert_q']))\n",
    "    print('{}-th loss:'.format(i), loss.detach().cpu().numpy())\n",
    "    if i > 0 and i % 5 == 0: # save model\n",
    "        weight_dict = dict(network=model.state_dict())\n",
    "        torch.save(weight_dict, 'log/meta_linear_q/step-{}-loss-{:.4f}'.format(i, loss.detach().cpu().numpy()))\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# print(torch.__version__)\n",
    "\n",
    "# class DummyModule(torch.nn.Module):\n",
    "#     def forward(self, x):\n",
    "#         V = torch.Tensor(2, 2)\n",
    "#         V[0, 0] = x\n",
    "#         return torch.sum(V * 3)\n",
    "\n",
    "\n",
    "# x = torch.tensor([1], requires_grad=True)\n",
    "# r = DummyModule()(x)\n",
    "# r.backward()\n",
    "# print(x.grad)\n",
    "\n",
    "\n",
    "print(torch.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
