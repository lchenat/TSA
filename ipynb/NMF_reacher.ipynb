{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from deep_rl.reacher.env import MultiGoalReacherEnv, DiscretizeActionEnv\n",
    "from deep_rl.network import *\n",
    "from deep_rl.utils import *\n",
    "from sklearn.decomposition import NMF\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def set_seed(t, r=None, p=None, c=None):\n",
    "    if r is None:\n",
    "        r = t\n",
    "    if p is None:\n",
    "        p = r\n",
    "    torch.manual_seed(t)\n",
    "    random.seed(r)\n",
    "    np.random.seed(p)\n",
    "    if c is not None:\n",
    "        torch.cuda.manual_seed(c)\n",
    "\n",
    "class GridDrawer:                           \n",
    "    def __init__(self, color_list):\n",
    "        self.color_list = np.asarray(color_list)\n",
    "\n",
    "    # input: a 2-d index matrix\n",
    "    # output: a 2-d rgb matrix\n",
    "    def draw(self, indices, repeat=16):\n",
    "        return np.uint8(255 * np.array(self.color_list[indices, :]).repeat(repeat, 0).repeat(repeat, 1))\n",
    "    \n",
    "# this is my color list\n",
    "color_map = dict([\n",
    "    #*[('grey-{}'.format(v), plt.cm.Greys(0.1 * v)) for v in range(1, 20)],\n",
    "    *[('purple-{}'.format(v), plt.cm.Purples(0.05 * v)) for v in range(1, 20)],\n",
    "    *[('blue-{}'.format(v), plt.cm.Blues(0.05 * v)) for v in range(1, 20)],\n",
    "    *[('green-{}'.format(v), plt.cm.Greens(0.05 * v)) for v in range(1, 20)],\n",
    "    *[('orange-{}'.format(v), plt.cm.Oranges(0.05 * v)) for v in range(1, 20)],\n",
    "    *[('red-{}'.format(v), plt.cm.Reds(0.05 * v)) for v in range(1, 20)],\n",
    "])\n",
    "\n",
    "def imshow(img):\n",
    "    display(Image.fromarray(np.asarray(img)))\n",
    "\n",
    "color_list = list(color_map.values())\n",
    "shuffle(color_list)\n",
    "color_list = [plt.cm.Greys(0.9)] + [plt.cm.Greys(0.5)] + color_list\n",
    "drawer = GridDrawer(color_list)\n",
    "\n",
    "# multitask NMF from: https://ieeexplore.ieee.org/document/6939673\n",
    "class MTNMF:\n",
    "    def __init__(self, n_components, l1_ratio=0.0, max_iter=200, tol=0.0001):\n",
    "        self.n_components = n_components\n",
    "        self.l1_ratio = l1_ratio\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "\n",
    "    def loss(self, X, A, S):\n",
    "        return 0.5 * ((X - np.matmul(A, S)) ** 2).sum() + self.l1_ratio * S.sum()\n",
    "        \n",
    "    # input: a stack of observed data X_1, ..., X_K\n",
    "    # output: S, A_1, ..., A_K\n",
    "    def fit(self, X):\n",
    "        K, N, M = X.shape\n",
    "        A = np.random.rand(K, N, self.n_components)\n",
    "        S = np.random.rand(self.n_components, M)\n",
    "        prev_loss = np.inf\n",
    "        cur_loss = None\n",
    "        for i in range(self.max_iter):\n",
    "            A_T = A.transpose(0, 2, 1)\n",
    "            new_S = S * (np.matmul(A_T, X).sum(0)) / (np.matmul(np.matmul(A_T, A), S).sum(0) + K * self.l1_ratio * np.ones((self.n_components, M)))\n",
    "            S = new_S\n",
    "            new_A = A * np.matmul(X, S.T) / np.matmul(np.matmul(A, S), S.T)\n",
    "            A = new_A\n",
    "            cur_loss = self.loss(X, A, S)\n",
    "            if i % 100 == 0: print('NMF loss:', cur_loss)\n",
    "            if abs(cur_loss - prev_loss) < self.tol: break\n",
    "            prev_loss = cur_loss # update loss\n",
    "        return A, S, {'loss': cur_loss, 'iter': i}\n",
    "    \n",
    "def rollout(env, policy, horizon):\n",
    "    states = []\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    info = dict(task_id=[0])\n",
    "    for _ in range(horizon):\n",
    "        states.append(state)\n",
    "        action = policy([state], info)['a'][0].cpu().detach().numpy()\n",
    "        state, _, _, _ = env.step(action) # note that info is not used\n",
    "    return states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiTask NMF (Discrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "states shape: (3000, 16)\n",
      "(3000, 2, 5)\n",
      "(3000, 2, 5)\n",
      "(3000, 2, 5)\n",
      "NMF loss: 4092.6389125642854\n",
      "NMF loss: 349.7319401196926\n",
      "NMF loss: 321.1620092836637\n",
      "NMF loss: 317.1796374612312\n",
      "NMF loss: 316.3620302698508\n",
      "NMF loss: 315.68755501450903\n",
      "NMF loss: 315.2892515700252\n",
      "NMF loss: 315.0964620008219\n",
      "NMF loss: 314.8006374689336\n",
      "NMF loss: 314.1984734560883\n",
      "NMF loss: 313.7411696417353\n",
      "NMF loss: 313.63586950802693\n",
      "NMF loss: 313.59624608387094\n",
      "NMF loss: 313.5739174693248\n",
      "NMF loss: 313.5560431288848\n",
      "(3, 3000, 10)\n"
     ]
    }
   ],
   "source": [
    "def get_expert(weight_path, state_dim, action_dim):\n",
    "    expert = CategoricalActorCriticNet(\n",
    "        4,\n",
    "        state_dim,\n",
    "        action_dim.prod(),\n",
    "        FCBody(\n",
    "            state_dim, \n",
    "            hidden_units=(16,)\n",
    "        ),\n",
    "        SplitBody(\n",
    "            MultiLinear(16, action_dim.sum(), 4, key='task_id', w_scale=1e-3),\n",
    "            2,\n",
    "        ),\n",
    "    )\n",
    "    # load weight\n",
    "    weight_dict = expert.state_dict()\n",
    "    loaded_weight_dict = {k: v for k, v in torch.load(\n",
    "        weight_path,\n",
    "        map_location=lambda storage, loc: storage)['network'].items()\n",
    "        if k in weight_dict}\n",
    "    weight_dict.update(loaded_weight_dict)\n",
    "    expert.load_state_dict(weight_dict)\n",
    "    return expert\n",
    "\n",
    "n_abs = 10\n",
    "l1_ratio=0.0 # this is currently not working... since alpha is not set\n",
    "state_dim = 16\n",
    "action_dim = np.array((5, 5))\n",
    "horizon = 100\n",
    "n_trajs = 10\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "expert_dict = {\n",
    "    1: '../log/reacher.1_corner/fc_discrete.baseline/split/0.190315-202731/models/step-704000-mean--6.36',\n",
    "    2: '../log/reacher.2_corner/fc_discrete.baseline/split/0.190315-203310/models/step-704000-mean--17.16',\n",
    "    3: '../log/reacher.3_corner/fc_discrete.baseline/split/0.190315-203532/models/step-704000-mean--11.25',\n",
    "    #1: '../log/reacher.ng.1_corner/fc_discrete.baseline/ng/0.190316-161504/models/step-704000-mean--6.08',\n",
    "    #2: '../log/reacher.ng.2_corner/fc_discrete.baseline/ng/0.190316-161535/models/step-704000-mean--26.97',\n",
    "    #3: '../log/reacher.ng.3_corner/fc_discrete.baseline/ng/0.190316-162113/models/step-704000-mean--16.12',\n",
    "}\n",
    "\n",
    "envs = [DiscretizeActionEnv(\n",
    "    MultiGoalReacherEnv(\n",
    "        [\n",
    "            [0.15, 0.0],\n",
    "            [-0.15, 0.0],\n",
    "            [0.0, 0.15],\n",
    "            [0.0, -0.15],\n",
    "        ],\n",
    "        sample_indices=[i],\n",
    "        with_goal_pos=True,\n",
    "    ),\n",
    "    n_bins=[5, 5],\n",
    ") for i in range(3)]\n",
    "decomposer = MTNMF(n_abs, max_iter=5000, tol=0.0001)\n",
    "\n",
    "states = []\n",
    "experts = dict()\n",
    "\n",
    "for goal_idx, weight_path in expert_dict.items():\n",
    "    experts[goal_idx] = get_expert(weight_path, state_dim, action_dim)\n",
    "    for _ in range(n_trajs):\n",
    "        states.append(rollout(envs[goal_idx-1], experts[goal_idx], horizon=horizon))\n",
    "states = np.concatenate(states)\n",
    "print('states shape:', states.shape)\n",
    "    \n",
    "pvs = []\n",
    "    \n",
    "for goal_idx in expert_dict:\n",
    "    infos = {'task_id': [goal_idx-1] * len(states)}\n",
    "    pv = F.softmax(experts[goal_idx].get_logits(states, infos), dim=-1).cpu().detach().numpy()\n",
    "    print(pv.shape)\n",
    "    pv = pv.reshape(pv.shape[0], -1)\n",
    "    pvs.append(pv)\n",
    "\n",
    "pvs = np.stack(pvs, 0)\n",
    "A, S, info = MTNMF(n_abs, max_iter=5000, l1_ratio=l1_ratio).fit(pvs.transpose(0, 2, 1))\n",
    "print(pvs.shape)\n",
    "\n",
    "fsave(\n",
    "    dict(\n",
    "        abs=S.T,\n",
    "        policies=list(pvs.reshape(pvs.shape[0], pvs.shape[1], 2, -1)),\n",
    "        states=[states for _ in range(len(pvs))],\n",
    "        infos=list([[{'task_id': i} for _ in range(len(states))] for i in range(3)]),\n",
    "    ),\n",
    "    '../data/nmf_sample/reacher/split.{}'.format(n_abs),\n",
    "    'pkl',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiTask NMF (continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "states shape: (3000, 8)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'info'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5e0d8eea9163>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgoal_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexpert_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgoal_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgoal_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tsa/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'info'"
     ]
    }
   ],
   "source": [
    "n_abs = 6\n",
    "l1_ratio=0.0 # this is currently not working... since alpha is not set\n",
    "state_dim = 8\n",
    "action_dim = 2\n",
    "horizon = 100\n",
    "n_trajs = 10\n",
    "n_bins = 10\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "def get_expert(weight_path, state_dim, action_dim):\n",
    "    expert = GaussianActorCriticNet(\n",
    "        4,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        FCBody(\n",
    "            state_dim, \n",
    "            hidden_units=(32,)\n",
    "        ),\n",
    "    )\n",
    "    # load weight\n",
    "    weight_dict = expert.state_dict()\n",
    "    loaded_weight_dict = {k: v for k, v in torch.load(\n",
    "        weight_path,\n",
    "        map_location=lambda storage, loc: storage)['network'].items()\n",
    "        if k in weight_dict}\n",
    "    weight_dict.update(loaded_weight_dict)\n",
    "    expert.load_state_dict(weight_dict)\n",
    "    return expert\n",
    "\n",
    "expert_dict = {\n",
    "    1: '../log/reacher.ng.1_corner/fc_discrete.gaussian/cont/0.190316-212125/models/step-128000-mean--18.15',\n",
    "    2: '../log/reacher.ng.2_corner/fc_discrete.gaussian/cont/0.190316-212209/models/step-128000-mean--11.82',\n",
    "    3: '../log/reacher.ng.3_corner/fc_discrete.gaussian/cont/0.190316-214339/models/step-128000-mean--5.18',\n",
    "}\n",
    "\n",
    "envs = [MultiGoalReacherEnv(\n",
    "        [\n",
    "            [0.15, 0.0],\n",
    "            [-0.15, 0.0],\n",
    "            [0.0, 0.15],\n",
    "            [0.0, -0.15],\n",
    "        ],\n",
    "        sample_indices=[i],\n",
    "        with_goal_pos=False,\n",
    ") for i in range(3)]\n",
    "decomposer = MTNMF(n_abs, max_iter=5000, tol=0.0001)\n",
    "\n",
    "states = []\n",
    "ax = np.linspace(-1, 1, n_bins)\n",
    "ay = np.linspace(-1, 1, n_bins)\n",
    "ax, ay = np.meshgrid(ax, ay)\n",
    "actions = np.stack([ax, ay], -1)\n",
    "\n",
    "experts = dict()\n",
    "\n",
    "for goal_idx, weight_path in expert_dict.items():\n",
    "    experts[goal_idx] = get_expert(weight_path, state_dim, action_dim)\n",
    "    for _ in range(n_trajs):\n",
    "        states.append(rollout(envs[goal_idx-1], experts[goal_idx], horizon=horizon))\n",
    "states = np.concatenate(states)\n",
    "print('states shape:', states.shape)\n",
    "    \n",
    "pvs = []\n",
    "    \n",
    "for goal_idx in expert_dict:\n",
    "    infos = {'task_id': [goal_idx-1] * len(states)}\n",
    "    mean = experts[goal_idx](states, infos)['mean'].cpu().detach().numpy()\n",
    "    std = experts[goal_idx].std.expand(len(states), *std.shape)\n",
    "    print(mean.shape, std.shape)\n",
    "    #pv = F.softmax(experts[goal_idx].get_logits(states, infos), dim=-1).cpu().detach().numpy()\n",
    "    pv = pv.reshape(pv.shape[0], -1)\n",
    "    pvs.append(pv)\n",
    "\n",
    "pvs = np.stack(pvs, 0)\n",
    "A, S, info = MTNMF(n_abs, max_iter=5000, l1_ratio=l1_ratio).fit(pvs.transpose(0, 2, 1))\n",
    "\n",
    "fsave(\n",
    "    dict(\n",
    "        abs=S.T,\n",
    "        policies=list(pvs.reshape(pvs.shape[0], pvs.shape[1], 2, -1)),\n",
    "        states=[states for _ in range(len(pvs))],\n",
    "        infos=list([[{'task_id': i} for _ in range(len(states))] for i in range(3)]),\n",
    "    ),\n",
    "    '../data/nmf_sample/reacher/cont.ng.{}'.format(n_abs),\n",
    "    'pkl',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.0\n",
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 1.,  2.,  3.],\n",
      "        [ 1.,  2.,  3.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "a = torch.Tensor([1, 2, 3])\n",
    "print(a.expand(3, 3))\n",
    "\n",
    "# (1, 2, 3)\n",
    "#probs = torch.Tensor([[[0.2, 0.3, 0.5], [0.1, 0.3, 0.6]]])\n",
    "#print(torch.nn.functional.softmax(probs, dim=2))\n",
    "# probs = torch.Tensor([[0.2, 0.3, 0.5]])\n",
    "# print(probs.shape[0])\n",
    "# log_probs = torch.log(probs)\n",
    "# print(log_probs)\n",
    "# dist = torch.distributions.Categorical(logits=log_probs)\n",
    "# action = dist.sample()\n",
    "# print(action)\n",
    "# log_prob = dist.log_prob(action)\n",
    "# print(log_prob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
