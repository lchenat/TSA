{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_rl.simple_grid.env import DiscreteGridWorld\n",
    "from deep_rl.network import *\n",
    "from deep_rl.utils import *\n",
    "from sklearn.decomposition import NMF\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def set_seed(t, r=None, p=None, c=None):\n",
    "    if r is None:\n",
    "        r = t\n",
    "    if p is None:\n",
    "        p = r\n",
    "    torch.manual_seed(t)\n",
    "    random.seed(r)\n",
    "    np.random.seed(p)\n",
    "    if c is not None:\n",
    "        torch.cuda.manual_seed(c)\n",
    "\n",
    "class GridDrawer:                           \n",
    "    def __init__(self, color_list):\n",
    "        self.color_list = np.asarray(color_list)\n",
    "\n",
    "    # input: a 2-d index matrix\n",
    "    # output: a 2-d rgb matrix\n",
    "    def draw(self, indices, repeat=16):\n",
    "        return np.uint8(255 * np.array(self.color_list[indices, :]).repeat(repeat, 0).repeat(repeat, 1))\n",
    "    \n",
    "# this is my color list\n",
    "color_map = dict([\n",
    "    #*[('grey-{}'.format(v), plt.cm.Greys(0.1 * v)) for v in range(1, 20)],\n",
    "    *[('purple-{}'.format(v), plt.cm.Purples(0.05 * v)) for v in range(1, 20)],\n",
    "    *[('blue-{}'.format(v), plt.cm.Blues(0.05 * v)) for v in range(1, 20)],\n",
    "    *[('green-{}'.format(v), plt.cm.Greens(0.05 * v)) for v in range(1, 20)],\n",
    "    *[('orange-{}'.format(v), plt.cm.Oranges(0.05 * v)) for v in range(1, 20)],\n",
    "    *[('red-{}'.format(v), plt.cm.Reds(0.05 * v)) for v in range(1, 20)],\n",
    "])\n",
    "\n",
    "def imshow(img):\n",
    "    display(Image.fromarray(np.asarray(img)))\n",
    "\n",
    "color_list = list(color_map.values())\n",
    "shuffle(color_list)\n",
    "color_list = [plt.cm.Greys(0.9)] + [plt.cm.Greys(0.5)] + color_list\n",
    "drawer = GridDrawer(color_list)\n",
    "\n",
    "# multitask NMF from: https://ieeexplore.ieee.org/document/6939673\n",
    "class MTNMF:\n",
    "    def __init__(self, n_components, l1_ratio=0.0, max_iter=200, tol=0.0001):\n",
    "        self.n_components = n_components\n",
    "        self.l1_ratio = l1_ratio\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "\n",
    "    def loss(self, X, A, S):\n",
    "        return 0.5 * ((X - np.matmul(A, S)) ** 2).sum() + self.l1_ratio * S.sum()\n",
    "        \n",
    "    # input: a stack of observed data X_1, ..., X_K\n",
    "    # output: S, A_1, ..., A_K\n",
    "    def fit(self, X):\n",
    "        K, N, M = X.shape\n",
    "        A = np.random.rand(K, N, self.n_components)\n",
    "        S = np.random.rand(self.n_components, M)\n",
    "        prev_loss = np.inf\n",
    "        cur_loss = None\n",
    "        for i in range(self.max_iter):\n",
    "            A_T = A.transpose(0, 2, 1)\n",
    "            new_S = S * (np.matmul(A_T, X).sum(0)) / (np.matmul(np.matmul(A_T, A), S).sum(0) + K * self.l1_ratio * np.ones((self.n_components, M)))\n",
    "            S = new_S\n",
    "            new_A = A * np.matmul(X, S.T) / np.matmul(np.matmul(A, S), S.T)\n",
    "            A = new_A\n",
    "            cur_loss = self.loss(X, A, S)\n",
    "            if i % 100 == 0: print('NMF loss:', cur_loss)\n",
    "            if abs(cur_loss - prev_loss) < self.tol: break\n",
    "            prev_loss = cur_loss # update loss\n",
    "        return A, S, {'loss': cur_loss, 'iter': i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_map = dict([\n",
    "    ('G', 0), # goal\n",
    "    ('#', 1),\n",
    "    *[(str(i), i + 2) for i in range(0, 100)],\n",
    "])\n",
    "\n",
    "def get_states(env):\n",
    "    # get the whole state space\n",
    "    states = []\n",
    "    for i in range(env.size[0]):\n",
    "        for j in range(env.size[1]):\n",
    "                if env.is_valid_loc((i, j)):\n",
    "                    states.append((i, j))\n",
    "    infos = {'task_id': [0] * len(states)} \n",
    "    return states, infos\n",
    "\n",
    "def get_img(env, abs_list):\n",
    "    indices = np.zeros(env.size, dtype=np.int64)\n",
    "    k = 0\n",
    "    for i in range(env.size[0]):\n",
    "        for j in range(env.size[1]):\n",
    "            if (i, j) == env.goal:\n",
    "                indices[i, j] = 0\n",
    "            elif env.map[i][j] == '#':\n",
    "                indices[i, j] = 1\n",
    "            else:\n",
    "                indices[i, j] = visualization_map[str(2 + abs_list[k])]\n",
    "                k += 1\n",
    "\n",
    "    img = drawer.draw(indices)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (1, 2), (1, 3), (1, 4), (1, 6), (1, 7), (1, 8), (1, 9), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 9), (3, 1), (3, 2), (3, 3), (3, 4), (3, 6), (3, 7), (3, 8), (3, 9), (4, 1), (4, 2), (4, 3), (4, 4), (4, 6), (4, 7), (4, 8), (4, 9), (5, 2), (5, 6), (5, 7), (5, 8), (5, 9), (6, 1), (6, 2), (6, 3), (6, 4), (6, 8), (7, 1), (7, 2), (7, 3), (7, 4), (7, 6), (7, 7), (7, 8), (7, 9), (8, 1), (8, 2), (8, 3), (8, 4), (8, 5), (8, 6), (8, 7), (8, 8), (8, 9), (9, 1), (9, 2), (9, 3), (9, 4), (9, 6), (9, 7), (9, 8)]\n",
      "n_states: 67\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALAAAACwCAYAAACvt+ReAAADAElEQVR4nO3cIW5VURRA0V9Sj+gECKK4WiRBMQEUBkNCGAChCoEj6JcaTB0hnQCqqcTWUdF0AhWMoMyhp8nNDmv58//Ny85VJ3dv27a7HUQ9Wn0AmBAwaQImTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZg0AZMmYNL2pz9wfXQzmv92+HY0//HqdOn/n5xdjObr32/q6eWT0bwbmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZtvA88tXqfd7wPezQbX37+ODcwaQImTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZg0AZMmYNIETNp4H3j1Puvqfdjp+7a7wwc5xn/LDUyagEkTMGkCJk3ApAmYNAGTJmDSBEyagEkTMGkCJk3ApAmYNAGTNt4HXr2POzXdZz65vBjNr/5+B+evRvO3L3890Enuxw1MmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkzbeB56+j/vh9YvR/MnZbB93us+72vh95l17n9sNTJqASRMwaQImTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZg0AZO2t23b3eQH/t4+e6iz3Mv0fdrp+7iPD/6M5ldbvY895QYmTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZg0AZMmYNIETJqASRvvA18f3YwOMN3HXW26Dzz9fnXT96XdwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBp+9Mf+HT4eTT//fz39AhLzd9HvhlNr96nnr7PPOUGJk3ApAmYNAGTJmDSBEyagEkTMGkCJk3ApAmYNAGTJmDSBEyagEkb7wOv9u7989H816svo/np+7a7+PvIP9/8GM0fHx+P5t3ApAmYNAGTJmDSBEyagEkTMGkCJk3ApAmYNAGTJmDSBEyagEkTMGl727bdrT4E3JcbmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAibtH1+SXGICM3t0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=176x176 at 0x7FA93A0A1F60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALAAAACwCAYAAACvt+ReAAADdklEQVR4nO3cMYsdZRiG4bNBULSVbRSiphAVrbaxEkmjVSCsYBVFqy0FA6exsnAhAUEYtlLsBFmFgEJEUqRKEQtRYqdRsAqmsEklrP8h78Jww3X1z5zhcPNVH7OzLMvJBqLOrP0CMCFg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQ9Mn3AH6/8Odpfefb96SuM3HqwO9r//ONXo/3BhfOj/Ycf/TraX/345dF+7f/PCUyagEkTMGkCJk3ApAmYNAGTJmDSBEyagEkTMGkCJk3ApAmYNAGTNr4PPHX57uej/fQ+8auP35vth/d5p566/fbwCXdG628+uTHan9sbzZ3AtAmYNAGTJmDSBEyagEkTMGkCJk3ApAmYNAGTJmDSBEyagEkTMGk7y7KcTB7w7/3nRy9w//UfRvu1PffLM6P99PvAa/v0jZdG+8cOPhvtncCkCZg0AZMmYNIETJqASRMwaQImTcCkCZg0AZMmYNIETJqASRMwaePvA2/fe3q0v3x3+gZta38f+daD3dF+bU5g0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQJmLTxfeCjazdG+yv778x+//jmaL+26X3ewy/+Hu0f/Xr2fd8Prt8Z7af9OIFJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0naWZTmZPOBg/7XTepeH8s/m7Gh/uL0+2p/buzfar+3gwvnRfnqfd8oJTJqASRMwaQImTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZg0AZM2/j7w9D7uk5u/Vt1fPXxhtD86nt0HXvs+9Wbz32j9+0+7o/30PrUTmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZtfB/43W9/O433eGhfXnxxtJ/eJ35r/9Jovxn+/trfR177PrUTmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZtZ1mWk8kDvj+zd1rvsorvLj4x2h8d3xzt1/8+8MzZvTdH++12O9o7gUkTMGkCJk3ApAmYNAGTJmDSBEyagEkTMGkCJk3ApAmYNAGTJmDSxveBYU1OYNIETJqASRMwaQImTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZi0/wE9YWouwRFOsQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=176x176 at 0x7FA93A0A1E10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_abs = 10\n",
    "l1_ratio = 0.0\n",
    "state_dim = 2\n",
    "n_action = 4\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "env = DiscreteGridWorld('fourroom', (1, 1), (9, 9))\n",
    "decomposer = NMF(n_abs, solver='mu', random_state=0, max_iter=5000, l1_ratio=l1_ratio)\n",
    "\n",
    "expert = CategoricalActorCriticNet(\n",
    "    1,\n",
    "    state_dim,\n",
    "    n_action,\n",
    "    FCBody(\n",
    "        state_dim, \n",
    "        hidden_units=(16,)\n",
    "    ),  \n",
    ")\n",
    "# load weight\n",
    "weight_dict = expert.state_dict()\n",
    "loaded_weight_dict = {k: v for k, v in torch.load(\n",
    "    'log/grid.fourroom.9_9/fc_discrete.baseline/_/0.190228-203237/models/step-128000-mean-1.00',\n",
    "    map_location=lambda storage, loc: storage)['network'].items()\n",
    "    if k in weight_dict}\n",
    "weight_dict.update(loaded_weight_dict)\n",
    "expert.load_state_dict(weight_dict)\n",
    "\n",
    "\n",
    "states, infos = get_states(env)                \n",
    "print(states)\n",
    "print('n_states:', len(states))\n",
    "                \n",
    "actions = expert(np.array(states), infos)['a'] # maybe take the whole distribution\n",
    "\n",
    "img = get_img(env, actions.cpu().detach().numpy())\n",
    "imshow(img)\n",
    "\n",
    "pvs = F.softmax(expert.get_logits(np.array(states), infos), dim=1).cpu().detach().numpy()\n",
    "#pvs = one_hot.encode(actions, n_action).cpu().detach().numpy()\n",
    "abs_mat = decomposer.fit_transform(pvs)\n",
    "policy_mat = decomposer.components_\n",
    "\n",
    "abs_list = np.array(abs_mat).argmax(1)\n",
    "#print(abs_list)\n",
    "abs_map = {s: i for s, i in zip(states, abs_mat)}\n",
    "#print(abs_map)\n",
    "fsave(abs_map, 'data/abs/grid/fourroom/single.{}.l1-{}'.format(n_abs, l1_ratio), 'pkl')\n",
    "\n",
    "img = get_img(env, abs_list)\n",
    "imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiTask NMF (shared states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALAAAACwCAYAAACvt+ReAAADNklEQVR4nO3cMavWZRzHYQ0HwcFAaWnJhBwCcTCiqakpWqIhp4YCkaBBFM4bcHILHg6CFbkU2BK1mYOj5BBBhEraooMg6OYQnd7D+R24+dB17d/nfw8f7unm2b/ZbHb2QdRLqw8AEwImTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZg0AZMmYNIETNqB6Q+ceu/MaH/p5v3pEZZ6/987o/2Dk3+P9pePfTrar7b9483R3g1MmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkzZ+Dzz184eHln7/4r1vR/sHe3SO3br48KvFJ5h5fd9ro70bmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZt+Xvg1S6/8clov/3DrdH++dMTo/1nZ98e7Ve7/vu10d4NTJqASRMwaQImTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZg0AZP2v38PXHf1yu3VRxg5fGS2dwOTJmDSBEyagEkTMGkCJk3ApAmYNAGTJmDSBEyagEkTMGkCJk3ApI3fA/9247vR/p2P3h3tp//PS5sbmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZt/B743PA979SL7S+Wfv/guS9H+8NH7u7RSXbn+dMTo/3q87uBSRMwaQImTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZg0AZMmYNLG74H/OPBitH/zn4Oj/aO3vh/tX/3149F+6q87ryz9/tFjs/3q98RuYNIETJqASRMwaQImTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZi08XvgqQtbfy79/uPt86P9ztdr3/NOffD5y6P9T5tne3KO3XIDkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQtfw9cd/z0k9F+9f8Df3Ph4Wh//ZfZe+qtra3R3g1MmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBk7Z/s9nsrD4E7JYbmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAibtP5++Sd7jCwKXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=176x176 at 0x7FA91F5FD4A8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_abs = 10\n",
    "l1_ratio=0.0 # this is currently not working... since alpha is not set\n",
    "state_dim = 2\n",
    "n_action = 4\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "expert_dict = {\n",
    "    (1, 1): 'log/grid.fourroom.1_1.md-1.T-100/fc_discrete.baseline/_/0.190303-160012/models/step-160000-mean-1.00',\n",
    "    (9, 1): 'log/grid.fourroom.9_1.md-1.T-100/fc_discrete.baseline/_/0.190303-160151/models/step-160000-mean-1.00',\n",
    "    (1, 9): 'log/grid.fourroom.1_9.md-1.T-100/fc_discrete.baseline/_/0.190303-160103/models/step-160000-mean-1.00',\n",
    "}\n",
    "env = DiscreteGridWorld('fourroom', (1, 1), (9, 9))\n",
    "decomposer = NMF(n_abs, solver='mu', random_state=0, max_iter=5000, l1_ratio=l1_ratio)\n",
    "#decomposer = NMF(n_abs, solver='mu', beta_loss='kullback-leibler', random_state=0, max_iter=5000, l1_ratio=l1_ratio)\n",
    "states, infos = get_states(env)\n",
    "states.append(env.goal)\n",
    "infos['task_id'].append(0)\n",
    "pvs = []\n",
    "\n",
    "for goal_loc, weight_path in expert_dict.items():\n",
    "    expert = CategoricalActorCriticNet(\n",
    "        1,\n",
    "        state_dim,\n",
    "        n_action,\n",
    "        FCBody(\n",
    "            state_dim, \n",
    "            hidden_units=(16,)\n",
    "        ),  \n",
    "    )\n",
    "    # load weight\n",
    "    weight_dict = expert.state_dict()\n",
    "    loaded_weight_dict = {k: v for k, v in torch.load(\n",
    "        weight_path,\n",
    "        map_location=lambda storage, loc: storage)['network'].items()\n",
    "        if k in weight_dict}\n",
    "    weight_dict.update(loaded_weight_dict)\n",
    "    expert.load_state_dict(weight_dict)\n",
    "    pvs.append(F.softmax(expert.get_logits(np.array(states), infos), dim=1).cpu().detach().numpy())\n",
    "\n",
    "pvs = np.concatenate(pvs, 1)\n",
    "abs_mat = decomposer.fit_transform(pvs)\n",
    "policy_mat = decomposer.components_\n",
    "\n",
    "abs_list = np.array(abs_mat).argmax(1) # argmax moved to algo as an option\n",
    "#print(abs_list)\n",
    "abs_map = {s: i for s, i in zip(states, abs_mat)}\n",
    "#print(abs_map)\n",
    "fsave(abs_map, 'data/abs/grid/fourroom/MTS.{}.l1-{}'.format(n_abs, l1_ratio), 'pkl')\n",
    "\n",
    "img = get_img(env, abs_list)\n",
    "imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiTask NMF (does not shared states) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF loss: 20.61729075229367\n",
      "NMF loss: 0.4947668717188714\n",
      "NMF loss: 0.368044002928282\n",
      "NMF loss: 0.33021151577011265\n",
      "NMF loss: 0.3025160413477864\n",
      "NMF loss: 0.2818449586101335\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALAAAACwCAYAAACvt+ReAAADSklEQVR4nO3csYvXdRzH8fMInOSCw38gwdDBHA6KCB2OaDgkiLOc1FVulu4viAMDpx/i4FCTXBKECDU4JBEKbU62tEoilM6m/0Pvgx9PfDz21/f3GZ58pg+/Q4vF4vUKRK0u+wAwIWDSBEyagEkTMGkCJk3ApAmYNAGTJmDSBEyagEkTMGkCJk3ApL0z/cD57YsHcY6sH+58P9pf+XzzgE7SdOOn+6O9G5g0AZMmYNIETJqASRMwaQImTcCkCZg0AZMmYNIETJqASRMwaQImbfweeG/359H+270To/3VP78b7cdOzeZX/7o12l8++dXsAHFuYNIETJqASRMwaQImTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZi08XvgZbt2/NJov3fz6Wi/tv5ktL+3ujHar5yczevcwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBp+ffAb7uv918t+wgjW8O9G5g0AZMmYNIETJqASRMwaQImTcCkCZg0AZMmYNIETJqASRMwaQImbfwe+NjG38MvnBitb9z5dbRfWx/Nxz559nK0/+3okQM6SZMbmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZt/B74yvbZ0X7v5tPR/tzOh6P91IPbj0b7tfUno/3Wf6P5yr3VjeHv/zE7wJAbmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZt/B74+uYHsw98+cv0CGn/Pn9/tD+8/9nsADu/j+bLfk/sBiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJG78HXra7i39G+3M77472Zy7M/p/4xfP3RvuH+6P5yubi49H+/vA98ZQbmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiYt/x542R7cfjTan/509h74ox+fjfYPvzg62j/+Znu039rdHe3dwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBphxaLxetlHwL+LzcwaQImTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZg0AZMmYNIETNobcElKj99o8HsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=176x176 at 0x7FA91F5FDA90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_abs = 10\n",
    "l1_ratio=0.0\n",
    "state_dim = 2\n",
    "n_action = 4\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "expert_dict = {\n",
    "    (1, 1): 'log/grid.fourroom.1_1.md-1.T-100/fc_discrete.baseline/_/0.190303-160012/models/step-160000-mean-1.00',\n",
    "    (9, 1): 'log/grid.fourroom.9_1.md-1.T-100/fc_discrete.baseline/_/0.190303-160151/models/step-160000-mean-1.00',\n",
    "    (1, 9): 'log/grid.fourroom.1_9.md-1.T-100/fc_discrete.baseline/_/0.190303-160103/models/step-160000-mean-1.00',\n",
    "}\n",
    "env = DiscreteGridWorld('fourroom', (1, 1), (9, 9))\n",
    "decomposer = MTNMF(n_abs, max_iter=5000, tol=0.0001)\n",
    "states, infos = get_states(env)\n",
    "states.append(env.goal)\n",
    "infos['task_id'].append(0)\n",
    "pvs = []\n",
    "\n",
    "for goal_loc, weight_path in expert_dict.items():\n",
    "    expert = CategoricalActorCriticNet(\n",
    "        1,\n",
    "        state_dim,\n",
    "        n_action,\n",
    "        FCBody(\n",
    "            state_dim, \n",
    "            hidden_units=(16,)\n",
    "        ),  \n",
    "    )\n",
    "    # load weight\n",
    "    weight_dict = expert.state_dict()\n",
    "    loaded_weight_dict = {k: v for k, v in torch.load(\n",
    "        weight_path,\n",
    "        map_location=lambda storage, loc: storage)['network'].items()\n",
    "        if k in weight_dict}\n",
    "    weight_dict.update(loaded_weight_dict)\n",
    "    expert.load_state_dict(weight_dict)\n",
    "    pvs.append(F.softmax(expert.get_logits(np.array(states), infos), dim=1).cpu().detach().numpy())\n",
    "\n",
    "pvs = np.stack(pvs, 0)\n",
    "A, S, info = MTNMF(n_abs, max_iter=5000, l1_ratio=l1_ratio).fit(pvs.transpose(0, 2, 1))\n",
    "\n",
    "abs_list = S.T.argmax(1)\n",
    "abs_map = {s: i for s, i in zip(states, abs_mat)}\n",
    "fsave(abs_map, 'data/abs/grid/fourroom/MT.{}.l1-{}'.format(n_abs, l1_ratio), 'pkl')\n",
    "\n",
    "# save nmf_smaple\n",
    "fsave(\n",
    "    dict(\n",
    "        abs=S.T,\n",
    "        policies=list(pvs),\n",
    "        states=[states for _ in range(3)],\n",
    "        infos=list([[{'task_id': i} for _ in range(len(states))] for i in range(3)]),\n",
    "    ),\n",
    "    'data/nmf_sample/grid/fourroom/{}'.format(n_abs),\n",
    "    'pkl',\n",
    ")\n",
    "\n",
    "img = get_img(env, abs_list)\n",
    "imshow(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
