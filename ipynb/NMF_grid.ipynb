{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from deep_rl.simple_grid.env import DiscreteGridWorld\n",
    "from deep_rl.network import *\n",
    "from deep_rl.utils import *\n",
    "from sklearn.decomposition import NMF\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def set_seed(t, r=None, p=None, c=None):\n",
    "    if r is None:\n",
    "        r = t\n",
    "    if p is None:\n",
    "        p = r\n",
    "    torch.manual_seed(t)\n",
    "    random.seed(r)\n",
    "    np.random.seed(p)\n",
    "    if c is not None:\n",
    "        torch.cuda.manual_seed(c)\n",
    "\n",
    "class GridDrawer:                           \n",
    "    def __init__(self, color_list):\n",
    "        self.color_list = np.asarray(color_list)\n",
    "\n",
    "    # input: a 2-d index matrix\n",
    "    # output: a 2-d rgb matrix\n",
    "    def draw(self, indices, repeat=16):\n",
    "        return np.uint8(255 * np.array(self.color_list[indices, :]).repeat(repeat, 0).repeat(repeat, 1))\n",
    "    \n",
    "# this is my color list\n",
    "color_map = dict([\n",
    "    #*[('grey-{}'.format(v), plt.cm.Greys(0.1 * v)) for v in range(1, 20)],\n",
    "    *[('purple-{}'.format(v), plt.cm.Purples(0.05 * v)) for v in range(1, 20)],\n",
    "    *[('blue-{}'.format(v), plt.cm.Blues(0.05 * v)) for v in range(1, 20)],\n",
    "    *[('green-{}'.format(v), plt.cm.Greens(0.05 * v)) for v in range(1, 20)],\n",
    "    *[('orange-{}'.format(v), plt.cm.Oranges(0.05 * v)) for v in range(1, 20)],\n",
    "    *[('red-{}'.format(v), plt.cm.Reds(0.05 * v)) for v in range(1, 20)],\n",
    "])\n",
    "\n",
    "def imshow(img):\n",
    "    display(Image.fromarray(np.asarray(img)))\n",
    "\n",
    "color_list = list(color_map.values())\n",
    "shuffle(color_list)\n",
    "color_list = [plt.cm.Greys(0.9)] + [plt.cm.Greys(0.5)] + color_list\n",
    "drawer = GridDrawer(color_list)\n",
    "\n",
    "# multitask NMF from: https://ieeexplore.ieee.org/document/6939673\n",
    "class MTNMF:\n",
    "    def __init__(self, n_components, l1_ratio=0.0, max_iter=200, tol=0.0001):\n",
    "        self.n_components = n_components\n",
    "        self.l1_ratio = l1_ratio\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "\n",
    "    def loss(self, X, A, S):\n",
    "        return 0.5 * ((X - np.matmul(A, S)) ** 2).sum() + self.l1_ratio * S.sum()\n",
    "        \n",
    "    # input: a stack of observed data X_1, ..., X_K\n",
    "    # output: S, A_1, ..., A_K\n",
    "    def fit(self, X):\n",
    "        K, N, M = X.shape\n",
    "        A = np.random.rand(K, N, self.n_components)\n",
    "        S = np.random.rand(self.n_components, M)\n",
    "        prev_loss = np.inf\n",
    "        cur_loss = None\n",
    "        for i in range(self.max_iter):\n",
    "            A_T = A.transpose(0, 2, 1)\n",
    "            new_S = S * (np.matmul(A_T, X).sum(0)) / (np.matmul(np.matmul(A_T, A), S).sum(0) + K * self.l1_ratio * np.ones((self.n_components, M)))\n",
    "            S = new_S\n",
    "            new_A = A * np.matmul(X, S.T) / np.matmul(np.matmul(A, S), S.T)\n",
    "            A = new_A\n",
    "            cur_loss = self.loss(X, A, S)\n",
    "            if i % 100 == 0: print('NMF loss:', cur_loss)\n",
    "            if abs(cur_loss - prev_loss) < self.tol: break\n",
    "            prev_loss = cur_loss # update loss\n",
    "        return A, S, {'loss': cur_loss, 'iter': i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_map = dict([\n",
    "    ('G', 0), # goal\n",
    "    ('#', 1),\n",
    "    *[(str(i), i + 2) for i in range(0, 100)],\n",
    "])\n",
    "\n",
    "def get_states(env):\n",
    "    # get the whole state space\n",
    "    states = []\n",
    "    for i in range(env.size[0]):\n",
    "        for j in range(env.size[1]):\n",
    "                if env.is_valid_loc((i, j)):\n",
    "                    states.append((i, j))\n",
    "    infos = {'task_id': [0] * len(states)} \n",
    "    return states, infos\n",
    "\n",
    "def get_img(env, abs_list):\n",
    "    indices = np.zeros(env.size, dtype=np.int64)\n",
    "    k = 0\n",
    "    for i in range(env.size[0]):\n",
    "        for j in range(env.size[1]):\n",
    "            if (i, j) == env.goal:\n",
    "                indices[i, j] = 0\n",
    "            elif env.map[i][j] == '#':\n",
    "                indices[i, j] = 1\n",
    "            else:\n",
    "                indices[i, j] = visualization_map[str(2 + abs_list[k])]\n",
    "                k += 1\n",
    "\n",
    "    img = drawer.draw(indices)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (1, 2), (1, 3), (1, 4), (1, 6), (1, 7), (1, 8), (1, 9), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 9), (3, 1), (3, 2), (3, 3), (3, 4), (3, 6), (3, 7), (3, 8), (3, 9), (4, 1), (4, 2), (4, 3), (4, 4), (4, 6), (4, 7), (4, 8), (4, 9), (5, 2), (5, 6), (5, 7), (5, 8), (5, 9), (6, 1), (6, 2), (6, 3), (6, 4), (6, 8), (7, 1), (7, 2), (7, 3), (7, 4), (7, 6), (7, 7), (7, 8), (7, 9), (8, 1), (8, 2), (8, 3), (8, 4), (8, 5), (8, 6), (8, 7), (8, 8), (8, 9), (9, 1), (9, 2), (9, 3), (9, 4), (9, 6), (9, 7), (9, 8)]\n",
      "n_states: 67\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALAAAACwCAYAAACvt+ReAAADAElEQVR4nO3cIW5VURRA0V9Sj+gECKK4WiRBMQEUBkNCGAChCoEj6JcaTB0hnQCqqcTWUdF0AhWMoMyhp8nNDmv58//Ny85VJ3dv27a7HUQ9Wn0AmBAwaQImTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZg0AZMmYNL2pz9wfXQzmv92+HY0//HqdOn/n5xdjObr32/q6eWT0bwbmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZtvA88tXqfd7wPezQbX37+ODcwaQImTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZg0AZMmYNIETNp4H3j1Puvqfdjp+7a7wwc5xn/LDUyagEkTMGkCJk3ApAmYNAGTJmDSBEyagEkTMGkCJk3ApAmYNAGTNt4HXr2POzXdZz65vBjNr/5+B+evRvO3L3890Enuxw1MmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkzbeB56+j/vh9YvR/MnZbB93us+72vh95l17n9sNTJqASRMwaQImTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZg0AZO2t23b3eQH/t4+e6iz3Mv0fdrp+7iPD/6M5ldbvY895QYmTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZg0AZMmYNIETJqASRvvA18f3YwOMN3HXW26Dzz9fnXT96XdwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBp+9Mf+HT4eTT//fz39AhLzd9HvhlNr96nnr7PPOUGJk3ApAmYNAGTJmDSBEyagEkTMGkCJk3ApAmYNAGTJmDSBEyagEkb7wOv9u7989H816svo/np+7a7+PvIP9/8GM0fHx+P5t3ApAmYNAGTJmDSBEyagEkTMGkCJk3ApAmYNAGTJmDSBEyagEkTMGl727bdrT4E3JcbmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAibtH1+SXGICM3t0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=176x176 at 0x7FA93A0A1F60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALAAAACwCAYAAACvt+ReAAADdklEQVR4nO3cMYsdZRiG4bNBULSVbRSiphAVrbaxEkmjVSCsYBVFqy0FA6exsnAhAUEYtlLsBFmFgEJEUqRKEQtRYqdRsAqmsEklrP8h78Jww3X1z5zhcPNVH7OzLMvJBqLOrP0CMCFg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQ9Mn3AH6/8Odpfefb96SuM3HqwO9r//ONXo/3BhfOj/Ycf/TraX/345dF+7f/PCUyagEkTMGkCJk3ApAmYNAGTJmDSBEyagEkTMGkCJk3ApAmYNAGTNr4PPHX57uej/fQ+8auP35vth/d5p566/fbwCXdG628+uTHan9sbzZ3AtAmYNAGTJmDSBEyagEkTMGkCJk3ApAmYNAGTJmDSBEyagEkTMGk7y7KcTB7w7/3nRy9w//UfRvu1PffLM6P99PvAa/v0jZdG+8cOPhvtncCkCZg0AZMmYNIETJqASRMwaQImTcCkCZg0AZMmYNIETJqASRMwaePvA2/fe3q0v3x3+gZta38f+daD3dF+bU5g0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQJmLTxfeCjazdG+yv778x+//jmaL+26X3ewy/+Hu0f/Xr2fd8Prt8Z7af9OIFJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0naWZTmZPOBg/7XTepeH8s/m7Gh/uL0+2p/buzfar+3gwvnRfnqfd8oJTJqASRMwaQImTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZg0AZM2/j7w9D7uk5u/Vt1fPXxhtD86nt0HXvs+9Wbz32j9+0+7o/30PrUTmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZtfB/43W9/O433eGhfXnxxtJ/eJ35r/9Jovxn+/trfR177PrUTmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZtZ1mWk8kDvj+zd1rvsorvLj4x2h8d3xzt1/8+8MzZvTdH++12O9o7gUkTMGkCJk3ApAmYNAGTJmDSBEyagEkTMGkCJk3ApAmYNAGTJmDSxveBYU1OYNIETJqASRMwaQImTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZi0/wE9YWouwRFOsQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=176x176 at 0x7FA93A0A1E10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_abs = 10\n",
    "l1_ratio = 0.0\n",
    "state_dim = 2\n",
    "n_action = 4\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "env = DiscreteGridWorld('fourroom', (1, 1), (9, 9))\n",
    "decomposer = NMF(n_abs, solver='mu', random_state=0, max_iter=5000, l1_ratio=l1_ratio)\n",
    "\n",
    "expert = CategoricalActorCriticNet(\n",
    "    1,\n",
    "    state_dim,\n",
    "    n_action,\n",
    "    FCBody(\n",
    "        state_dim, \n",
    "        hidden_units=(16,)\n",
    "    ),  \n",
    ")\n",
    "# load weight\n",
    "weight_dict = expert.state_dict()\n",
    "loaded_weight_dict = {k: v for k, v in torch.load(\n",
    "    'log/grid.fourroom.9_9/fc_discrete.baseline/_/0.190228-203237/models/step-128000-mean-1.00',\n",
    "    map_location=lambda storage, loc: storage)['network'].items()\n",
    "    if k in weight_dict}\n",
    "weight_dict.update(loaded_weight_dict)\n",
    "expert.load_state_dict(weight_dict)\n",
    "\n",
    "\n",
    "states, infos = get_states(env)                \n",
    "print(states)\n",
    "print('n_states:', len(states))\n",
    "                \n",
    "actions = expert(np.array(states), infos)['a'] # maybe take the whole distribution\n",
    "\n",
    "img = get_img(env, actions.cpu().detach().numpy())\n",
    "imshow(img)\n",
    "\n",
    "pvs = F.softmax(expert.get_logits(np.array(states), infos), dim=1).cpu().detach().numpy()\n",
    "#pvs = one_hot.encode(actions, n_action).cpu().detach().numpy()\n",
    "abs_mat = decomposer.fit_transform(pvs)\n",
    "policy_mat = decomposer.components_\n",
    "\n",
    "abs_list = np.array(abs_mat).argmax(1)\n",
    "#print(abs_list)\n",
    "abs_map = {s: i for s, i in zip(states, abs_mat)}\n",
    "#print(abs_map)\n",
    "fsave(abs_map, 'data/abs/grid/fourroom/single.{}.l1-{}'.format(n_abs, l1_ratio), 'pkl')\n",
    "\n",
    "img = get_img(env, abs_list)\n",
    "imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiTask NMF (shared states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liyuc/anaconda3/envs/tsa/lib/python3.6/site-packages/sklearn/decomposition/nmf.py:1050: ConvergenceWarning: Maximum number of iteration 5000 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALAAAACwCAYAAACvt+ReAAACV0lEQVR4nO3csW3DQBAAQdNQOW7GuTM14UY+dKZc9bgGF0H14BPwWGgmJ/kgFhcd/lhrnW8Q9b77ADAhYNIETJqASRMwaQImTcCkCZg0AZMmYNIETJqASRMwaQImTcCkXaYv+Lr/POMcWbfP6+h5/2/2/0xg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQJmLTxPvCre/V93t1MYNIETJqASRMwaQImTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZg0+8BD7gfeywQmTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZg0AZMmYNIETJqASRvfD7z7ftzp92kzgUkTMGkCJk3ApAmYNAGTJmDSBEyagEkTMGkCJk3ApAmYNAGTJmDSjrXWOXnBdJ+3rr6PXN/HNoFJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0sb3AzNT36fevU9sApMmYNIETJqASRMwaQImTcCkCZg0AZMmYNIETJqASRMwaQImTcCk2Qcequ/z1pnApAmYNAGTJmDSBEyagEkTMGkCJk3ApAmYNAGTJmDSBEyagEkTMGn2gYem99vW94k/fv9Gz38Pv28CkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQda61z9yHgv0xg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQJmLQH8NsqXgFN63MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=176x176 at 0x7FBF2BF7AD30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_abs = 4\n",
    "l1_ratio=0.0 # this is currently not working... since alpha is not set\n",
    "state_dim = 2\n",
    "n_action = 4\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "expert_dict = {\n",
    "    (1, 1): '../log/grid.fourroom.1_1.md-1.T-100/fc_discrete.baseline/_/0.190303-160012/models/step-160000-mean-1.00',\n",
    "    (9, 1): '../log/grid.fourroom.9_1.md-1.T-100/fc_discrete.baseline/_/0.190303-160151/models/step-160000-mean-1.00',\n",
    "    (1, 9): '../log/grid.fourroom.1_9.md-1.T-100/fc_discrete.baseline/_/0.190303-160103/models/step-160000-mean-1.00',\n",
    "}\n",
    "env = DiscreteGridWorld('fourroom', (1, 1), (9, 9))\n",
    "decomposer = NMF(n_abs, solver='mu', random_state=0, max_iter=5000, l1_ratio=l1_ratio)\n",
    "#decomposer = NMF(n_abs, solver='mu', beta_loss='kullback-leibler', random_state=0, max_iter=5000, l1_ratio=l1_ratio)\n",
    "states, infos = get_states(env)\n",
    "states.append(env.goal)\n",
    "infos['task_id'].append(0)\n",
    "pvs = []\n",
    "\n",
    "for goal_loc, weight_path in expert_dict.items():\n",
    "    expert = CategoricalActorCriticNet(\n",
    "        1,\n",
    "        state_dim,\n",
    "        n_action,\n",
    "        FCBody(\n",
    "            state_dim, \n",
    "            hidden_units=(16,)\n",
    "        ),  \n",
    "    )\n",
    "    # load weight\n",
    "    weight_dict = expert.state_dict()\n",
    "    loaded_weight_dict = {k: v for k, v in torch.load(\n",
    "        weight_path,\n",
    "        map_location=lambda storage, loc: storage)['network'].items()\n",
    "        if k in weight_dict}\n",
    "    weight_dict.update(loaded_weight_dict)\n",
    "    expert.load_state_dict(weight_dict)\n",
    "    pvs.append(F.softmax(expert.get_logits(np.array(states), infos), dim=1).cpu().detach().numpy())\n",
    "\n",
    "pvs = np.concatenate(pvs, 1)\n",
    "abs_mat = decomposer.fit_transform(pvs)\n",
    "policy_mat = decomposer.components_\n",
    "\n",
    "abs_list = np.array(abs_mat).argmax(1) # argmax moved to algo as an option\n",
    "#print(abs_list)\n",
    "abs_map = {s: i for s, i in zip(states, abs_mat)}\n",
    "#print(abs_map)\n",
    "fsave(abs_map, 'data/abs/grid/fourroom/MTS.{}.l1-{}'.format(n_abs, l1_ratio), 'pkl')\n",
    "\n",
    "img = get_img(env, abs_list)\n",
    "imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiTask NMF (does not shared states) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../log/grid.fourroom.1_1.md-1.T-100/fc_discrete.baseline/_/0.190303-160012/models/step-160000-mean-1.00'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-835bf3223769>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m#weight_dict.update(loaded_weight_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m#expert.load_state_dict(weight_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mexpert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'network'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mpvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tsa/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../log/grid.fourroom.1_1.md-1.T-100/fc_discrete.baseline/_/0.190303-160012/models/step-160000-mean-1.00'"
     ]
    }
   ],
   "source": [
    "n_abs = 4\n",
    "l1_ratio=0.0\n",
    "state_dim = 2\n",
    "n_action = 4\n",
    "\n",
    "set_seed(1)\n",
    "\n",
    "expert_dict = {\n",
    "    (1, 1): '../log/grid.fourroom.1_1.md-1.T-100/fc_discrete.baseline/_/0.190303-160012/models/step-160000-mean-1.00',\n",
    "    (9, 1): '../log/grid.fourroom.9_1.md-1.T-100/fc_discrete.baseline/_/0.190303-160151/models/step-160000-mean-1.00',\n",
    "    (1, 9): '../log/grid.fourroom.1_9.md-1.T-100/fc_discrete.baseline/_/0.190303-160103/models/step-160000-mean-1.00',\n",
    "}\n",
    "env = DiscreteGridWorld('fourroom', (1, 1), (9, 9))\n",
    "decomposer = MTNMF(n_abs, max_iter=5000, tol=0.0001)\n",
    "states, infos = get_states(env)\n",
    "states.append(env.goal)\n",
    "infos['task_id'].append(0)\n",
    "pvs = []\n",
    "\n",
    "for goal_loc, weight_path in expert_dict.items():\n",
    "    expert = CategoricalActorCriticNet(\n",
    "        1,\n",
    "        state_dim,\n",
    "        n_action,\n",
    "        FCBody(\n",
    "            state_dim, \n",
    "            hidden_units=(16,) # this have changed...\n",
    "        ),  \n",
    "    )\n",
    "    # load weight\n",
    "    #weight_dict = expert.state_dict()\n",
    "    #loaded_weight_dict = {k: v for k, v in torch.load(\n",
    "    #    weight_path,\n",
    "    #    map_location=lambda storage, loc: storage)['network'].items()\n",
    "    #    if k in weight_dict}\n",
    "    #weight_dict.update(loaded_weight_dict)\n",
    "    #expert.load_state_dict(weight_dict)\n",
    "    expert.load_state_dict(torch.load(weight_path, map_location=lambda storage, loc: storage)['network'])\n",
    "    pvs.append(F.softmax(expert.get_logits(np.array(states), infos), dim=1).cpu().detach().numpy())\n",
    "\n",
    "pvs = np.stack(pvs, 0)\n",
    "A, S, info = MTNMF(n_abs, max_iter=5000, l1_ratio=l1_ratio).fit(pvs.transpose(0, 2, 1))\n",
    "\n",
    "abs_list = S.T.argmax(1)\n",
    "#abs_map = {s: i for s, i in zip(states, abs_mat)}\n",
    "#fsave(abs_map, 'data/abs/grid/fourroom/MT.{}.l1-{}'.format(n_abs, l1_ratio), 'pkl')\n",
    "\n",
    "# save nmf_smaple\n",
    "# fsave(\n",
    "#     dict(\n",
    "#         abs=S.T,\n",
    "#         policies=list(pvs),\n",
    "#         states=[states for _ in range(3)],\n",
    "#         infos=list([[{'task_id': i} for _ in range(len(states))] for i in range(3)]),\n",
    "#     ),\n",
    "#     'data/nmf_sample/grid/fourroom/{}'.format(n_abs),\n",
    "#     'pkl',\n",
    "# )\n",
    "\n",
    "img = get_img(env, abs_list)\n",
    "imshow(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
