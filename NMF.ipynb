{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_rl.simple_grid.env import DiscreteGridWorld\n",
    "from deep_rl.network import *\n",
    "from deep_rl.utils import *\n",
    "from sklearn.decomposition import NMF\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def set_seed(t, r=None, p=None, c=None):\n",
    "    if r is None:\n",
    "        r = t\n",
    "    if p is None:\n",
    "        p = r\n",
    "    torch.manual_seed(t)\n",
    "    random.seed(r)\n",
    "    np.random.seed(p)\n",
    "    if c is not None:\n",
    "        torch.cuda.manual_seed(c)\n",
    "\n",
    "class GridDrawer:                           \n",
    "    def __init__(self, color_list):\n",
    "        self.color_list = np.asarray(color_list)\n",
    "\n",
    "    # input: a 2-d index matrix\n",
    "    # output: a 2-d rgb matrix\n",
    "    def draw(self, indices, repeat=16):\n",
    "        return np.uint8(255 * np.array(self.color_list[indices, :]).repeat(repeat, 0).repeat(repeat, 1))\n",
    "    \n",
    "# this is my color list\n",
    "color_map = dict([\n",
    "    #*[('grey-{}'.format(v), plt.cm.Greys(0.1 * v)) for v in range(1, 20)],\n",
    "    *[('purple-{}'.format(v), plt.cm.Purples(0.05 * v)) for v in range(1, 20)],\n",
    "    *[('blue-{}'.format(v), plt.cm.Blues(0.05 * v)) for v in range(1, 20)],\n",
    "    *[('green-{}'.format(v), plt.cm.Greens(0.05 * v)) for v in range(1, 20)],\n",
    "    *[('orange-{}'.format(v), plt.cm.Oranges(0.05 * v)) for v in range(1, 20)],\n",
    "    *[('red-{}'.format(v), plt.cm.Reds(0.05 * v)) for v in range(1, 20)],\n",
    "])\n",
    "\n",
    "def imshow(img):\n",
    "    display(Image.fromarray(np.asarray(img)))\n",
    "\n",
    "color_list = list(color_map.values())\n",
    "shuffle(color_list)\n",
    "color_list = [plt.cm.Greys(0.9)] + [plt.cm.Greys(0.5)] + color_list\n",
    "drawer = GridDrawer(color_list)\n",
    "\n",
    "# multitask NMF from: https://ieeexplore.ieee.org/document/6939673\n",
    "class MTNMF:\n",
    "    def __init__(self, n_components, l1_ratio=0.0, max_iter=200, tol=0.0001):\n",
    "        self.n_components = n_components\n",
    "        self.l1_ratio = l1_ratio\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "\n",
    "    def loss(self, X, A, S):\n",
    "        return 0.5 * ((X - np.matmul(A, S)) ** 2).sum() + self.l1_ratio * S.sum()\n",
    "        \n",
    "    # input: a stack of observed data X_1, ..., X_K\n",
    "    # output: S, A_1, ..., A_K\n",
    "    def fit(self, X):\n",
    "        K, N, M = X.shape\n",
    "        A = np.random.rand(K, N, self.n_components)\n",
    "        S = np.random.rand(self.n_components, M)\n",
    "        prev_loss = np.inf\n",
    "        cur_loss = None\n",
    "        for i in range(self.max_iter):\n",
    "            A_T = A.transpose(0, 2, 1)\n",
    "            new_S = S * (np.matmul(A_T, X).sum(0)) / (np.matmul(np.matmul(A_T, A), S).sum(0) + K * self.l1_ratio * np.ones((self.n_components, M)))\n",
    "            S = new_S\n",
    "            new_A = A * np.matmul(X, S.T) / np.matmul(np.matmul(A, S), S.T)\n",
    "            A = new_A\n",
    "            cur_loss = self.loss(X, A, S)\n",
    "            if i % 100 == 0: print('NMF loss:', cur_loss)\n",
    "            if abs(cur_loss - prev_loss) < self.tol: break\n",
    "            prev_loss = cur_loss # update loss\n",
    "        return A, S, {'loss': cur_loss, 'iter': i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_map = dict([\n",
    "    ('G', 0), # goal\n",
    "    ('#', 1),\n",
    "    *[(str(i), i + 2) for i in range(0, 100)],\n",
    "])\n",
    "\n",
    "def get_states(env):\n",
    "    # get the whole state space\n",
    "    states = []\n",
    "    for i in range(env.size[0]):\n",
    "        for j in range(env.size[1]):\n",
    "                if env.is_valid_loc((i, j)):\n",
    "                    states.append((i, j))\n",
    "    infos = {'task_id': [0] * len(states)} \n",
    "    return states, infos\n",
    "\n",
    "def get_img(env, abs_list):\n",
    "    indices = np.zeros(env.size, dtype=np.int64)\n",
    "    k = 0\n",
    "    for i in range(env.size[0]):\n",
    "        for j in range(env.size[1]):\n",
    "            if (i, j) == env.goal:\n",
    "                indices[i, j] = 0\n",
    "            elif env.map[i][j] == '#':\n",
    "                indices[i, j] = 1\n",
    "            else:\n",
    "                indices[i, j] = visualization_map[str(2 + abs_list[k])]\n",
    "                k += 1\n",
    "\n",
    "    img = drawer.draw(indices)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (1, 2), (1, 3), (1, 4), (1, 6), (1, 7), (1, 8), (1, 9), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 9), (3, 1), (3, 2), (3, 3), (3, 4), (3, 6), (3, 7), (3, 8), (3, 9), (4, 1), (4, 2), (4, 3), (4, 4), (4, 6), (4, 7), (4, 8), (4, 9), (5, 2), (5, 6), (5, 7), (5, 8), (5, 9), (6, 1), (6, 2), (6, 3), (6, 4), (6, 8), (7, 1), (7, 2), (7, 3), (7, 4), (7, 6), (7, 7), (7, 8), (7, 9), (8, 1), (8, 2), (8, 3), (8, 4), (8, 5), (8, 6), (8, 7), (8, 8), (8, 9), (9, 1), (9, 2), (9, 3), (9, 4), (9, 6), (9, 7), (9, 8)]\n",
      "n_states: 67\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALAAAACwCAYAAACvt+ReAAADAElEQVR4nO3cIW5VURRA0V9Sj+gECKK4WiRBMQEUBkNCGAChCoEj6JcaTB0hnQCqqcTWUdF0AhWMoMyhp8nNDmv58//Ny85VJ3dv27a7HUQ9Wn0AmBAwaQImTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZg0AZMmYNL2pz9wfXQzmv92+HY0//HqdOn/n5xdjObr32/q6eWT0bwbmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZtvA88tXqfd7wPezQbX37+ODcwaQImTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZg0AZMmYNIETNp4H3j1Puvqfdjp+7a7wwc5xn/LDUyagEkTMGkCJk3ApAmYNAGTJmDSBEyagEkTMGkCJk3ApAmYNAGTNt4HXr2POzXdZz65vBjNr/5+B+evRvO3L3890Enuxw1MmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkzbeB56+j/vh9YvR/MnZbB93us+72vh95l17n9sNTJqASRMwaQImTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZg0AZO2t23b3eQH/t4+e6iz3Mv0fdrp+7iPD/6M5ldbvY895QYmTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZg0AZMmYNIETJqASRvvA18f3YwOMN3HXW26Dzz9fnXT96XdwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBp+9Mf+HT4eTT//fz39AhLzd9HvhlNr96nnr7PPOUGJk3ApAmYNAGTJmDSBEyagEkTMGkCJk3ApAmYNAGTJmDSBEyagEkb7wOv9u7989H816svo/np+7a7+PvIP9/8GM0fHx+P5t3ApAmYNAGTJmDSBEyagEkTMGkCJk3ApAmYNAGTJmDSBEyagEkTMGl727bdrT4E3JcbmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAibtH1+SXGICM3t0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=176x176 at 0x7FA93A0A1F60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALAAAACwCAYAAACvt+ReAAADdklEQVR4nO3cMYsdZRiG4bNBULSVbRSiphAVrbaxEkmjVSCsYBVFqy0FA6exsnAhAUEYtlLsBFmFgEJEUqRKEQtRYqdRsAqmsEklrP8h78Jww3X1z5zhcPNVH7OzLMvJBqLOrP0CMCFg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQ9Mn3AH6/8Odpfefb96SuM3HqwO9r//ONXo/3BhfOj/Ycf/TraX/345dF+7f/PCUyagEkTMGkCJk3ApAmYNAGTJmDSBEyagEkTMGkCJk3ApAmYNAGTNr4PPHX57uej/fQ+8auP35vth/d5p566/fbwCXdG628+uTHan9sbzZ3AtAmYNAGTJmDSBEyagEkTMGkCJk3ApAmYNAGTJmDSBEyagEkTMGk7y7KcTB7w7/3nRy9w//UfRvu1PffLM6P99PvAa/v0jZdG+8cOPhvtncCkCZg0AZMmYNIETJqASRMwaQImTcCkCZg0AZMmYNIETJqASRMwaePvA2/fe3q0v3x3+gZta38f+daD3dF+bU5g0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQJmLTxfeCjazdG+yv778x+//jmaL+26X3ewy/+Hu0f/Xr2fd8Prt8Z7af9OIFJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0naWZTmZPOBg/7XTepeH8s/m7Gh/uL0+2p/buzfar+3gwvnRfnqfd8oJTJqASRMwaQImTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZg0AZM2/j7w9D7uk5u/Vt1fPXxhtD86nt0HXvs+9Wbz32j9+0+7o/30PrUTmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZtfB/43W9/O433eGhfXnxxtJ/eJ35r/9Jovxn+/trfR177PrUTmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZtZ1mWk8kDvj+zd1rvsorvLj4x2h8d3xzt1/8+8MzZvTdH++12O9o7gUkTMGkCJk3ApAmYNAGTJmDSBEyagEkTMGkCJk3ApAmYNAGTJmDSxveBYU1OYNIETJqASRMwaQImTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZi0/wE9YWouwRFOsQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=176x176 at 0x7FA93A0A1E10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_abs = 10\n",
    "l1_ratio = 0.0\n",
    "state_dim = 2\n",
    "n_action = 4\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "env = DiscreteGridWorld('fourroom', (1, 1), (9, 9))\n",
    "decomposer = NMF(n_abs, solver='mu', random_state=0, max_iter=5000, l1_ratio=l1_ratio)\n",
    "\n",
    "expert = CategoricalActorCriticNet(\n",
    "    1,\n",
    "    state_dim,\n",
    "    n_action,\n",
    "    FCBody(\n",
    "        state_dim, \n",
    "        hidden_units=(16,)\n",
    "    ),  \n",
    ")\n",
    "# load weight\n",
    "weight_dict = expert.state_dict()\n",
    "loaded_weight_dict = {k: v for k, v in torch.load(\n",
    "    'log/grid.fourroom.9_9/fc_discrete.baseline/_/0.190228-203237/models/step-128000-mean-1.00',\n",
    "    map_location=lambda storage, loc: storage)['network'].items()\n",
    "    if k in weight_dict}\n",
    "weight_dict.update(loaded_weight_dict)\n",
    "expert.load_state_dict(weight_dict)\n",
    "\n",
    "\n",
    "states, infos = get_states(env)                \n",
    "print(states)\n",
    "print('n_states:', len(states))\n",
    "                \n",
    "actions = expert(np.array(states), infos)['a'] # maybe take the whole distribution\n",
    "\n",
    "img = get_img(env, actions.cpu().detach().numpy())\n",
    "imshow(img)\n",
    "\n",
    "pvs = F.softmax(expert.get_logits(np.array(states), infos), dim=1).cpu().detach().numpy()\n",
    "#pvs = one_hot.encode(actions, n_action).cpu().detach().numpy()\n",
    "abs_mat = decomposer.fit_transform(pvs)\n",
    "policy_mat = decomposer.components_\n",
    "\n",
    "abs_list = np.array(abs_mat).argmax(1)\n",
    "#print(abs_list)\n",
    "abs_map = {s: i for s, i in zip(states, abs_mat)}\n",
    "#print(abs_map)\n",
    "fsave(abs_map, 'data/abs/grid/fourroom/single.{}.l1-{}'.format(n_abs, l1_ratio), 'pkl')\n",
    "\n",
    "img = get_img(env, abs_list)\n",
    "imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiTask NMF (shared states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALAAAACwCAYAAACvt+ReAAADPklEQVR4nO3csavVZRzH8XulpgaHK61GkaGE04U2pamGJjmCk7t70Jmc79D6o1VcFLy4VFA0hG5Bg7SERJZrKOXQ5HD7H+73wI83vV7753ee4c0zPZz9ZVlO9iDqzNoHgAkBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAibtjekHrtz4aLT/evlneoRVnT14Otpf39zc0UnW8dur16P9kx/ujfZuYNIETJqASRMwaQImTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZi08XvgqZcff7/q7x/8+MloX3/PO/X+2TdH+yfD33cDkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKSt/h54bdP3yA+OZ/8PfGtzdbR/sXd+tK9zA5MmYNIETJqASRMwaQImTcCkCZg0AZMmYNIETJqASRMwaQImTcCk/e/fA9ed23u+9hFW5QYmTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZg0AZMmYNIETJqASRu/B358/6fZBy7P5u/+8s7sAyv7fPvraP/l0cUdnaTJDUyagEkTMGkCJk3ApAmYNAGTJmDSBEyagEkTMGkCJk3ApAmYNAGTNn4P/Ozynzs4xum9evnBaP/ij79H+/cO/1p1/9XxbH9rc3X4+49G+yk3MGkCJk3ApAmYNAGTJmDSBEyagEkTMGkCJk3ApAmYNAGTJmDSBEza/rIsJ5MPfHvmcHSAix8ej/ZTX1y4Pdo/OL472v/+89uj/fbo09H+3N7z0X5q+p7YDUyagEkTMGkCJk3ApAmYNAGTJmDSBEyagEkTMGkCJk3ApAmYNAGTNv5/4LVN3/NOXd/cnH1gs5tznNZnD/8d7b+59taOTnI6bmDSBEyagEkTMGkCJk3ApAmYNAGTJmDSBEyagEkTMGkCJk3ApAmYtPx74KPtd6P99P91p/8PPH5PPHTn2qXR/vzhbL/dbkd7NzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRM2v6yLCdrHwJOyw1MmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBk/YfbItOETZJpEIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=176x176 at 0x7FA93A0A1C18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_abs = 10\n",
    "l1_ratio=0.0 # this is currently not working... since alpha is not set\n",
    "state_dim = 2\n",
    "n_action = 4\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "expert_dict = {\n",
    "    (1, 1): 'log/grid.fourroom.1_1.md-1.T-100/fc_discrete.baseline/_/0.190303-160012/models/step-160000-mean-1.00',\n",
    "    (9, 1): 'log/grid.fourroom.9_1.md-1.T-100/fc_discrete.baseline/_/0.190303-160151/models/step-160000-mean-1.00',\n",
    "    (1, 9): 'log/grid.fourroom.9_1.md-1.T-100/fc_discrete.baseline/_/0.190303-160151/models/step-160000-mean-1.00',\n",
    "}\n",
    "env = DiscreteGridWorld('fourroom', (1, 1), (9, 9))\n",
    "decomposer = NMF(n_abs, solver='mu', random_state=0, max_iter=5000, l1_ratio=l1_ratio)\n",
    "#decomposer = NMF(n_abs, solver='mu', beta_loss='kullback-leibler', random_state=0, max_iter=5000, l1_ratio=l1_ratio)\n",
    "states, infos = get_states(env)\n",
    "states.append(env.goal)\n",
    "infos['task_id'].append(0)\n",
    "pvs = []\n",
    "\n",
    "for goal_loc, weight_path in expert_dict.items():\n",
    "    expert = CategoricalActorCriticNet(\n",
    "        1,\n",
    "        state_dim,\n",
    "        n_action,\n",
    "        FCBody(\n",
    "            state_dim, \n",
    "            hidden_units=(16,)\n",
    "        ),  \n",
    "    )\n",
    "    # load weight\n",
    "    weight_dict = expert.state_dict()\n",
    "    loaded_weight_dict = {k: v for k, v in torch.load(\n",
    "        weight_path,\n",
    "        map_location=lambda storage, loc: storage)['network'].items()\n",
    "        if k in weight_dict}\n",
    "    weight_dict.update(loaded_weight_dict)\n",
    "    expert.load_state_dict(weight_dict)\n",
    "    pvs.append(F.softmax(expert.get_logits(np.array(states), infos), dim=1).cpu().detach().numpy())\n",
    "\n",
    "pvs = np.concatenate(pvs, 1)\n",
    "abs_mat = decomposer.fit_transform(pvs)\n",
    "policy_mat = decomposer.components_\n",
    "\n",
    "abs_list = np.array(abs_mat).argmax(1) # argmax moved to algo as an option\n",
    "#print(abs_list)\n",
    "abs_map = {s: i for s, i in zip(states, abs_mat)}\n",
    "#print(abs_map)\n",
    "fsave(abs_map, 'data/abs/grid/fourroom/MTS.{}.l1-{}'.format(n_abs, l1_ratio), 'pkl')\n",
    "\n",
    "img = get_img(env, abs_list)\n",
    "imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiTask NMF (does not shared states) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF loss: 22.4550305957206\n",
      "NMF loss: 0.11270363091110927\n",
      "NMF loss: 0.048864130080309334\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALAAAACwCAYAAACvt+ReAAADS0lEQVR4nO3cv6vVdRzHce/ltuagEDS0SD/AwQbJaNDhUgQOQdzAFndpjmxpq0an7x9QSxcKN5fEoTsYNxqEluxOtak43KlB8PY/+D7w5UmPx/4653PgyWf6cLaWZTk5BVHbax8AJgRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQJmLSd6Qd8snd9E+fI+vGn70f746dvbugkTafPPBzt3cCkCZg0AZMmYNIETJqASRMwaQImTcCkCZg0AZMmYNIETJqASRMwaeP3wGdP/T3af/7Xd6P9F298NdpPXb52adXvrzvYn+3dwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBp4/fAazs6frbq9z+4ezja39m+ONq/9ds/o/3azs1+vhuYNgGTJmDSBEyagEkTMGkCJk3ApAmYNAGTJmDSBEyagEkTMGkCJi3/Hvj2o29H+49f+XJDJ1nHn++8tvYRRs49fzzau4FJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0raWZTlZ8wBvv//paP/g7g8bOsk6pr//63tHGzrJOq4+/320dwOTJmDSBEyagEkTMGkCJk3ApAmYNAGTJmDSBEyagEkTMGkCJk3ApI3fA9/YuzI6wK/Hr472r59+abR/tPNwtD/YPxzt13Zn++JoP33PO+UGJk3ApAmYNAGTJmDSBEyagEkTMGkCJk3ApAmYNAGTJmDSBEyagEnbmX7Ard0Lo/27t5+M9kfHz0b7l8+M5mM3Ptod7W99eH52gM/uj+Zrvyd2A5MmYNIETJqASRMwaQImTcCkCZg0AZMmYNIETJqASRMwaQImTcCkjd8D/99dvnZp9gH/buYcL2p3eW+0vzd8TzzlBiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJ8x546GD/cLQ/P/x/4A9+mX3/z1dm75n/+GZvtL968+Zo7wYmTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZg0AZMmYNIETJqASdtaluVk7UPAi3IDkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKT9B8iIUoY5rlOSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=176x176 at 0x7FA91F615CC0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_abs = 10\n",
    "l1_ratio=0.0\n",
    "state_dim = 2\n",
    "n_action = 4\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "expert_dict = {\n",
    "    (1, 1): 'log/grid.fourroom.1_1.md-1.T-100/fc_discrete.baseline/_/0.190303-160012/models/step-160000-mean-1.00',\n",
    "    (9, 1): 'log/grid.fourroom.9_1.md-1.T-100/fc_discrete.baseline/_/0.190303-160151/models/step-160000-mean-1.00',\n",
    "    (1, 9): 'log/grid.fourroom.9_1.md-1.T-100/fc_discrete.baseline/_/0.190303-160151/models/step-160000-mean-1.00',\n",
    "}\n",
    "env = DiscreteGridWorld('fourroom', (1, 1), (9, 9))\n",
    "decomposer = MTNMF(n_abs, max_iter=5000, tol=0.0001)\n",
    "states, infos = get_states(env)\n",
    "states.append(env.goal)\n",
    "infos['task_id'].append(0)\n",
    "pvs = []\n",
    "\n",
    "for goal_loc, weight_path in expert_dict.items():\n",
    "    expert = CategoricalActorCriticNet(\n",
    "        1,\n",
    "        state_dim,\n",
    "        n_action,\n",
    "        FCBody(\n",
    "            state_dim, \n",
    "            hidden_units=(16,)\n",
    "        ),  \n",
    "    )\n",
    "    # load weight\n",
    "    weight_dict = expert.state_dict()\n",
    "    loaded_weight_dict = {k: v for k, v in torch.load(\n",
    "        weight_path,\n",
    "        map_location=lambda storage, loc: storage)['network'].items()\n",
    "        if k in weight_dict}\n",
    "    weight_dict.update(loaded_weight_dict)\n",
    "    expert.load_state_dict(weight_dict)\n",
    "    pvs.append(F.softmax(expert.get_logits(np.array(states), infos), dim=1).cpu().detach().numpy())\n",
    "\n",
    "pvs = np.stack(pvs, 0)\n",
    "A, S, info = MTNMF(n_abs, max_iter=5000, l1_ratio=l1_ratio).fit(pvs.transpose(0, 2, 1))\n",
    "\n",
    "abs_list = S.T.argmax(1)\n",
    "#print(S.T)\n",
    "abs_map = {s: i for s, i in zip(states, abs_mat)}\n",
    "#print(abs_map)\n",
    "fsave(abs_map, 'data/abs/grid/fourroom/MT.{}.l1-{}'.format(n_abs, l1_ratio), 'pkl')\n",
    "\n",
    "img = get_img(env, abs_list)\n",
    "imshow(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
