{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_rl.simple_grid.env import DiscreteGridWorld\n",
    "from deep_rl.network import *\n",
    "from deep_rl.utils import *\n",
    "from sklearn.decomposition import NMF\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GridDrawer:                           \n",
    "    def __init__(self, color_list):\n",
    "        self.color_list = np.asarray(color_list)\n",
    "\n",
    "    # input: a 2-d index matrix\n",
    "    # output: a 2-d rgb matrix\n",
    "    def draw(self, indices, repeat=16):\n",
    "        return np.uint8(255 * np.array(self.color_list[indices, :]).repeat(repeat, 0).repeat(repeat, 1))\n",
    "    \n",
    "# this is my color list\n",
    "color_map = dict([\n",
    "    #*[('grey-{}'.format(v), plt.cm.Greys(0.1 * v)) for v in range(1, 20)],\n",
    "    *[('purple-{}'.format(v), plt.cm.Purples(0.05 * v)) for v in range(1, 20)],\n",
    "    *[('blue-{}'.format(v), plt.cm.Blues(0.05 * v)) for v in range(1, 20)],\n",
    "    *[('green-{}'.format(v), plt.cm.Greens(0.05 * v)) for v in range(1, 20)],\n",
    "    *[('orange-{}'.format(v), plt.cm.Oranges(0.05 * v)) for v in range(1, 20)],\n",
    "    *[('red-{}'.format(v), plt.cm.Reds(0.05 * v)) for v in range(1, 20)],\n",
    "])\n",
    "\n",
    "def imshow(img):\n",
    "    display(Image.fromarray(np.asarray(img)))\n",
    "\n",
    "color_list = list(color_map.values())\n",
    "shuffle(color_list)\n",
    "color_list = [plt.cm.Greys(0.9)] + [plt.cm.Greys(0.5)] + color_list\n",
    "drawer = GridDrawer(color_list)\n",
    "\n",
    "# multitask NMF from: https://ieeexplore.ieee.org/document/6939673\n",
    "class MTNMF:\n",
    "    def __init__(self, n_components, l1_ratio=0.0, max_iter=200, tol=0.0001):\n",
    "        self.n_components = n_components\n",
    "        self.l1_ratio = l1_ratio\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "\n",
    "    def loss(self, X, A, S):\n",
    "        return 0.5 * ((X - np.matmul(A, S)) ** 2).sum() + self.l1_ratio * S.sum()\n",
    "        \n",
    "    # input: a stack of observed data X_1, ..., X_K\n",
    "    # output: S, A_1, ..., A_K\n",
    "    def fit(self, X):\n",
    "        K, N, M = X.shape\n",
    "        A = np.random.rand(K, N, self.n_components)\n",
    "        S = np.random.rand(self.n_components, M)\n",
    "        prev_loss = np.inf\n",
    "        cur_loss = None\n",
    "        for i in range(self.max_iter):\n",
    "            A_T = A.transpose(0, 2, 1)\n",
    "            new_S = S * (np.matmul(A_T, X).sum(0)) / (np.matmul(np.matmul(A_T, A), S).sum(0) + K * self.l1_ratio * np.ones((self.n_components, M)))\n",
    "            S = new_S\n",
    "            new_A = A * np.matmul(X, S.T) / np.matmul(np.matmul(A, S), S.T)\n",
    "            A = new_A\n",
    "            cur_loss = self.loss(X, A, S)\n",
    "            if i % 100 == 0: print('loss:', cur_loss)\n",
    "            if abs(cur_loss - prev_loss) < self.tol: break\n",
    "        return A, S, {'loss': cur_loss, 'iter': i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 50596.30248200452\n",
      "loss: 47183.050079330555\n",
      "loss: 46624.847221129115\n",
      "loss: 46411.82125858207\n",
      "loss: 46295.73268889187\n",
      "loss: 46222.640539390835\n",
      "loss: 46172.95030749777\n",
      "loss: 46136.9957616219\n",
      "loss: 46109.71161836178\n",
      "loss: 46088.30171579195\n",
      "loss: 46070.95957547753\n",
      "loss: 46056.49317949241\n",
      "loss: 46044.47302251493\n",
      "loss: 46034.277495750735\n",
      "loss: 46025.56246442853\n",
      "loss: 46018.22330388267\n",
      "loss: 46011.63857509089\n",
      "loss: 46005.726946820316\n",
      "loss: 46000.544167109874\n",
      "loss: 45996.00625260112\n",
      "loss: 45991.909966482344\n",
      "loss: 45988.236922479795\n",
      "loss: 45984.90763111566\n",
      "loss: 45981.86092601572\n",
      "loss: 45979.09559901517\n",
      "loss: 45976.5876751501\n",
      "loss: 45974.2393740085\n",
      "loss: 45972.030255880505\n",
      "loss: 45970.04149886593\n",
      "loss: 45968.08051674779\n",
      "loss: 45966.25505605357\n",
      "loss: 45964.52762605321\n",
      "loss: 45962.88043822667\n",
      "loss: 45961.33801530149\n",
      "loss: 45959.84225356178\n",
      "loss: 45958.2707558232\n",
      "loss: 45956.76065142559\n",
      "loss: 45955.33404174617\n",
      "loss: 45954.0173165943\n",
      "loss: 45952.818846229144\n",
      "loss: 45951.718060265535\n",
      "loss: 45950.60837302371\n",
      "loss: 45949.566859918006\n",
      "loss: 45948.611013901485\n",
      "loss: 45947.69461280951\n",
      "loss: 45946.76159443773\n",
      "loss: 45945.92276725682\n",
      "loss: 45945.1115135236\n",
      "loss: 45944.32618535662\n",
      "loss: 45943.59629555223\n",
      "loss: 45942.85005463695\n",
      "loss: 45942.15047394338\n",
      "loss: 45941.529496012074\n",
      "loss: 45940.984588153515\n",
      "loss: 45940.44251852468\n",
      "loss: 45939.8792065705\n",
      "loss: 45939.24667666205\n",
      "loss: 45938.65449614046\n",
      "loss: 45938.124604649245\n",
      "loss: 45937.648103414416\n",
      "loss: 45937.20936702191\n",
      "loss: 45936.80901324963\n",
      "loss: 45936.42618529471\n",
      "loss: 45936.035668923716\n",
      "loss: 45935.66749817958\n",
      "loss: 45935.29708785416\n",
      "loss: 45934.95846992706\n",
      "loss: 45934.654997726444\n",
      "loss: 45934.346079795985\n",
      "loss: 45934.03564272133\n",
      "loss: 45933.724233143876\n",
      "loss: 45933.41763002107\n",
      "loss: 45933.1165314699\n",
      "loss: 45932.83032401393\n",
      "loss: 45932.54525923473\n",
      "loss: 45932.256094247954\n",
      "loss: 45931.98608017196\n",
      "loss: 45931.75371784913\n",
      "loss: 45931.537504834545\n",
      "loss: 45931.31463851355\n",
      "loss: 45931.091970131536\n",
      "loss: 45930.88780606391\n",
      "loss: 45930.65878599065\n",
      "loss: 45930.42581220499\n",
      "loss: 45930.204427212884\n",
      "loss: 45929.98731333486\n",
      "loss: 45929.76976632023\n",
      "loss: 45929.54089503666\n",
      "loss: 45929.316573000935\n",
      "loss: 45929.103084077135\n",
      "loss: 45928.91084708769\n",
      "loss: 45928.72783529585\n",
      "loss: 45928.535553839894\n",
      "loss: 45928.34034866344\n",
      "loss: 45928.16044540082\n",
      "loss: 45927.983762120726\n",
      "loss: 45927.813700989245\n",
      "loss: 45927.62724654959\n",
      "loss: 45927.445577191174\n",
      "loss: 45927.2728173389\n",
      "loss: 45927.09946485916\n",
      "loss: 45926.95170006393\n",
      "loss: 45926.808244901265\n",
      "loss: 45926.65881110261\n",
      "loss: 45926.503373368665\n",
      "loss: 45926.35007769208\n",
      "loss: 45926.21074208393\n",
      "loss: 45926.07599851629\n",
      "loss: 45925.94137467983\n",
      "loss: 45925.80153424051\n",
      "loss: 45925.665916254846\n",
      "loss: 45925.528975935966\n",
      "loss: 45925.38310002171\n",
      "loss: 45925.23361149124\n",
      "loss: 45925.090585431266\n",
      "loss: 45924.96155118373\n",
      "loss: 45924.83310019648\n",
      "loss: 45924.69319124644\n",
      "loss: 45924.537799787984\n",
      "loss: 45924.412160251224\n",
      "loss: 45924.29717315331\n",
      "loss: 45924.17909120039\n",
      "loss: 45924.05982420329\n",
      "loss: 45923.94411695291\n",
      "loss: 45923.82933369386\n",
      "loss: 45923.72128599322\n",
      "loss: 45923.6282903114\n",
      "loss: 45923.53941618317\n",
      "loss: 45923.45159704577\n",
      "loss: 45923.357475720346\n",
      "loss: 45923.273807108846\n",
      "loss: 45923.19332936781\n",
      "loss: 45923.11124709727\n",
      "loss: 45923.026521948326\n",
      "loss: 45922.940894458625\n",
      "loss: 45922.856995123904\n",
      "loss: 45922.76909912318\n",
      "loss: 45922.69090199266\n",
      "loss: 45922.62051441243\n",
      "loss: 45922.55327584403\n",
      "loss: 45922.49005114164\n",
      "loss: 45922.42129625473\n",
      "loss: 45922.34483760335\n",
      "loss: 45922.26985618935\n",
      "loss: 45922.194145035864\n",
      "loss: 45922.11036503457\n",
      "loss: 45922.03384300552\n",
      "loss: 45921.96891209011\n",
      "loss: 45921.90353265985\n",
      "loss: 45921.82925054118\n",
      "loss: 45921.75611418685\n",
      "loss: 45921.687020188656\n",
      "loss: 45921.616152487106\n",
      "loss: 45921.54531451565\n",
      "loss: 45921.47396069426\n",
      "loss: 45921.40426874831\n",
      "loss: 45921.34151531919\n",
      "loss: 45921.283459906444\n",
      "loss: 45921.22269596491\n",
      "loss: 45921.15943062276\n",
      "loss: 45921.09787777725\n",
      "loss: 45921.036201230054\n",
      "loss: 45920.970922855464\n",
      "loss: 45920.89590691865\n",
      "loss: 45920.829448945566\n",
      "loss: 45920.76294852426\n",
      "loss: 45920.68659034991\n",
      "loss: 45920.62136129003\n",
      "loss: 45920.56177789305\n",
      "loss: 45920.50323543984\n",
      "loss: 45920.444344508156\n",
      "loss: 45920.38776155821\n",
      "loss: 45920.33064383227\n",
      "loss: 45920.270679434325\n",
      "loss: 45920.20661308575\n",
      "loss: 45920.15019000982\n",
      "loss: 45920.096919911884\n",
      "loss: 45920.033768003224\n",
      "loss: 45919.96221871083\n",
      "loss: 45919.89524478832\n",
      "loss: 45919.831079255004\n",
      "loss: 45919.76729067898\n",
      "loss: 45919.70737565199\n",
      "loss: 45919.645543754734\n",
      "loss: 45919.57053650823\n",
      "loss: 45919.51266987978\n",
      "loss: 45919.46256500086\n",
      "loss: 45919.40865334378\n",
      "loss: 45919.35156684221\n",
      "loss: 45919.296971329335\n",
      "loss: 45919.24180303405\n",
      "loss: 45919.18354827921\n",
      "loss: 45919.12918576469\n",
      "loss: 45919.07745896749\n",
      "loss: 45919.029208064196\n",
      "loss: 45918.978961095236\n",
      "loss: 45918.926675129704\n",
      "loss: 45918.87642212691\n",
      "loss: 45918.825406642434\n",
      "loss: 45918.77690046252\n",
      "loss: 45918.7294093595\n",
      "loss: 45918.679776978985\n",
      "loss: 45918.63028052679\n",
      "loss: 45918.58283006199\n",
      "loss: 45918.53812646071\n",
      "loss: 45918.488020699726\n",
      "loss: 45918.42887400108\n",
      "loss: 45918.37625953795\n",
      "loss: 45918.32280118789\n",
      "loss: 45918.273393701005\n",
      "loss: 45918.228444569024\n",
      "loss: 45918.183778592414\n",
      "loss: 45918.1354039564\n",
      "loss: 45918.08379064686\n",
      "loss: 45918.03787270204\n",
      "loss: 45917.996533863356\n",
      "loss: 45917.9547307546\n",
      "loss: 45917.9084751173\n",
      "loss: 45917.85540518465\n",
      "loss: 45917.80486419808\n",
      "loss: 45917.754439337536\n",
      "loss: 45917.702914846624\n",
      "loss: 45917.654223379875\n",
      "loss: 45917.60927503157\n",
      "loss: 45917.56706555799\n",
      "loss: 45917.52303644535\n",
      "loss: 45917.4676163454\n",
      "loss: 45917.40623305941\n",
      "loss: 45917.349122829655\n",
      "loss: 45917.29309448173\n",
      "loss: 45917.23252176324\n",
      "loss: 45917.17198218216\n",
      "loss: 45917.1215214891\n",
      "loss: 45917.06892337739\n",
      "loss: 45917.01054900076\n",
      "loss: 45916.95302183123\n",
      "loss: 45916.883062306275\n",
      "loss: 45916.809219838346\n",
      "loss: 45916.7406732609\n",
      "loss: 45916.67349542141\n",
      "loss: 45916.61934666949\n",
      "loss: 45916.57163663369\n",
      "loss: 45916.52514794353\n",
      "loss: 45916.47886369975\n",
      "loss: 45916.43233808372\n",
      "loss: 45916.38867729175\n",
      "loss: 45916.350030419526\n",
      "loss: 45916.31397092258\n",
      "loss: 45916.27617754381\n",
      "loss: 45916.23457695149\n",
      "loss: 45916.1920367133\n",
      "loss: 45916.14175461612\n",
      "loss: 45916.08134378551\n",
      "loss: 45916.04020206971\n",
      "loss: 45915.99346794322\n",
      "loss: 45915.93463557399\n",
      "loss: 45915.88830518507\n",
      "loss: 45915.84783705756\n",
      "loss: 45915.80510044397\n",
      "loss: 45915.75781373723\n",
      "loss: 45915.70470738693\n",
      "loss: 45915.65556702268\n",
      "loss: 45915.608397190255\n",
      "loss: 45915.55309237574\n",
      "loss: 45915.504175055015\n",
      "loss: 45915.461315860055\n",
      "loss: 45915.421988111244\n",
      "loss: 45915.38161467297\n",
      "loss: 45915.33469778062\n",
      "loss: 45915.281850439686\n",
      "loss: 45915.240506001726\n",
      "loss: 45915.20360076211\n",
      "loss: 45915.16870248913\n",
      "loss: 45915.136459218156\n",
      "loss: 45915.10460943485\n",
      "loss: 45915.06890834387\n",
      "loss: 45915.03031480847\n",
      "loss: 45914.99521706738\n",
      "loss: 45914.96218685853\n",
      "loss: 45914.928991675086\n",
      "loss: 45914.900510340834\n",
      "loss: 45914.87628005771\n",
      "loss: 45914.84285562729\n",
      "loss: 45914.805558257176\n",
      "loss: 45914.76327841644\n",
      "loss: 45914.719928799444\n",
      "loss: 45914.682837559514\n",
      "loss: 45914.647707819655\n",
      "loss: 45914.61458209703\n",
      "loss: 45914.58340408355\n",
      "loss: 45914.55036367066\n",
      "loss: 45914.51556671452\n",
      "loss: 45914.48310011992\n",
      "loss: 45914.44981169896\n",
      "loss: 45914.41655489452\n",
      "loss: 45914.38468573578\n",
      "loss: 45914.351778586904\n",
      "loss: 45914.31276395664\n",
      "loss: 45914.276612330556\n",
      "loss: 45914.24180181554\n",
      "loss: 45914.2110741467\n",
      "loss: 45914.182772328844\n",
      "loss: 45914.15209892804\n",
      "loss: 45914.12204545048\n",
      "loss: 45914.09454287291\n",
      "loss: 45914.0674328874\n",
      "loss: 45914.038885830254\n",
      "loss: 45914.00884685947\n",
      "loss: 45913.9741887847\n",
      "loss: 45913.930578233165\n",
      "loss: 45913.88486017712\n",
      "loss: 45913.835737082336\n",
      "loss: 45913.79829039136\n",
      "loss: 45913.763362569545\n",
      "loss: 45913.73654232257\n",
      "loss: 45913.71335168352\n",
      "loss: 45913.686620227694\n",
      "loss: 45913.654178199264\n",
      "loss: 45913.619949708096\n",
      "loss: 45913.588820959405\n",
      "loss: 45913.56016704544\n",
      "loss: 45913.531756276316\n",
      "loss: 45913.50568704964\n",
      "loss: 45913.48249901524\n",
      "loss: 45913.459150371214\n",
      "loss: 45913.43216336488\n",
      "loss: 45913.40115279498\n",
      "loss: 45913.37298634779\n",
      "loss: 45913.3448396474\n",
      "loss: 45913.310813212556\n",
      "loss: 45913.28828712982\n",
      "loss: 45913.26709665792\n",
      "loss: 45913.244650431596\n",
      "loss: 45913.22015956519\n",
      "loss: 45913.194473482836\n",
      "loss: 45913.1639424149\n",
      "loss: 45913.12474676379\n",
      "loss: 45913.095294603474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 45913.06262634131\n",
      "loss: 45913.02491250362\n",
      "loss: 45912.99178263438\n",
      "loss: 45912.96237065512\n",
      "loss: 45912.93524402689\n",
      "loss: 45912.906424917324\n",
      "loss: 45912.87553668455\n",
      "loss: 45912.84835002391\n",
      "loss: 45912.823148233445\n",
      "loss: 45912.79595332338\n",
      "loss: 45912.765574977755\n",
      "loss: 45912.736063993\n",
      "loss: 45912.710129983796\n",
      "loss: 45912.68796070902\n",
      "loss: 45912.6662489041\n",
      "loss: 45912.64002275063\n",
      "loss: 45912.61594699196\n",
      "loss: 45912.59424491352\n",
      "loss: 45912.569770292896\n",
      "loss: 45912.53730239126\n",
      "loss: 45912.513148167236\n",
      "loss: 45912.491673359575\n",
      "loss: 45912.472115856195\n",
      "loss: 45912.45471579221\n",
      "loss: 45912.43768437824\n",
      "loss: 45912.418691933286\n",
      "loss: 45912.39939639454\n",
      "loss: 45912.377617985796\n",
      "loss: 45912.344473493475\n",
      "loss: 45912.31554830436\n",
      "loss: 45912.287727141214\n",
      "loss: 45912.26289291569\n",
      "loss: 45912.23583334614\n",
      "loss: 45912.20688407756\n",
      "loss: 45912.17843459243\n",
      "loss: 45912.15098470391\n",
      "loss: 45912.12892074637\n",
      "loss: 45912.11108304939\n",
      "loss: 45912.09432343164\n",
      "loss: 45912.077084795026\n",
      "loss: 45912.05981869801\n",
      "loss: 45912.04277182675\n",
      "loss: 45912.024404201395\n",
      "loss: 45912.00460720701\n",
      "loss: 45911.98494719765\n",
      "loss: 45911.96705604274\n",
      "loss: 45911.95098648622\n",
      "loss: 45911.93552864193\n",
      "loss: 45911.91892684143\n",
      "loss: 45911.89986681207\n",
      "loss: 45911.87987691204\n",
      "loss: 45911.8609448389\n",
      "loss: 45911.844229103175\n",
      "loss: 45911.828289755314\n",
      "loss: 45911.811387538335\n",
      "loss: 45911.79131051097\n",
      "loss: 45911.76446433301\n",
      "loss: 45911.73655133926\n",
      "loss: 45911.718244231044\n",
      "loss: 45911.70235404455\n",
      "loss: 45911.684211530286\n",
      "loss: 45911.663847656884\n",
      "loss: 45911.64075195681\n",
      "loss: 45911.61918344897\n",
      "loss: 45911.59833957304\n",
      "loss: 45911.57896664786\n",
      "loss: 45911.55873881146\n",
      "loss: 45911.53916935787\n",
      "loss: 45911.52234252565\n",
      "loss: 45911.505183551635\n",
      "loss: 45911.486485232956\n",
      "loss: 45911.46744973625\n",
      "loss: 45911.449089558104\n",
      "loss: 45911.42999545321\n",
      "loss: 45911.40532607795\n",
      "loss: 45911.37269862661\n",
      "loss: 45911.345942858345\n",
      "loss: 45911.3288405556\n",
      "loss: 45911.313794210044\n",
      "loss: 45911.29406695277\n",
      "loss: 45911.26073654545\n",
      "loss: 45911.24001356038\n",
      "loss: 45911.222332461155\n",
      "loss: 45911.20331997039\n",
      "loss: 45911.17837719498\n",
      "loss: 45911.144124015256\n",
      "loss: 45911.10236000902\n",
      "loss: 45911.073261703335\n",
      "loss: 45911.05199212306\n",
      "loss: 45911.033928430035\n",
      "loss: 45911.01771367065\n",
      "loss: 45911.0025482889\n",
      "loss: 45910.987914385434\n",
      "loss: 45910.972913634796\n",
      "loss: 45910.96114463398\n",
      "loss: 45910.95272972743\n",
      "loss: 45910.94475493975\n",
      "loss: 45910.93574771045\n",
      "loss: 45910.92645791267\n",
      "loss: 45910.91740400769\n",
      "loss: 45910.90820380749\n",
      "loss: 45910.89938830944\n",
      "loss: 45910.88922091906\n",
      "loss: 45910.87538222891\n",
      "loss: 45910.859311617925\n",
      "loss: 45910.841845719384\n",
      "loss: 45910.83086003121\n",
      "loss: 45910.82277141383\n",
      "loss: 45910.814393786684\n",
      "loss: 45910.805866432696\n",
      "loss: 45910.7957808827\n",
      "loss: 45910.77801402998\n",
      "loss: 45910.76150040601\n",
      "loss: 45910.753115236985\n",
      "loss: 45910.74563181941\n",
      "loss: 45910.73811426701\n",
      "loss: 45910.73048200599\n",
      "loss: 45910.72042782465\n",
      "loss: 45910.707611993275\n",
      "loss: 45910.69908584357\n",
      "loss: 45910.69168643398\n",
      "loss: 45910.68271073396\n",
      "loss: 45910.66991976254\n",
      "loss: 45910.653057259915\n",
      "loss: 45910.638270874115\n",
      "loss: 45910.62702289989\n",
      "loss: 45910.615661009535\n",
      "loss: 45910.60071014918\n",
      "loss: 45910.58437972175\n",
      "loss: 45910.56868832183\n",
      "loss: 45910.55286658438\n",
      "loss: 45910.54066329164\n",
      "loss: 45910.525691679366\n",
      "loss: 45910.49247129676\n",
      "loss: 45910.46358546973\n",
      "loss: 45910.43833329674\n",
      "loss: 45910.41379600072\n",
      "loss: 45910.38716770585\n",
      "loss: 45910.3616891178\n",
      "loss: 45910.33874439766\n",
      "loss: 45910.31483318775\n",
      "loss: 45910.289764497524\n",
      "loss: 45910.26808197663\n",
      "loss: 45910.24585603106\n",
      "loss: 45910.219171442135\n",
      "loss: 45910.190892810344\n",
      "loss: 45910.16881151872\n",
      "loss: 45910.150814357934\n",
      "loss: 45910.13374141056\n",
      "loss: 45910.1169468632\n",
      "loss: 45910.10040878444\n",
      "loss: 45910.08286020307\n",
      "loss: 45910.06187045161\n",
      "loss: 45910.0394680997\n",
      "loss: 45910.020508417816\n",
      "loss: 45910.001473587065\n",
      "loss: 45909.98330594286\n",
      "loss: 45909.96806636022\n",
      "loss: 45909.95468120051\n",
      "loss: 45909.94246069727\n",
      "loss: 45909.93082354404\n",
      "loss: 45909.91907652563\n",
      "loss: 45909.9070015324\n",
      "loss: 45909.89369513227\n",
      "loss: 45909.87509469239\n",
      "loss: 45909.84749878446\n",
      "loss: 45909.80852903802\n",
      "loss: 45909.789527731846\n",
      "loss: 45909.774908361236\n",
      "loss: 45909.75938400796\n",
      "loss: 45909.74442089122\n",
      "loss: 45909.72915456318\n",
      "loss: 45909.715808278896\n",
      "loss: 45909.70419172782\n",
      "loss: 45909.69078958141\n",
      "loss: 45909.67446195306\n",
      "loss: 45909.65626214491\n",
      "loss: 45909.63681633387\n",
      "loss: 45909.61549408833\n",
      "loss: 45909.589464332086\n",
      "loss: 45909.55904199333\n",
      "loss: 45909.53607690116\n",
      "loss: 45909.515222052774\n",
      "loss: 45909.49524210957\n",
      "loss: 45909.478558104296\n",
      "loss: 45909.4645440696\n",
      "loss: 45909.45164557251\n",
      "loss: 45909.4391804404\n",
      "loss: 45909.427518564036\n",
      "loss: 45909.41754231949\n",
      "loss: 45909.40884264133\n",
      "loss: 45909.40020573066\n",
      "loss: 45909.391533329115\n",
      "loss: 45909.38232264857\n",
      "loss: 45909.37086078041\n",
      "loss: 45909.358794462685\n",
      "loss: 45909.347236202986\n",
      "loss: 45909.33558132526\n",
      "loss: 45909.32540383356\n",
      "loss: 45909.316420212905\n",
      "loss: 45909.30784897862\n",
      "loss: 45909.299254929414\n",
      "loss: 45909.29047842999\n",
      "loss: 45909.28120342509\n",
      "loss: 45909.26991231815\n",
      "loss: 45909.25383969844\n",
      "loss: 45909.235391884125\n",
      "loss: 45909.21992760514\n",
      "loss: 45909.20407534201\n",
      "loss: 45909.18694393435\n",
      "loss: 45909.17007042306\n",
      "loss: 45909.15470754946\n",
      "loss: 45909.14005517904\n",
      "loss: 45909.120098632106\n",
      "loss: 45909.102096154194\n",
      "loss: 45909.088971579986\n",
      "loss: 45909.077938865245\n",
      "loss: 45909.06789114866\n",
      "loss: 45909.057898394916\n",
      "loss: 45909.047703278324\n",
      "loss: 45909.03717653456\n",
      "loss: 45909.0257612936\n",
      "loss: 45909.01316904055\n",
      "loss: 45909.00136111623\n",
      "loss: 45908.99093948558\n",
      "loss: 45908.98072415175\n",
      "loss: 45908.97035876568\n",
      "loss: 45908.9600552514\n",
      "loss: 45908.950094285545\n",
      "loss: 45908.94063658581\n",
      "loss: 45908.930929528586\n",
      "loss: 45908.92172062799\n",
      "loss: 45908.91329299486\n",
      "loss: 45908.9052655119\n",
      "loss: 45908.89706022302\n",
      "loss: 45908.884852757\n",
      "loss: 45908.86548594079\n",
      "loss: 45908.84627336382\n",
      "loss: 45908.833693103305\n",
      "loss: 45908.82511944448\n",
      "loss: 45908.81754469674\n",
      "loss: 45908.80997917834\n",
      "loss: 45908.80191891175\n",
      "loss: 45908.79299190154\n",
      "loss: 45908.78282557965\n",
      "loss: 45908.77079925672\n",
      "loss: 45908.75648080499\n",
      "loss: 45908.740447580756\n",
      "loss: 45908.723896905525\n",
      "loss: 45908.70640670112\n",
      "loss: 45908.68760226736\n",
      "loss: 45908.66793602741\n",
      "loss: 45908.64718149871\n",
      "loss: 45908.62703882613\n",
      "loss: 45908.60987857518\n",
      "loss: 45908.59387738627\n",
      "loss: 45908.57706378802\n",
      "loss: 45908.55894644995\n",
      "loss: 45908.54052954776\n",
      "loss: 45908.52373595813\n",
      "loss: 45908.51058802189\n",
      "loss: 45908.49995822372\n",
      "loss: 45908.49036566029\n",
      "loss: 45908.48110198047\n",
      "loss: 45908.47151515853\n",
      "loss: 45908.46105242521\n",
      "loss: 45908.450448053496\n",
      "loss: 45908.440919050314\n",
      "loss: 45908.432489819024\n",
      "loss: 45908.42524687808\n",
      "loss: 45908.41827281856\n",
      "loss: 45908.407640206606\n",
      "loss: 45908.38775911268\n",
      "loss: 45908.36957160116\n",
      "loss: 45908.35904601133\n",
      "loss: 45908.34851219691\n",
      "loss: 45908.33830966343\n",
      "loss: 45908.32827742357\n",
      "loss: 45908.316811890836\n",
      "loss: 45908.304654967375\n",
      "loss: 45908.295473107944\n",
      "loss: 45908.28814561338\n",
      "loss: 45908.28115156463\n",
      "loss: 45908.27404167519\n",
      "loss: 45908.266339412316\n",
      "loss: 45908.256902030655\n",
      "loss: 45908.24745647762\n",
      "loss: 45908.23986723677\n",
      "loss: 45908.232192038966\n",
      "loss: 45908.22357563559\n",
      "loss: 45908.21503148072\n",
      "loss: 45908.20759492916\n",
      "loss: 45908.200223585256\n",
      "loss: 45908.19185597188\n",
      "loss: 45908.18102201128\n",
      "loss: 45908.1683350138\n",
      "loss: 45908.15817718066\n",
      "loss: 45908.145975631276\n",
      "loss: 45908.13046323023\n",
      "loss: 45908.12159213792\n",
      "loss: 45908.115900601515\n",
      "loss: 45908.111128936645\n",
      "loss: 45908.10606459589\n",
      "loss: 45908.09882659741\n",
      "loss: 45908.08646817545\n",
      "loss: 45908.07254886808\n",
      "loss: 45908.06418140773\n",
      "loss: 45908.057997266515\n",
      "loss: 45908.05163006584\n",
      "loss: 45908.04308572014\n",
      "loss: 45908.03205394225\n",
      "loss: 45908.019670829344\n",
      "loss: 45908.00808921636\n",
      "loss: 45907.997096708175\n",
      "loss: 45907.98685468388\n",
      "loss: 45907.97764224564\n",
      "loss: 45907.96855653022\n",
      "loss: 45907.95906571776\n",
      "loss: 45907.949121843114\n",
      "loss: 45907.938741752674\n",
      "loss: 45907.92774879869\n",
      "loss: 45907.915503965196\n",
      "loss: 45907.90094430491\n",
      "loss: 45907.88651436647\n",
      "loss: 45907.8758546971\n",
      "loss: 45907.86669119442\n",
      "loss: 45907.85780759966\n",
      "loss: 45907.84908242777\n",
      "loss: 45907.83983826228\n",
      "loss: 45907.82813536222\n",
      "loss: 45907.81431867327\n",
      "loss: 45907.804000266646\n",
      "loss: 45907.7953603816\n",
      "loss: 45907.786870082\n",
      "loss: 45907.77845080913\n",
      "loss: 45907.76850908352\n",
      "loss: 45907.75093552782\n",
      "loss: 45907.72423122054\n",
      "loss: 45907.705187444815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 45907.690479016484\n",
      "loss: 45907.671291744104\n",
      "loss: 45907.66037739202\n",
      "loss: 45907.6507341623\n",
      "loss: 45907.63774744326\n",
      "loss: 45907.618212287634\n",
      "loss: 45907.59857574493\n",
      "loss: 45907.582181819365\n",
      "loss: 45907.56628304914\n",
      "loss: 45907.55104839548\n",
      "loss: 45907.53511145241\n",
      "loss: 45907.517978054966\n",
      "loss: 45907.50072163563\n",
      "loss: 45907.488143903734\n",
      "loss: 45907.47839678634\n",
      "loss: 45907.46931394434\n",
      "loss: 45907.461248992695\n",
      "loss: 45907.453808226695\n",
      "loss: 45907.44617579524\n",
      "loss: 45907.437665420344\n",
      "loss: 45907.427585939906\n",
      "loss: 45907.41538521342\n",
      "loss: 45907.40179859354\n",
      "loss: 45907.389315879765\n",
      "loss: 45907.37851476836\n",
      "loss: 45907.368692824544\n",
      "loss: 45907.359655307955\n",
      "loss: 45907.3512425991\n",
      "loss: 45907.34284386268\n",
      "loss: 45907.33352079929\n",
      "loss: 45907.32342444077\n",
      "loss: 45907.31365863023\n",
      "loss: 45907.30273130169\n",
      "loss: 45907.29034375396\n",
      "loss: 45907.279755337855\n",
      "loss: 45907.26994613786\n",
      "loss: 45907.258130097376\n",
      "loss: 45907.24558734529\n",
      "loss: 45907.23504486809\n",
      "loss: 45907.22434839122\n",
      "loss: 45907.21283068265\n",
      "loss: 45907.19996880247\n",
      "loss: 45907.18885133892\n",
      "loss: 45907.17982199419\n",
      "loss: 45907.171598690176\n",
      "loss: 45907.161749722756\n",
      "loss: 45907.14909239912\n",
      "loss: 45907.136459207606\n",
      "loss: 45907.12247282698\n",
      "loss: 45907.1029622437\n",
      "loss: 45907.08798260874\n",
      "loss: 45907.075298746386\n",
      "loss: 45907.05830941376\n",
      "loss: 45907.03946657152\n",
      "loss: 45907.023038927415\n",
      "loss: 45907.010995673954\n",
      "loss: 45907.00116559574\n",
      "loss: 45906.990810577016\n",
      "loss: 45906.979168800346\n",
      "loss: 45906.96733804918\n",
      "loss: 45906.95625138071\n",
      "loss: 45906.94542843052\n",
      "loss: 45906.93454408891\n",
      "loss: 45906.92340594215\n",
      "loss: 45906.91107858096\n",
      "loss: 45906.89697069118\n",
      "loss: 45906.884650523294\n",
      "loss: 45906.87361532054\n",
      "loss: 45906.863783164044\n",
      "loss: 45906.85463216763\n",
      "loss: 45906.84433950485\n",
      "loss: 45906.83339943749\n",
      "loss: 45906.8239614424\n",
      "loss: 45906.813412773816\n",
      "loss: 45906.800425750684\n",
      "loss: 45906.789537517405\n",
      "loss: 45906.78015425681\n",
      "loss: 45906.77093274857\n",
      "loss: 45906.76094853397\n",
      "loss: 45906.75124473828\n",
      "loss: 45906.74035550152\n",
      "loss: 45906.727788505406\n",
      "loss: 45906.71686113016\n",
      "loss: 45906.70681602582\n",
      "loss: 45906.696571224995\n",
      "loss: 45906.68594125532\n",
      "loss: 45906.674955803974\n",
      "loss: 45906.66368605482\n",
      "loss: 45906.65216083396\n",
      "loss: 45906.64040279257\n",
      "loss: 45906.62856743166\n",
      "loss: 45906.616638083695\n",
      "loss: 45906.60285872582\n",
      "loss: 45906.58243106516\n",
      "loss: 45906.5611975126\n",
      "loss: 45906.54644883708\n",
      "loss: 45906.5339876421\n",
      "loss: 45906.52183764895\n",
      "loss: 45906.511355143906\n",
      "loss: 45906.50188247479\n",
      "loss: 45906.49059559555\n",
      "loss: 45906.476397320665\n",
      "loss: 45906.463661650305\n",
      "loss: 45906.454342323006\n",
      "loss: 45906.44658086579\n",
      "loss: 45906.43934421042\n",
      "loss: 45906.43217481575\n",
      "loss: 45906.42465908429\n",
      "loss: 45906.41710826199\n",
      "loss: 45906.41029621605\n",
      "loss: 45906.40389787606\n",
      "loss: 45906.3972229847\n",
      "loss: 45906.39012399522\n",
      "loss: 45906.382879845994\n",
      "loss: 45906.374739129686\n",
      "loss: 45906.36333538858\n",
      "loss: 45906.352233950514\n",
      "loss: 45906.344346243444\n",
      "loss: 45906.336132276694\n",
      "loss: 45906.327106483346\n",
      "loss: 45906.318158325026\n",
      "loss: 45906.30724138581\n",
      "loss: 45906.29271968431\n",
      "loss: 45906.28132222369\n",
      "loss: 45906.271972376075\n",
      "loss: 45906.263383734884\n",
      "loss: 45906.25630504879\n",
      "loss: 45906.250046953806\n",
      "loss: 45906.24205186897\n",
      "loss: 45906.22981599473\n",
      "loss: 45906.22132543351\n",
      "loss: 45906.216051981704\n",
      "loss: 45906.2113553774\n",
      "loss: 45906.20698281232\n",
      "loss: 45906.20272120771\n",
      "loss: 45906.19812264943\n",
      "loss: 45906.19262771305\n",
      "loss: 45906.18498359469\n",
      "loss: 45906.17485201796\n",
      "loss: 45906.166376974375\n",
      "loss: 45906.16019654864\n",
      "loss: 45906.15441604714\n",
      "loss: 45906.149010820045\n",
      "loss: 45906.14373442408\n",
      "loss: 45906.13749116994\n",
      "loss: 45906.12795531065\n",
      "loss: 45906.11008692984\n",
      "loss: 45906.096053399364\n",
      "loss: 45906.08907639978\n",
      "loss: 45906.08288256933\n",
      "loss: 45906.07630986871\n",
      "loss: 45906.06916813118\n",
      "loss: 45906.06160424629\n",
      "loss: 45906.053661488826\n",
      "loss: 45906.04512850612\n",
      "loss: 45906.03562246533\n",
      "loss: 45906.0249881385\n",
      "loss: 45906.01436613227\n",
      "loss: 45906.0046494488\n",
      "loss: 45905.99483771095\n",
      "loss: 45905.982854740156\n",
      "loss: 45905.9678613835\n",
      "loss: 45905.95517990838\n",
      "loss: 45905.94584375354\n",
      "loss: 45905.938029965364\n",
      "loss: 45905.93055106245\n",
      "loss: 45905.92182498525\n",
      "loss: 45905.90928064442\n",
      "loss: 45905.89638230796\n",
      "loss: 45905.8878791864\n",
      "loss: 45905.88069813135\n",
      "loss: 45905.87318060496\n",
      "loss: 45905.86225446334\n",
      "loss: 45905.84510549801\n",
      "loss: 45905.83298041019\n",
      "loss: 45905.82388100499\n",
      "loss: 45905.813881183145\n",
      "loss: 45905.802709400494\n",
      "loss: 45905.79256954047\n",
      "loss: 45905.78321793079\n",
      "loss: 45905.7727988936\n",
      "loss: 45905.75847107333\n",
      "loss: 45905.74200830424\n",
      "loss: 45905.728096368905\n",
      "loss: 45905.714296278114\n",
      "loss: 45905.70153948797\n",
      "loss: 45905.69041634134\n",
      "loss: 45905.68018066441\n",
      "loss: 45905.66994564436\n",
      "loss: 45905.66029075277\n",
      "loss: 45905.651504133326\n",
      "loss: 45905.638893896255\n",
      "loss: 45905.61080474902\n",
      "loss: 45905.59785632677\n",
      "loss: 45905.59075701478\n",
      "loss: 45905.58385103094\n",
      "loss: 45905.57636083379\n",
      "loss: 45905.567865412675\n",
      "loss: 45905.5582519278\n",
      "loss: 45905.54767554756\n",
      "loss: 45905.535703056106\n",
      "loss: 45905.52033283813\n",
      "loss: 45905.50204213061\n",
      "loss: 45905.484214295866\n",
      "loss: 45905.46795463953\n",
      "loss: 45905.455995578755\n",
      "loss: 45905.446525980464\n",
      "loss: 45905.43729573977\n",
      "loss: 45905.42526978752\n",
      "loss: 45905.4114231921\n",
      "loss: 45905.40210167641\n",
      "loss: 45905.394917546764\n",
      "loss: 45905.388623867875\n",
      "loss: 45905.3827666136\n",
      "loss: 45905.376987462\n",
      "loss: 45905.36986133384\n",
      "loss: 45905.35554581405\n",
      "loss: 45905.33905423529\n",
      "loss: 45905.33191478355\n",
      "loss: 45905.32676076\n",
      "loss: 45905.32190688744\n",
      "loss: 45905.31679422469\n",
      "loss: 45905.31017391003\n",
      "loss: 45905.29980064332\n",
      "loss: 45905.28702338496\n",
      "loss: 45905.27373385789\n",
      "loss: 45905.259386035264\n",
      "loss: 45905.2500773355\n",
      "loss: 45905.24414375865\n",
      "loss: 45905.23827108165\n"
     ]
    }
   ],
   "source": [
    "# Test MTNMF\n",
    "X = np.random.rand(10, 300, 400)\n",
    "A, S, info = MTNMF(20, max_iter=500000).fit(X)\n",
    "print(A.shape, S.shape, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (1, 2), (1, 3), (1, 4), (1, 6), (1, 7), (1, 8), (1, 9), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 9), (3, 1), (3, 2), (3, 3), (3, 4), (3, 6), (3, 7), (3, 8), (3, 9), (4, 1), (4, 2), (4, 3), (4, 4), (4, 6), (4, 7), (4, 8), (4, 9), (5, 2), (5, 6), (5, 7), (5, 8), (5, 9), (6, 1), (6, 2), (6, 3), (6, 4), (6, 8), (7, 1), (7, 2), (7, 3), (7, 4), (7, 6), (7, 7), (7, 8), (7, 9), (8, 1), (8, 2), (8, 3), (8, 4), (8, 5), (8, 6), (8, 7), (8, 8), (8, 9), (9, 1), (9, 2), (9, 3), (9, 4), (9, 6), (9, 7), (9, 8)]\n",
      "n_states: 67\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALAAAACwCAYAAACvt+ReAAADAElEQVR4nO3cMYpVZxjH4TuDjTvIDmwmuITUgXSWihBImCIbmFmBKziFJGCjpZ1g7RIEF5EdTJdxD/kXhx8+T//ec+69P77q5bs6juPxAlHXZ78ALARMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQJmLQn6wf8+e3TNP/w5uM0//T+xTT/9uUxzV9//TzNr79f3d83v03zTmDSBEyagEkTMGkCJk3ApAmYNAGTJmDSBEyagEkTMGkCJk3ApAmYtHkfeN2nvXz7d5sfn3/74a/t+ZzKCUyagEkTMGkCJk3ApAmYNAGTJmDSBEyagEkTMGkCJk3ApAmYNAGTNu8Dr17f/DTNr/cDr9b7bX/0+4FXTmDSBEyagEkTMGkCJk3ApAmYNAGTJmDSBEyagEkTMGkCJk3ApAmYtHkfeL1fd71f+Haa3p9//fXz+AYsnMCkCZg0AZMmYNIETJqASRMwaQImTcCkCZg0AZMmYNIETJqASRMwaVfHcTye+QLr/bjr/bxn++/5r9P8uo99tvX/cwKTJmDSBEyagEkTMGkCJk3ApAmYNAGTJmDSBEyagEkTMGkCJk3ApM37wOs+73w/8Mn7sPV95Po+thOYNAGTJmDSBEyagEkTMGkCJk3ApAmYNAGTJmDSBEyagEkTMGkCJu3J+gEPbz5O869Pfv7T+xfjG2zWfdz1+z9cfp/mX03Tl8v7D++meScwaQImTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZg0AZMmYNIETNq8D3z7yz/T/Nsvf0zz6z7vuk+77sNe7rd94B+dE5g0AZMmYNIETJqASRMwaQImTcCkCZg0AZMmYNIETJqASRMwaQImbd4HXvd5933ibZ93td5v+2rcRz7bzc/Ppvm7u7tp3glMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBk3Z1HMfj2S8B/5cTmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAibtOxI6Vy1PgROuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=176x176 at 0x7F60D80A90F0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 8 8 8 8 7 8 6 1 1 1 1 8 8 9 8 7 3 1 1 1 8 8 8 7 8 1 1 1 1 8 6 9 2 8 3 9\n",
      " 8 2 2 0 7 8 4 2 2 2 2 2 2 7 4 4 0 2 0 2 0 7 2 4 4 4 2 2 2 2]\n",
      "{(1, 1): 1, (1, 2): 8, (1, 3): 8, (1, 4): 8, (1, 6): 8, (1, 7): 7, (1, 8): 8, (1, 9): 6, (2, 1): 1, (2, 2): 1, (2, 3): 1, (2, 4): 1, (2, 5): 8, (2, 6): 8, (2, 7): 9, (2, 8): 8, (2, 9): 7, (3, 1): 3, (3, 2): 1, (3, 3): 1, (3, 4): 1, (3, 6): 8, (3, 7): 8, (3, 8): 8, (3, 9): 7, (4, 1): 8, (4, 2): 1, (4, 3): 1, (4, 4): 1, (4, 6): 1, (4, 7): 8, (4, 8): 6, (4, 9): 9, (5, 2): 2, (5, 6): 8, (5, 7): 3, (5, 8): 9, (5, 9): 8, (6, 1): 2, (6, 2): 2, (6, 3): 0, (6, 4): 7, (6, 8): 8, (7, 1): 4, (7, 2): 2, (7, 3): 2, (7, 4): 2, (7, 6): 2, (7, 7): 2, (7, 8): 2, (7, 9): 7, (8, 1): 4, (8, 2): 4, (8, 3): 0, (8, 4): 2, (8, 5): 0, (8, 6): 2, (8, 7): 0, (8, 8): 7, (8, 9): 2, (9, 1): 4, (9, 2): 4, (9, 3): 4, (9, 4): 2, (9, 6): 2, (9, 7): 2, (9, 8): 2}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALAAAACwCAYAAACvt+ReAAADY0lEQVR4nO3cIauWdxjA4XPOjmFhqM2yYhlMpl1RwSbYDGJYGhuMsSILx0/gMNgeZFYtBtvBlbEw0TQYbnMGQTC5YhDDgpvH77BbePjhdfX7ff/hx5Nu7s1lWfY2IGpr7QfAhIBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBk7Y9/YEv/9wdzT/auT19wqoe7N4czR8/9/lo/rO/fxnN/3Ho1Gj+wMHno/m7N38azfsCkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKSN94Gnjnx/Ye0nzKy8z7x99dvR/G/XHo7mP378ZDQ/5QtMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkzbeB/7nyp3R/IeXz0+fsKq17wNP93nrfIFJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0sb7wE9f/juaPzJ9QNz0PvKj4X3iAwefj+a3L83uEy9nvhvN+wKTJmDSBEyagEkTMGkCJk3ApAmYNAGTJmDSBEyagEkTMGkCJk3ApG0uy7K35gPeHDs7mt96+OM7esk6pveBD++frXTv/2K2kT29Tzy9r+wLTJqASRMwaQImTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZg0AZM2vg883eedunfjv9H8rRuHR/M//PpsND/dh30wml5/n3fKF5g0AZMmYNIETJqASRMwaQImTcCkCZg0AZMmYNIETJqASRMwaQImbXwf+OWLT0YP+OjMbB93bdP7xGvvU099ve+v0fx0n9oXmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZtfB946tXPT0fzv9+Z7aOeWM6N5qf3iU8so/GN+9/sjuan95Gvv/50NL+1YR+Y95iASRMwaQImTcCkCZg0AZMmYNIETJqASRMwaQImTcCkCZg0AZO2+j7w1NHzs33UqZNffTCafzP8/+k+8/XXs/+/evH0aH5nZ2c07wtMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBk7a5LMve2o+A/8sXmDQBkyZg0gRMmoBJEzBpAiZNwKQJmDQBkyZg0gRMmoBJEzBpAibtLViLaRhYKLSHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=176x176 at 0x7F60D80A9128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_abs = 10\n",
    "state_dim = 2\n",
    "n_action = 4\n",
    "\n",
    "visualization_map = dict([\n",
    "    ('G', 0), # goal\n",
    "    ('#', 1),\n",
    "    *[(str(i), i + 2) for i in range(0, 100)],\n",
    "])\n",
    "\n",
    "def get_states(env):\n",
    "    # get the whole state space\n",
    "    states = []\n",
    "    for i in range(env.size[0]):\n",
    "        for j in range(env.size[1]):\n",
    "                if env.is_valid_loc((i, j)):\n",
    "                    states.append((i, j))\n",
    "    infos = {'task_id': [0] * len(states)} \n",
    "    return states, infos\n",
    "\n",
    "def get_img(env, abs_list):\n",
    "    indices = np.zeros(env.size, dtype=np.int64)\n",
    "    k = 0\n",
    "    for i in range(env.size[0]):\n",
    "        for j in range(env.size[1]):\n",
    "            if (i, j) == env.goal:\n",
    "                indices[i, j] = 0\n",
    "            elif env.map[i][j] == '#':\n",
    "                indices[i, j] = 1\n",
    "            else:\n",
    "                indices[i, j] = visualization_map[str(2 + abs_list[k])]\n",
    "                k += 1\n",
    "\n",
    "    img = drawer.draw(indices)\n",
    "    return img\n",
    "\n",
    "env = DiscreteGridWorld('fourroom', (1, 1), (9, 9))\n",
    "decomposer = NMF(n_abs, solver='mu', beta_loss='kullback-leibler', random_state=0)\n",
    "\n",
    "expert = CategoricalActorCriticNet(\n",
    "    1,\n",
    "    state_dim,\n",
    "    n_action,\n",
    "    FCBody(\n",
    "        state_dim, \n",
    "        hidden_units=(16,)\n",
    "    ),  \n",
    ")\n",
    "# load weight\n",
    "weight_dict = expert.state_dict()\n",
    "loaded_weight_dict = {k: v for k, v in torch.load(\n",
    "    'log/grid.fourroom/fc_discrete.baseline/_/0.190228-203237/models/step-128000-mean-1.00',\n",
    "    map_location=lambda storage, loc: storage)['network'].items()\n",
    "    if k in weight_dict}\n",
    "weight_dict.update(loaded_weight_dict)\n",
    "expert.load_state_dict(weight_dict)\n",
    "\n",
    "\n",
    "states, infos = get_states(env)                \n",
    "print(states)\n",
    "print('n_states:', len(states))\n",
    "                \n",
    "actions = expert(np.array(states), infos)['a'] # maybe take the whole distribution\n",
    "\n",
    "img = get_img(env, actions.cpu().detach().numpy())\n",
    "imshow(img)\n",
    "\n",
    "pvs = F.softmax(expert.get_logits(np.array(states), infos), dim=1).cpu().detach().numpy()\n",
    "#pvs = one_hot.encode(actions, n_action).cpu().detach().numpy()\n",
    "abs_mat = decomposer.fit_transform(pvs)\n",
    "policy_mat = decomposer.components_\n",
    "\n",
    "abs_list = np.array(abs_mat).argmax(1)\n",
    "print(abs_list)\n",
    "abs_map = {s: i for s, i in zip(states, abs_list)}\n",
    "print(abs_map)\n",
    "fsave(abs_map, 'data/abs/grid/fourroom/prob.{}'.format(n_abs), 'pkl')\n",
    "\n",
    "img = get_img(env, abs_list)\n",
    "imshow(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
